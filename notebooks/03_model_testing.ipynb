{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Testing Notebook\n",
        "## Reasoning Distillation Project\n",
        "\n",
        "This notebook tests:\n",
        "1. Student model initialization (FLAN-T5)\n",
        "2. Model forward pass and generation\n",
        "3. Teacher model setup (DatasetTeacher)\n",
        "4. End-to-end inference pipeline\n",
        "5. Model performance metrics\n",
        "6. Memory and efficiency analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from src.data.data_loader import TeacherDataLoader\n",
        "from src.data.preprocessor import ReasoningPreprocessor, PreprocessConfig\n",
        "from src.data.dataset import ESNLIDataset, create_dataloaders\n",
        "\n",
        "from src.models.student import (\n",
        "    StudentModel,\n",
        "    StudentConfig,\n",
        "    create_student_model,\n",
        "    compare_model_sizes\n",
        ")\n",
        "from src.models.teacher import (\n",
        "    DatasetTeacher,\n",
        "    compare_teacher_modes\n",
        ")\n",
        "\n",
        "# Styling\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e928f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device (GPU or CPU)\n",
        "# This determines where models and tensors will be loaded\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DEVICE CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n‚úì Device: {device.upper()}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úì CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Using CPU - Training will be slower\")\n",
        "    print(f\"   CPU cores: {torch.get_num_threads()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Compare Model Sizes\n",
        "Understand the different FLAN-T5 model sizes available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display model size comparison\n",
        "compare_model_sizes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Student Model\n",
        "Load and inspect FLAN-T5 student model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create student model configuration\n",
        "print(\"=\" * 70)\n",
        "print(\"INITIALIZING STUDENT MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "student_config = StudentConfig(\n",
        "    model_name=\"google/flan-t5-base\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=128,\n",
        "    device=device,\n",
        "    num_beams=4,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "# Initialize student\n",
        "student = StudentModel(student_config)\n",
        "\n",
        "print(\"\\n‚úì Student model initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display model information\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STUDENT MODEL INFORMATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model_info = student.get_model_info()\n",
        "pprint(model_info)\n",
        "\n",
        "print(\"\\nMemory Footprint:\")\n",
        "memory = student.get_memory_footprint()\n",
        "for key, value in memory.items():\n",
        "    print(f\"  {key}: {value:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model architecture\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nEncoder:\")\n",
        "print(f\"  Layers: {model_info['encoder_layers']}\")\n",
        "print(f\"  Hidden size: {model_info['hidden_size']}\")\n",
        "print(f\"  Attention heads: {model_info['num_heads']}\")\n",
        "\n",
        "print(f\"\\nDecoder:\")\n",
        "print(f\"  Layers: {model_info['decoder_layers']}\")\n",
        "print(f\"  Hidden size: {model_info['hidden_size']}\")\n",
        "print(f\"  Attention heads: {model_info['num_heads']}\")\n",
        "\n",
        "print(f\"\\nVocabulary size: {model_info['vocab_size']:,}\")\n",
        "print(f\"Total parameters: {model_info['parameters']:,}\")\n",
        "\n",
        "# Calculate parameter distribution\n",
        "encoder_params = sum(p.numel() for p in student.model.encoder.parameters())\n",
        "decoder_params = sum(p.numel() for p in student.model.decoder.parameters())\n",
        "other_params = model_info['parameters'] - encoder_params - decoder_params\n",
        "\n",
        "# Plot parameter distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "components = ['Encoder', 'Decoder', 'Other (embeddings, etc.)']\n",
        "params = [encoder_params, decoder_params, other_params]\n",
        "colors = ['#3498db', '#e74c3c', '#95a5a6']\n",
        "\n",
        "bars = ax.bar(components, params, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax.set_ylabel('Number of Parameters')\n",
        "ax.set_title('FLAN-T5 Parameter Distribution')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, param in zip(bars, params):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{param/1e6:.1f}M\\n({param/model_info[\"parameters\"]*100:.1f}%)',\n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load small dataset for testing\n",
        "print(\"=\" * 70)\n",
        "print(\"LOADING TEST DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "loader = TeacherDataLoader()\n",
        "esnli_data = loader.load_esnli()\n",
        "\n",
        "# Use small subset\n",
        "test_data = esnli_data['validation'].select(range(20))\n",
        "\n",
        "print(f\"\\n‚úì Loaded {len(test_data)} test samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset and dataloader\n",
        "preprocess_config = PreprocessConfig(\n",
        "    model_name=\"google/flan-t5-base\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=128\n",
        ")\n",
        "preprocessor = ReasoningPreprocessor(preprocess_config)\n",
        "\n",
        "test_dataset = ESNLIDataset(\n",
        "    test_data,\n",
        "    preprocessor,\n",
        "    use_cache=False\n",
        ")\n",
        "\n",
        "test_loader = create_dataloaders(\n",
        "    test_dataset,\n",
        "    batch_size=4,\n",
        "    num_workers=0,\n",
        "    pad_token_id=preprocessor.tokenizer.pad_token_id,\n",
        "    shuffle_train=False\n",
        ")\n",
        "\n",
        "print(f\"‚úì Created test dataloader with {len(test_loader)} batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Forward Pass (Training Mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test forward pass\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING FORWARD PASS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get a batch\n",
        "batch = next(iter(test_loader))\n",
        "\n",
        "# Move to device\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "print(f\"\\nBatch shapes:\")\n",
        "for key, value in batch.items():\n",
        "    print(f\"  {key}: {value.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "student.model.train()\n",
        "outputs = student(\n",
        "    input_ids=batch['input_ids'],\n",
        "    attention_mask=batch['attention_mask'],\n",
        "    labels=batch['labels']\n",
        ")\n",
        "\n",
        "print(f\"\\nOutputs:\")\n",
        "print(f\"  Loss: {outputs['loss'].item():.4f}\")\n",
        "print(f\"  Logits shape: {outputs['logits'].shape}\")\n",
        "print(f\"  Logits range: [{outputs['logits'].min():.2f}, {outputs['logits'].max():.2f}]\")\n",
        "\n",
        "print(\"\\n‚úì Forward pass successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize loss landscape for one batch\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ANALYZING BATCH LOSSES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Compute loss for each sample in batch\n",
        "student.model.eval()\n",
        "batch_losses = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(batch['input_ids'].shape[0]):\n",
        "        sample_output = student(\n",
        "            input_ids=batch['input_ids'][i:i+1],\n",
        "            attention_mask=batch['attention_mask'][i:i+1],\n",
        "            labels=batch['labels'][i:i+1]\n",
        "        )\n",
        "        batch_losses.append(sample_output['loss'].item())\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(len(batch_losses)), batch_losses, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
        "plt.axhline(float(np.mean(batch_losses)), color='blue', linestyle='--', label=f'Mean: {np.mean(batch_losses):.3f}')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Per-Sample Loss in Batch (Untrained Model)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nLoss statistics:\")\n",
        "print(f\"  Mean: {np.mean(batch_losses):.4f}\")\n",
        "print(f\"  Std: {np.std(batch_losses):.4f}\")\n",
        "print(f\"  Min: {np.min(batch_losses):.4f}\")\n",
        "print(f\"  Max: {np.max(batch_losses):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Generation (Inference Mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test generation\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING GENERATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "student.model.eval()\n",
        "\n",
        "# Generate from batch\n",
        "print(\"\\nGenerating predictions...\")\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = student.generate(\n",
        "        input_ids=batch['input_ids'],\n",
        "        attention_mask=batch['attention_mask'],\n",
        "        max_length=128,\n",
        "        num_beams=4\n",
        "    )\n",
        "\n",
        "generation_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úì Generation complete in {generation_time:.2f}s\")\n",
        "print(f\"  Time per sample: {generation_time / batch['input_ids'].shape[0]:.3f}s\")\n",
        "print(f\"\\nGenerated IDs shape: {generated_ids.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decode and display predictions\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PREDICTIONS vs GROUND TRUTH\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "predictions = student.decode_batch(generated_ids)\n",
        "\n",
        "# Decode ground truth\n",
        "labels_for_decode = batch['labels'].clone()\n",
        "labels_for_decode[labels_for_decode == -100] = preprocessor.tokenizer.pad_token_id\n",
        "ground_truths = student.decode_batch(labels_for_decode)\n",
        "\n",
        "# Decode inputs\n",
        "inputs = student.decode_batch(batch['input_ids'])\n",
        "\n",
        "# Display comparisons\n",
        "for i in range(min(3, len(predictions))):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"SAMPLE {i+1}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nInput:\\n{inputs[i]}\")\n",
        "    print(f\"\\nGround Truth:\\n{ground_truths[i]}\")\n",
        "    print(f\"\\nPrediction (untrained):\\n{predictions[i]}\")\n",
        "    print(f\"\\nPrediction length: {len(predictions[i].split())} words\")\n",
        "    print(f\"Ground truth length: {len(ground_truths[i].split())} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Different Generation Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare generation strategies\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPARING GENERATION STRATEGIES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Take single sample\n",
        "single_input = batch['input_ids'][0:1]\n",
        "single_mask = batch['attention_mask'][0:1]\n",
        "\n",
        "strategies = {\n",
        "    'Greedy': {'num_beams': 1, 'do_sample': False},\n",
        "    'Beam Search (4)': {'num_beams': 4, 'do_sample': False},\n",
        "    'Sampling (T=1.0)': {'num_beams': 1, 'do_sample': True, 'temperature': 1.0, 'top_k': 50},\n",
        "    'Sampling (T=0.7)': {'num_beams': 1, 'do_sample': True, 'temperature': 0.7, 'top_k': 50}\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for name, params in strategies.items():\n",
        "        start = time.time()\n",
        "        generated = student.generate(\n",
        "            input_ids=single_input,\n",
        "            attention_mask=single_mask,\n",
        "            max_length=128,\n",
        "            **params\n",
        "        )\n",
        "        elapsed = time.time() - start\n",
        "        \n",
        "        decoded = student.decode_batch(generated)[0]\n",
        "        results[name] = {'text': decoded, 'time': elapsed}\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nInput: {inputs[0]}\\n\")\n",
        "\n",
        "for name, result in results.items():\n",
        "    print(f\"\\n--- {name} ({result['time']:.3f}s) ---\")\n",
        "    print(result['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Initialize Teacher (DatasetTeacher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare teacher modes\n",
        "compare_teacher_modes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize DatasetTeacher\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INITIALIZING DATASET TEACHER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "teacher = DatasetTeacher()\n",
        "\n",
        "print(\"\\n‚úì DatasetTeacher initialized!\")\n",
        "print(\"\\nThis teacher uses pre-generated explanations from:\")\n",
        "print(\"  ‚Ä¢ e-SNLI: Human-written explanations\")\n",
        "print(\"\\nNo additional compute required!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract teacher knowledge from samples\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXTRACTING TEACHER KNOWLEDGE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get raw samples\n",
        "sample_indices = [0, 1, 2]\n",
        "\n",
        "for idx in sample_indices:\n",
        "    raw_sample = test_data[idx]\n",
        "    \n",
        "    # Extract teacher knowledge\n",
        "    teacher_knowledge = teacher.extract_teacher_knowledge(raw_sample, task_type=\"nli\")\n",
        "    \n",
        "    print(f\"\\n--- Sample {idx + 1} ---\")\n",
        "    print(f\"Premise: {raw_sample['premise']}\")\n",
        "    print(f\"Hypothesis: {raw_sample['hypothesis']}\")\n",
        "    print(f\"\\nTeacher Knowledge:\")\n",
        "    print(f\"  Label: {teacher_knowledge['label']}\")\n",
        "    print(f\"  Explanation: {teacher_knowledge['explanation']}\")\n",
        "    print(f\"  Alternative explanations available: {len([e for e in teacher_knowledge['alternative_explanations'] if e])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Performance Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark inference speed\n",
        "print(\"=\" * 70)\n",
        "print(\"INFERENCE SPEED BENCHMARK\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "batch_sizes = [1, 4, 8, 16] if device == \"cuda\" else [1, 4, 8]\n",
        "inference_times = []\n",
        "\n",
        "student.model.eval()\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    # Create batch\n",
        "    test_input = batch['input_ids'][:bs].to(device)\n",
        "    test_mask = batch['attention_mask'][:bs].to(device)\n",
        "    \n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        _ = student.generate(test_input, test_mask, max_length=128, num_beams=1)\n",
        "    \n",
        "    # Benchmark\n",
        "    times = []\n",
        "    for _ in range(5):\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            _ = student.generate(test_input, test_mask, max_length=128, num_beams=1)\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        times.append(time.time() - start)\n",
        "    \n",
        "    avg_time = np.mean(times)\n",
        "    throughput = bs / avg_time\n",
        "    inference_times.append({'batch_size': bs, 'time': avg_time, 'throughput': throughput})\n",
        "    \n",
        "    print(f\"\\nBatch size {bs}:\")\n",
        "    print(f\"  Avg time: {avg_time:.3f}s\")\n",
        "    print(f\"  Throughput: {throughput:.2f} samples/sec\")\n",
        "    print(f\"  Time per sample: {avg_time/bs:.3f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize throughput\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "batch_sizes_list = [r['batch_size'] for r in inference_times]\n",
        "times_list = [r['time'] for r in inference_times]\n",
        "throughput_list = [r['throughput'] for r in inference_times]\n",
        "\n",
        "# Plot 1: Time vs Batch Size\n",
        "ax1.plot(batch_sizes_list, times_list, marker='o', linewidth=2, markersize=8, color='#e74c3c')\n",
        "ax1.set_xlabel('Batch Size')\n",
        "ax1.set_ylabel('Time (seconds)')\n",
        "ax1.set_title('Inference Time vs Batch Size')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Throughput vs Batch Size\n",
        "ax2.plot(batch_sizes_list, throughput_list, marker='s', linewidth=2, markersize=8, color='#2ecc71')\n",
        "ax2.set_xlabel('Batch Size')\n",
        "ax2.set_ylabel('Throughput (samples/sec)')\n",
        "ax2.set_title('Throughput vs Batch Size')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Optimal batch size for throughput: {batch_sizes_list[np.argmax(throughput_list)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Memory Usage Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze memory usage\n",
        "print(\"=\" * 70)\n",
        "print(\"MEMORY USAGE ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    \n",
        "    # Measure memory during forward pass\n",
        "    initial_memory = torch.cuda.memory_allocated() / 1e6\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = student(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            labels=batch['labels']\n",
        "        )\n",
        "    \n",
        "    forward_memory = torch.cuda.memory_allocated() / 1e6\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / 1e6\n",
        "    \n",
        "    print(f\"\\nGPU Memory Usage:\")\n",
        "    print(f\"  Model parameters: {memory['total_mb']:.2f} MB\")\n",
        "    print(f\"  After forward pass: {forward_memory:.2f} MB\")\n",
        "    print(f\"  Peak usage: {peak_memory:.2f} MB\")\n",
        "    print(f\"  Activations: {forward_memory - memory['total_mb']:.2f} MB\")\n",
        "    \n",
        "    components = ['Model\\nParameters', 'Activations', 'Peak Usage']\n",
        "    sizes = [memory['total_mb'], forward_memory - memory['total_mb'], peak_memory - forward_memory]\n",
        "    colors = ['#3498db', '#e67e22', '#e74c3c']\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(components, sizes, color=colors, alpha=0.7, edgecolor='black')\n",
        "    plt.ylabel('Memory (MB)')\n",
        "    plt.title('GPU Memory Breakdown During Inference')\n",
        "    \n",
        "    for bar, size in zip(bars, sizes):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{size:.1f} MB',\n",
        "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"\\nCPU mode - GPU memory analysis not available\")\n",
        "    print(f\"Model memory footprint: {memory['total_mb']:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test Model Freezing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test freezing/unfreezing components\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING MODEL FREEZING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def count_trainable_params(model):\n",
        "    return sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
        "\n",
        "# Initial state\n",
        "initial_trainable = count_trainable_params(student)\n",
        "print(f\"\\nInitial trainable parameters: {initial_trainable:,}\")\n",
        "\n",
        "# Freeze encoder\n",
        "student.freeze_encoder()\n",
        "encoder_frozen = count_trainable_params(student)\n",
        "print(f\"After freezing encoder: {encoder_frozen:,} ({encoder_frozen/initial_trainable*100:.1f}%)\")\n",
        "\n",
        "# Unfreeze encoder, freeze decoder\n",
        "student.unfreeze_encoder()\n",
        "student.freeze_decoder()\n",
        "decoder_frozen = count_trainable_params(student)\n",
        "print(f\"After freezing decoder: {decoder_frozen:,} ({decoder_frozen/initial_trainable*100:.1f}%)\")\n",
        "\n",
        "# Unfreeze all\n",
        "student.unfreeze_decoder()\n",
        "final_trainable = count_trainable_params(student)\n",
        "print(f\"After unfreezing all: {final_trainable:,}\")\n",
        "\n",
        "assert initial_trainable == final_trainable, \"Parameter count mismatch after unfreezing!\"\n",
        "print(\"\\n‚úì Freezing/unfreezing works correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Compare Different Model Sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare small vs base models\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPARING MODEL SIZES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create small model\n",
        "print(\"\\nLoading FLAN-T5-small...\")\n",
        "student_small = create_student_model(model_size=\"small\", device=device)\n",
        "\n",
        "models_comparison = [\n",
        "    {'name': 'FLAN-T5-small', 'model': student_small},\n",
        "    {'name': 'FLAN-T5-base', 'model': student}\n",
        "]\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "for model_dict in models_comparison:\n",
        "    model_name = model_dict['name']\n",
        "    model = model_dict['model']\n",
        "    \n",
        "    info = model.get_model_info()\n",
        "    mem = model.get_memory_footprint()\n",
        "    \n",
        "    # Measure inference time\n",
        "    model.model.eval()\n",
        "    test_input = batch['input_ids'][:4].to(device)\n",
        "    test_mask = batch['attention_mask'][:4].to(device)\n",
        "    \n",
        "    times = []\n",
        "    for _ in range(3):\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            _ = model.generate(test_input, test_mask, max_length=128, num_beams=1)\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        times.append(time.time() - start)\n",
        "    \n",
        "    avg_time = np.mean(times)\n",
        "    \n",
        "    comparison_results.append({\n",
        "        'name': model_name,\n",
        "        'parameters': info['parameters'],\n",
        "        'memory_mb': mem['total_mb'],\n",
        "        'inference_time': avg_time,\n",
        "        'layers': info['encoder_layers']\n",
        "    })\n",
        "\n",
        "# Display comparison\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"{'Model':<20} {'Params':<15} {'Memory (MB)':<15} {'Time (s)':<15} {'Layers':<10}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in comparison_results:\n",
        "    print(f\"{result['name']:<20} {result['parameters']/1e6:<14.1f}M {result['memory_mb']:<15.1f} \"\n",
        "          f\"{result['inference_time']:<15.3f} {result['layers']:<10}\")\n",
        "\n",
        "# Calculate speedup\n",
        "speedup = comparison_results[1]['inference_time'] / comparison_results[0]['inference_time']\n",
        "compression = comparison_results[1]['parameters'] / comparison_results[0]['parameters']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"Small is {speedup:.2f}x faster than base\")\n",
        "print(f\"Base has {compression:.2f}x more parameters than small\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "model_names = [r['name'].replace('FLAN-T5-', '') for r in comparison_results]\n",
        "colors_viz = ['#3498db', '#e74c3c']\n",
        "\n",
        "# Parameters\n",
        "params = [r['parameters']/1e6 for r in comparison_results]\n",
        "axes[0].bar(model_names, params, color=colors_viz, alpha=0.7, edgecolor='black')\n",
        "axes[0].set_ylabel('Parameters (Millions)')\n",
        "axes[0].set_title('Model Size')\n",
        "for i, v in enumerate(params):\n",
        "    axes[0].text(i, v + 5, f'{v:.1f}M', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Memory\n",
        "memory_vals = [r['memory_mb'] for r in comparison_results]\n",
        "axes[1].bar(model_names, memory_vals, color=colors_viz, alpha=0.7, edgecolor='black')\n",
        "axes[1].set_ylabel('Memory (MB)')\n",
        "axes[1].set_title('Memory Footprint')\n",
        "for i, v in enumerate(memory_vals):\n",
        "    axes[1].text(i, v + 10, f'{v:.0f}MB', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Inference Time\n",
        "time_vals = [r['inference_time'] for r in comparison_results]\n",
        "axes[2].bar(model_names, time_vals, color=colors_viz, alpha=0.7, edgecolor='black')\n",
        "axes[2].set_ylabel('Inference Time (seconds)')\n",
        "axes[2].set_title('Inference Speed (batch=4)')\n",
        "for i, v in enumerate(time_vals):\n",
        "    axes[2].text(i, v + 0.01, f'{v:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.suptitle('FLAN-T5 Model Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL TESTING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n‚úÖ STUDENT MODEL (FLAN-T5-base):\")\n",
        "print(f\"  ‚Ä¢ Parameters: {model_info['parameters']:,}\")\n",
        "print(f\"  ‚Ä¢ Memory: {memory['total_mb']:.2f} MB\")\n",
        "print(f\"  ‚Ä¢ Inference speed: ~{1/inference_times[0]['time']:.2f} samples/sec (single)\")\n",
        "print(f\"  ‚Ä¢ Forward pass: WORKING ‚úì\")\n",
        "print(f\"  ‚Ä¢ Generation: WORKING ‚úì\")\n",
        "print(f\"  ‚Ä¢ Freezing: WORKING ‚úì\")\n",
        "\n",
        "print(\"\\n‚úÖ TEACHER MODEL (DatasetTeacher):\")\n",
        "print(\"  ‚Ä¢ Mode: Dataset-as-Teacher\")\n",
        "print(\"  ‚Ä¢ Compute cost: ZERO\")\n",
        "print(\"  ‚Ä¢ Knowledge extraction: WORKING ‚úì\")\n",
        "print(\"  ‚Ä¢ Data sources: e-SNLI (human)\")\n",
        "\n",
        "print(\"\\nüìä PERFORMANCE INSIGHTS:\")\n",
        "print(f\"  ‚Ä¢ FLAN-T5-small is {speedup:.2f}x faster but {compression:.2f}x smaller\")\n",
        "print(f\"  ‚Ä¢ Optimal batch size for throughput: {batch_sizes_list[np.argmax(throughput_list)]}\")\n",
        "print(f\"  ‚Ä¢ Generation strategies tested: 4 variants\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
