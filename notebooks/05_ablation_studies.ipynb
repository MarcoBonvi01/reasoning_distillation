{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e320ca5",
   "metadata": {},
   "source": [
    "# Ablation Studies Notebook\n",
    "## Reasoning Distillation Project\n",
    "\n",
    "This notebook performs systematic ablation studies to understand **knowledge distillation** hyperparameters:\n",
    "\n",
    "### Knowledge Distillation Setup:\n",
    "```\n",
    "Dataset ‚Üí Teacher Model (FLAN-T5-XL) ‚Üí Soft Logits (probabilities)\n",
    "       ‚Üò                              ‚Üó\n",
    "         Student Model (FLAN-T5-Small)\n",
    "         \n",
    "Loss = Œ±¬∑CE(student, labels) + Œ≤¬∑KL(student||teacher)\n",
    "```\n",
    "\n",
    "### Ablation Studies:\n",
    "1. **Distillation Weight (Œ≤)**: How much to learn from teacher vs labels (0.0, 0.3, 0.5, 0.7)\n",
    "2. **Temperature**: Softness of probability distributions (1.0, 2.0, 3.0, 4.0)\n",
    "3. **Label Smoothing**: Regularization effect (0.0, 0.1, 0.2)\n",
    "4. **Training Data Size**: Data efficiency with distillation (10%, 50%, 100%)\n",
    "5. **Generation Temperature**: Inference-time temperature variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "from src.data.data_loader import TeacherDataLoader\n",
    "from src.data.preprocessor import ReasoningPreprocessor, PreprocessConfig\n",
    "from src.data.dataset import ESNLIDataset, create_dataloaders\n",
    "\n",
    "from src.models.student import StudentModel, StudentConfig\n",
    "from src.models.teacher import FlanT5Teacher, TeacherConfig  # Teacher model for distillation\n",
    "\n",
    "from src.training.distillation import (\n",
    "    DistillationConfig,\n",
    "    TokenLevelDistillation,  # Token-level distillation with teacher\n",
    "    compare_distillation_strategies\n",
    ")\n",
    "\n",
    "from src.training.trainer import Trainer, TrainingConfig\n",
    "\n",
    "from src.evaluation.evaluator import Evaluator, EvaluationConfig\n",
    "from src.evaluation.metrics import MetricsConfig, format_metrics\n",
    "\n",
    "# Styling\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (GPU or CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc9323",
   "metadata": {},
   "source": [
    "## 1. Load Base Dataset and Teacher Model\n",
    "\n",
    "For ablation studies, we use smaller subsets but still require the teacher model to provide soft probability distributions for knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8612c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "loader = TeacherDataLoader()\n",
    "esnli_data = loader.load_esnli()\n",
    "\n",
    "# Use subset for faster ablation experiments\n",
    "train_subset = esnli_data['train'].select(range(500))  # 500 samples\n",
    "val_subset = esnli_data['validation'].select(range(100))  # 100 samples\n",
    "\n",
    "print(f\"\\n‚úì Train samples: {len(train_subset)}\")\n",
    "print(f\"‚úì Val samples: {len(val_subset)}\")\n",
    "\n",
    "# Prepare data\n",
    "preprocess_config = PreprocessConfig(\n",
    "    model_name=\"google/flan-t5-small\",\n",
    "    max_source_length=128,\n",
    "    max_target_length=64\n",
    ")\n",
    "\n",
    "preprocessor = ReasoningPreprocessor(preprocess_config)\n",
    "train_dataset = ESNLIDataset(train_subset, preprocessor, use_cache=True)\n",
    "val_dataset = ESNLIDataset(val_subset, preprocessor, use_cache=True)\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=0,\n",
    "    pad_token_id=preprocessor.tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Teacher Model (FLAN-T5-XL) for knowledge distillation\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING TEACHER MODEL (google/flan-t5-xl)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "teacher_config = TeacherConfig(\n",
    "    model_name=\"google/flan-t5-xl\",\n",
    "    device=device,\n",
    "    use_fp16=True if device == \"cuda\" else False,\n",
    "    max_source_length=128,\n",
    "    max_target_length=64\n",
    ")\n",
    "\n",
    "print(f\"Loading {teacher_config.model_name}...\")\n",
    "print(f\"Device: {device}, FP16: {teacher_config.use_fp16}\")\n",
    "\n",
    "teacher = FlanT5Teacher(teacher_config)\n",
    "\n",
    "print(f\"\\n‚úì Teacher loaded!\")\n",
    "print(f\"  Parameters: {teacher.count_parameters():,}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Teacher model will be used for ALL ablation studies\")\n",
    "print(f\"   to provide soft probability distributions (dark knowledge)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c35b85c",
   "metadata": {},
   "source": [
    "## 2. Ablation Study 1: Distillation Weight (Œ≤)\n",
    "\n",
    "Test the effect of different distillation weights on model performance.\n",
    "\n",
    "**Loss = Œ±¬∑CE(student, labels) + Œ≤¬∑KL(student||teacher)**\n",
    "\n",
    "- Œ≤ = 0.0: No distillation (standard supervised learning)\n",
    "- Œ≤ = 0.3: Moderate distillation\n",
    "- Œ≤ = 0.5: Balanced distillation (recommended)\n",
    "- Œ≤ = 0.7: Strong distillation (more teacher influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea96130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation 1: Distillation Weight (Œ≤)\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION STUDY 1: DISTILLATION WEIGHT (Œ≤)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "distill_weights = [0.0, 0.3, 0.5, 0.7]\n",
    "distill_weight_results = []\n",
    "\n",
    "for beta in distill_weights:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing Œ≤ (distill_weight) = {beta}\")\n",
    "    print(f\"Loss = 1.0¬∑CE + {beta}¬∑KL(student||teacher)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create fresh student model\n",
    "    student_config = StudentConfig(\n",
    "        model_name=\"google/flan-t5-small\",\n",
    "        max_source_length=128,\n",
    "        max_target_length=64,\n",
    "        device=device\n",
    "    )\n",
    "    student = StudentModel(student_config)\n",
    "    \n",
    "    # Create distillation strategy with teacher model\n",
    "    distill_config = DistillationConfig(\n",
    "        ce_weight=1.0,           # Œ± - Cross-entropy weight\n",
    "        distill_weight=beta,     # Œ≤ - KL divergence weight\n",
    "        temperature=2.0,         # Temperature for softening\n",
    "        label_smoothing=0.0\n",
    "    )\n",
    "    \n",
    "    # Token-level distillation requires teacher model\n",
    "    distillation_strategy = TokenLevelDistillation(\n",
    "        teacher_model=teacher,\n",
    "        config=distill_config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    training_config = TrainingConfig(\n",
    "        num_epochs=3,\n",
    "        learning_rate=5e-5,\n",
    "        eval_steps=20,\n",
    "        save_steps=1000,  # Don't save\n",
    "        logging_steps=10,\n",
    "        output_dir=f\"../experiments/ablation_distill_weight_{beta}\",\n",
    "        eval_strategy=\"steps\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=student,\n",
    "        train_dataloader=train_loader,\n",
    "        eval_dataloader=val_loader,\n",
    "        distillation_strategy=distillation_strategy,\n",
    "        config=training_config\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_config = EvaluationConfig(\n",
    "        metrics_config=MetricsConfig(\n",
    "            compute_rouge=True,\n",
    "            compute_bertscore=False,\n",
    "            compute_faithfulness=True\n",
    "        ),\n",
    "        save_predictions=False,\n",
    "        output_dir=f\"../experiments/ablation_distill_weight_{beta}_eval\"\n",
    "    )\n",
    "    \n",
    "    evaluator = Evaluator(student, eval_config)\n",
    "    results = evaluator.evaluate(val_loader, split_name=\"val\")\n",
    "    \n",
    "    # Store results\n",
    "    distill_weight_results.append({\n",
    "        'distill_weight': beta,\n",
    "        'accuracy': results['metrics']['label_accuracy'],\n",
    "        'rouge1': results['metrics']['rouge1'],\n",
    "        'rougeL': results['metrics']['rougeL'],\n",
    "        'faithfulness': results['metrics']['faithfulness'],\n",
    "        'final_train_loss': history['train_history'][-1]['loss'],\n",
    "        'final_eval_loss': history['eval_history'][-1]['eval_loss'] if history['eval_history'] else None,\n",
    "        'training_time': training_time\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úì Œ≤ = {beta} completed\")\n",
    "    print(f\"  Accuracy: {results['metrics']['label_accuracy']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {results['metrics']['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISTILLATION WEIGHT ABLATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distillation weight results\n",
    "distill_df = pd.DataFrame(distill_weight_results)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'rouge1', 'rougeL', 'faithfulness']\n",
    "colors_palette = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics_to_plot, colors_palette)):\n",
    "    axes[idx].plot(distill_df['distill_weight'], distill_df[metric], \n",
    "                   marker='o', linewidth=2, markersize=10, color=color)\n",
    "    axes[idx].set_xlabel('Distillation Weight (Œ≤)')\n",
    "    axes[idx].set_ylabel(metric.upper())\n",
    "    axes[idx].set_title(f'{metric.upper()} vs Distillation Weight')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for x, y in zip(distill_df['distill_weight'], distill_df[metric]):\n",
    "        axes[idx].text(x, y + 0.01, f'{y:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Ablation Study: Distillation Weight (Œ≤) Impact\\nLoss = Œ±¬∑CE + Œ≤¬∑KL(student||teacher)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistillation Weight Results Summary:\")\n",
    "print(distill_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c0a76",
   "metadata": {},
   "source": [
    "## 3. Ablation Study 2: Distillation Temperature\n",
    "\n",
    "Test different temperature values for softening probability distributions.\n",
    "\n",
    "Higher temperature ‚Üí softer distributions ‚Üí more knowledge transfer from teacher's non-top predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b97a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation 2: Distillation Temperature\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION STUDY 2: DISTILLATION TEMPERATURE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "temperatures = [1.0, 2.0, 3.0, 4.0]\n",
    "temperature_results = []\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing Temperature = {temp}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create fresh student model\n",
    "    student = StudentModel(StudentConfig(\n",
    "        model_name=\"google/flan-t5-small\",\n",
    "        max_source_length=128,\n",
    "        max_target_length=64,\n",
    "        device=device\n",
    "    ))\n",
    "    \n",
    "    # Create distillation strategy with different temperature\n",
    "    distill_config = DistillationConfig(\n",
    "        ce_weight=1.0,\n",
    "        distill_weight=0.5,      # Fixed distillation weight\n",
    "        temperature=temp,         # Varying temperature\n",
    "        label_smoothing=0.0\n",
    "    )\n",
    "    \n",
    "    distillation_strategy = TokenLevelDistillation(\n",
    "        teacher_model=teacher,\n",
    "        config=distill_config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    training_config = TrainingConfig(\n",
    "        num_epochs=3,\n",
    "        learning_rate=5e-5,\n",
    "        eval_steps=20,\n",
    "        save_steps=1000,\n",
    "        logging_steps=10,\n",
    "        output_dir=f\"../experiments/ablation_temp_{temp}\",\n",
    "        eval_strategy=\"steps\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=student,\n",
    "        train_dataloader=train_loader,\n",
    "        eval_dataloader=val_loader,\n",
    "        distillation_strategy=distillation_strategy,\n",
    "        config=training_config\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = Evaluator(student, EvaluationConfig(\n",
    "        metrics_config=MetricsConfig(\n",
    "            compute_rouge=True,\n",
    "            compute_bertscore=False,\n",
    "            compute_faithfulness=True\n",
    "        ),\n",
    "        save_predictions=False,\n",
    "        output_dir=f\"../experiments/ablation_temp_{temp}_eval\"\n",
    "    ))\n",
    "    \n",
    "    results = evaluator.evaluate(val_loader, split_name=\"val\")\n",
    "    \n",
    "    # Store results\n",
    "    temperature_results.append({\n",
    "        'temperature': temp,\n",
    "        'accuracy': results['metrics']['label_accuracy'],\n",
    "        'rouge1': results['metrics']['rouge1'],\n",
    "        'rougeL': results['metrics']['rougeL'],\n",
    "        'faithfulness': results['metrics']['faithfulness'],\n",
    "        'training_time': training_time\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úì Temperature {temp} completed\")\n",
    "    print(f\"  Accuracy: {results['metrics']['label_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEMPERATURE ABLATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature results\n",
    "temp_df = pd.DataFrame(temperature_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Performance vs Temperature\n",
    "metrics = ['accuracy', 'rouge1', 'rougeL', 'faithfulness']\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "for metric, color in zip(metrics, colors):\n",
    "    axes[0].plot(temp_df['temperature'], temp_df[metric], \n",
    "                 marker='o', linewidth=2, markersize=8, label=metric.upper(), color=color)\n",
    "\n",
    "axes[0].set_xlabel('Distillation Temperature')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Performance vs Distillation Temperature')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy zoom\n",
    "axes[1].plot(temp_df['temperature'], temp_df['accuracy'], \n",
    "             marker='o', linewidth=3, markersize=10, color='#e74c3c')\n",
    "axes[1].set_xlabel('Distillation Temperature')\n",
    "axes[1].set_ylabel('Label Accuracy')\n",
    "axes[1].set_title('Label Accuracy vs Temperature')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "for x, y in zip(temp_df['temperature'], temp_df['accuracy']):\n",
    "    axes[1].text(x, y + 0.005, f'{y:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('Ablation Study: Distillation Temperature Impact', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTemperature Results Summary:\")\n",
    "print(temp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300feff",
   "metadata": {},
   "source": [
    "## 4. Ablation Study 3: Training Data Size with Distillation\n",
    "\n",
    "Evaluate how model performance scales with training data size when using knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation 3: Training Data Size\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION STUDY 3: TRAINING DATA SIZE WITH DISTILLATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "data_fractions = [0.1, 0.5, 1.0]\n",
    "data_size_results = []\n",
    "\n",
    "# Use larger base dataset for this study\n",
    "full_train = esnli_data['train'].select(range(2000))\n",
    "\n",
    "for fraction in data_fractions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing Data Fraction = {fraction*100}%\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Sample data\n",
    "    n_samples = int(len(full_train) * fraction)\n",
    "    train_fraction = full_train.select(range(n_samples))\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset_frac = ESNLIDataset(train_fraction, preprocessor, use_cache=True)\n",
    "    train_loader_frac, _ = create_dataloaders(\n",
    "        train_dataset=train_dataset_frac,\n",
    "        val_dataset=val_dataset,\n",
    "        batch_size=16,\n",
    "        num_workers=0,\n",
    "        pad_token_id=preprocessor.tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Create fresh student model\n",
    "    student = StudentModel(StudentConfig(\n",
    "        model_name=\"google/flan-t5-small\",\n",
    "        max_source_length=128,\n",
    "        max_target_length=64,\n",
    "        device=device\n",
    "    ))\n",
    "    \n",
    "    # Create distillation strategy (fixed optimal params)\n",
    "    distillation_strategy = TokenLevelDistillation(\n",
    "        teacher_model=teacher,\n",
    "        config=DistillationConfig(\n",
    "            ce_weight=1.0,\n",
    "            distill_weight=0.5,\n",
    "            temperature=2.0,\n",
    "            label_smoothing=0.0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    training_config = TrainingConfig(\n",
    "        num_epochs=3,\n",
    "        learning_rate=5e-5,\n",
    "        eval_steps=20,\n",
    "        save_steps=1000,\n",
    "        logging_steps=10,\n",
    "        output_dir=f\"../experiments/ablation_datasize_{fraction}\",\n",
    "        eval_strategy=\"steps\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=student,\n",
    "        train_dataloader=train_loader_frac,\n",
    "        eval_dataloader=val_loader,\n",
    "        distillation_strategy=distillation_strategy,\n",
    "        config=training_config\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = Evaluator(student, EvaluationConfig(\n",
    "        metrics_config=MetricsConfig(\n",
    "            compute_rouge=True,\n",
    "            compute_bertscore=False,\n",
    "            compute_faithfulness=True\n",
    "        ),\n",
    "        save_predictions=False,\n",
    "        output_dir=f\"../experiments/ablation_datasize_{fraction}_eval\"\n",
    "    ))\n",
    "    \n",
    "    results = evaluator.evaluate(val_loader, split_name=\"val\")\n",
    "    \n",
    "    # Store results\n",
    "    data_size_results.append({\n",
    "        'fraction': fraction,\n",
    "        'n_samples': n_samples,\n",
    "        'accuracy': results['metrics']['label_accuracy'],\n",
    "        'rouge1': results['metrics']['rouge1'],\n",
    "        'rougeL': results['metrics']['rougeL'],\n",
    "        'faithfulness': results['metrics']['faithfulness'],\n",
    "        'training_time': training_time\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úì Data fraction {fraction} completed\")\n",
    "    print(f\"  Samples: {n_samples}\")\n",
    "    print(f\"  Accuracy: {results['metrics']['label_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA SIZE ABLATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data size results\n",
    "datasize_df = pd.DataFrame(data_size_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Performance vs Data Size\n",
    "metrics = ['accuracy', 'rouge1', 'rougeL', 'faithfulness']\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "for metric, color in zip(metrics, colors):\n",
    "    axes[0].plot(datasize_df['n_samples'], datasize_df[metric], \n",
    "                 marker='o', linewidth=2, markersize=8, label=metric.upper(), color=color)\n",
    "\n",
    "axes[0].set_xlabel('Number of Training Samples')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Performance vs Training Data Size (with Distillation)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Time vs Data Size\n",
    "axes[1].plot(datasize_df['n_samples'], datasize_df['training_time'], \n",
    "             marker='s', linewidth=2, markersize=8, color='#9b59b6')\n",
    "axes[1].set_xlabel('Number of Training Samples')\n",
    "axes[1].set_ylabel('Training Time (seconds)')\n",
    "axes[1].set_title('Training Time vs Data Size')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "for x, y in zip(datasize_df['n_samples'], datasize_df['training_time']):\n",
    "    axes[1].text(x, y + 5, f'{y:.0f}s', ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Ablation Study: Training Data Size Impact with Knowledge Distillation', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nData Size Results Summary:\")\n",
    "print(datasize_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63153a0",
   "metadata": {},
   "source": [
    "## 5. Summary and Recommendations\n",
    "\n",
    "Compile findings from all ablation studies to determine optimal hyperparameters for knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all ablation results\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION STUDIES SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nDISTILLATION WEIGHT (Œ≤):\")\n",
    "best_distill_idx = distill_df['accuracy'].idxmax()\n",
    "best_distill_weight: float = distill_df.loc[best_distill_idx, 'distill_weight']  # type: ignore\n",
    "best_distill_accuracy: float = distill_df.loc[best_distill_idx, 'accuracy']  # type: ignore\n",
    "print(f\"  Best value: Œ≤ = {best_distill_weight}\")\n",
    "print(f\"  Best accuracy: {best_distill_accuracy:.4f}\")\n",
    "print(f\"  Insight: {'Distillation helps!' if best_distill_weight > 0 else 'No distillation benefit'}\")\n",
    "print(f\"  Recommendation: Use distill_weight={best_distill_weight} for final training\")\n",
    "\n",
    "print(\"\\nDISTILLATION TEMPERATURE:\")\n",
    "best_temp_idx = temp_df['accuracy'].idxmax()\n",
    "best_temperature: float = temp_df.loc[best_temp_idx, 'temperature']  # type: ignore\n",
    "best_temp_accuracy: float = temp_df.loc[best_temp_idx, 'accuracy']  # type: ignore\n",
    "print(f\"  Best value: T = {best_temperature}\")\n",
    "print(f\"  Best accuracy: {best_temp_accuracy:.4f}\")\n",
    "print(f\"  Recommendation: Use temperature={best_temperature} for distillation\")\n",
    "\n",
    "print(\"\\nTRAINING DATA SIZE:\")\n",
    "print(\"  Performance scaling with distillation:\")\n",
    "for idx in range(len(datasize_df)):\n",
    "    n_samples_val: int = datasize_df.loc[idx, 'n_samples']  # type: ignore\n",
    "    accuracy_val: float = datasize_df.loc[idx, 'accuracy']  # type: ignore\n",
    "    print(f\"    {n_samples_val:4d} samples ‚Üí Accuracy: {accuracy_val:.4f}\")\n",
    "\n",
    "# Extract values for calculations\n",
    "accuracy_first: float = datasize_df.loc[0, 'accuracy']  # type: ignore\n",
    "accuracy_mid: float = datasize_df.loc[1, 'accuracy']  # type: ignore\n",
    "accuracy_last: float = datasize_df.loc[len(datasize_df)-1, 'accuracy']  # type: ignore\n",
    "improvement = accuracy_last - accuracy_first\n",
    "data_efficiency = accuracy_mid / accuracy_last if accuracy_last != 0 else 0.0\n",
    "print(f\"  Recommendation: {'More data helps significantly' if improvement > 0.1 else 'Diminishing returns - distillation may compensate for less data'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS FOR KNOWLEDGE DISTILLATION:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"1. Optimal distillation weight (Œ≤): {best_distill_weight}\")\n",
    "print(f\"2. Optimal temperature: {best_temperature}\")\n",
    "print(f\"3. Data efficiency: {'High' if data_efficiency > 0.9 else 'Moderate'}\")\n",
    "print(f\"\\nRecommended configuration for notebook 06 (Training Loop):\")\n",
    "print(f\"   - ce_weight (Œ±) = 1.0\")\n",
    "print(f\"   - distill_weight (Œ≤) = {best_distill_weight}\")\n",
    "print(f\"   - temperature = {best_temperature}\")\n",
    "print(f\"   - Teacher: google/flan-t5-xl\")\n",
    "print(f\"   - Student: google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to CSV for later analysis\n",
    "output_dir = Path(\"../experiments/ablation_studies\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "distill_df.to_csv(output_dir / \"distillation_weight_results.csv\", index=False)\n",
    "temp_df.to_csv(output_dir / \"temperature_results.csv\", index=False)\n",
    "datasize_df.to_csv(output_dir / \"data_size_results.csv\", index=False)\n",
    "\n",
    "print(f\"‚úì All ablation results saved to {output_dir}\")\n",
    "print(f\"\\nüìÅ Files saved:\")\n",
    "print(f\"   - distillation_weight_results.csv\")\n",
    "print(f\"   - temperature_results.csv\")\n",
    "print(f\"   - data_size_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
