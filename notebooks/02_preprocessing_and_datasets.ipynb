{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing and Dataset Testing\n",
        "## Reasoning Distillation Project\n",
        "\n",
        "This notebook tests:\n",
        "1. TaskFormatter for prompt creation\n",
        "2. ReasoningPreprocessor for tokenization\n",
        "3. PyTorch Dataset classes (e-SNLI, Alpaca, MultiTask)\n",
        "4. DataLoader creation and batching\n",
        "5. End-to-end pipeline validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root / 'src'))\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "\n",
        "from src.data.data_loader import TeacherDataLoader, DatasetConfig\n",
        "from src.data.preprocessor import (\n",
        "    ReasoningPreprocessor,\n",
        "    PreprocessConfig,\n",
        "    TaskFormatter,\n",
        "    quick_preprocess_sample\n",
        ")\n",
        "from src.data.dataset import (\n",
        "    ESNLIDataset,\n",
        "    AlpacaDataset,\n",
        "    MultiTaskDataset,\n",
        "    create_dataloaders,\n",
        "    load_datasets_from_config\n",
        ")\n",
        "\n",
        "# Styling\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "print(\"âœ“ Imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Test TaskFormatter\n",
        "Verify that prompts are correctly formatted for both NLI and instruction tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize formatter\n",
        "formatter = TaskFormatter()\n",
        "\n",
        "# Test NLI formatting\n",
        "print(\"=\" * 70)\n",
        "print(\"NLI TASK FORMATTING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "premise = \"A person on a horse jumps over a broken down airplane.\"\n",
        "hypothesis = \"A person is training his horse for a competition.\"\n",
        "label = 1  # neutral\n",
        "explanation = \"The person is not necessarily training his horse.\"\n",
        "\n",
        "source, target = formatter.format_nli(premise, hypothesis, label, explanation)\n",
        "\n",
        "print(f\"\\nSource (Input):\\n{source}\")\n",
        "print(f\"\\nTarget (Output):\\n{target}\")\n",
        "print(f\"\\nSource length: {len(source.split())} words\")\n",
        "if target:\n",
        "    print(f\"Target length: {len(target.split())} words\")\n",
        "else:\n",
        "    print(\"Target length: 0 words (None)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Instruction formatting\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INSTRUCTION TASK FORMATTING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "instruction = \"Write a poem about spring.\"\n",
        "input_text = \"\"  # No input context\n",
        "output = \"Blossoms dance in gentle breeze,\\nNature wakes from winter's freeze.\"\n",
        "\n",
        "source, target = formatter.format_instruction(instruction, input_text, output)\n",
        "\n",
        "print(f\"\\nSource (Input):\\n{source}\")\n",
        "print(f\"\\nTarget (Output):\\n{target}\")\n",
        "\n",
        "# With input context\n",
        "instruction2 = \"Summarize the following text.\"\n",
        "input_text2 = \"Artificial intelligence has revolutionized many industries.\"\n",
        "output2 = \"AI has transformed various sectors.\"\n",
        "\n",
        "source2, target2 = formatter.format_instruction(instruction2, input_text2, output2)\n",
        "\n",
        "print(f\"\\n--- With Input Context ---\")\n",
        "print(f\"\\nSource (Input):\\n{source2}\")\n",
        "print(f\"\\nTarget (Output):\\n{target2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test ReasoningPreprocessor\n",
        "Test tokenization and encoding for FLAN-T5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize preprocessor\n",
        "config = PreprocessConfig(\n",
        "    model_name=\"google/flan-t5-base\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=128,\n",
        "    padding=\"max_length\"\n",
        ")\n",
        "\n",
        "preprocessor = ReasoningPreprocessor(config)\n",
        "\n",
        "# Display tokenizer info\n",
        "print(\"=\" * 70)\n",
        "print(\"TOKENIZER INFORMATION\")\n",
        "print(\"=\" * 70)\n",
        "pprint(preprocessor.get_tokenizer_info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test tokenization on e-SNLI sample\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TOKENIZING e-SNLI SAMPLE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "esnli_sample = {\n",
        "    'premise': \"A person on a horse jumps over a broken down airplane.\",\n",
        "    'hypothesis': \"A person is training his horse for a competition.\",\n",
        "    'label': 1,\n",
        "    'explanation_1': \"The person is not necessarily training his horse.\"\n",
        "}\n",
        "\n",
        "tokenized = preprocessor.preprocess_esnli_sample(esnli_sample)\n",
        "\n",
        "print(f\"\\nInput IDs shape: {tokenized['input_ids'].shape}\")\n",
        "print(f\"Attention mask shape: {tokenized['attention_mask'].shape}\")\n",
        "print(f\"Labels shape: {tokenized['labels'].shape}\")\n",
        "\n",
        "# Show actual tokens\n",
        "print(f\"\\n--- First 20 Input Tokens ---\")\n",
        "print(tokenized['input_ids'][:20].tolist())\n",
        "\n",
        "print(f\"\\n--- Decoded Input ---\")\n",
        "decoded_input = preprocessor.decode_prediction(tokenized['input_ids'])\n",
        "print(decoded_input)\n",
        "\n",
        "print(f\"\\n--- First 20 Label Tokens ---\")\n",
        "print(tokenized['labels'][:20].tolist())\n",
        "\n",
        "print(f\"\\n--- Decoded Labels ---\")\n",
        "# Replace -100 with pad token for decoding\n",
        "labels_for_decode = tokenized['labels'].clone()\n",
        "labels_for_decode[labels_for_decode == -100] = preprocessor.tokenizer.pad_token_id\n",
        "decoded_labels = preprocessor.decode_prediction(labels_for_decode)\n",
        "print(decoded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize attention mask\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ATTENTION MASK VISUALIZATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "attention_mask = tokenized['attention_mask'].numpy()\n",
        "input_ids = tokenized['input_ids'].numpy()\n",
        "\n",
        "# Find where actual content ends\n",
        "content_length = attention_mask.sum()\n",
        "print(f\"\\nActual content tokens: {content_length} / {len(attention_mask)}\")\n",
        "print(f\"Padding tokens: {len(attention_mask) - content_length}\")\n",
        "\n",
        "# Plot attention mask\n",
        "plt.figure(figsize=(14, 3))\n",
        "plt.imshow(attention_mask.reshape(1, -1), cmap='RdYlGn', aspect='auto')\n",
        "plt.colorbar(label='Attention (1=attend, 0=ignore)')\n",
        "plt.xlabel('Token Position')\n",
        "plt.yticks([])\n",
        "plt.title('Attention Mask Pattern')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Real Data and Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load small subsets for testing\n",
        "print(\"Loading data...\")\n",
        "loader = TeacherDataLoader()\n",
        "\n",
        "# Load e-SNLI (small subset)\n",
        "esnli_full = loader.load_esnli()\n",
        "esnli_train_small = esnli_full['train'].select(range(100))  # First 100 samples\n",
        "esnli_val_small = esnli_full['validation'].select(range(50))  # First 50 samples\n",
        "\n",
        "# Load Alpaca (small subset)\n",
        "alpaca_small = loader.load_alpaca(max_samples=100)\n",
        "\n",
        "print(f\"âœ“ Loaded {len(esnli_train_small)} e-SNLI train samples\")\n",
        "print(f\"âœ“ Loaded {len(esnli_val_small)} e-SNLI val samples\")\n",
        "print(f\"âœ“ Loaded {len(alpaca_small)} Alpaca samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create PyTorch datasets\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING PYTORCH DATASETS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocess_config = PreprocessConfig(\n",
        "    model_name=\"google/flan-t5-base\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=128\n",
        ")\n",
        "preprocessor = ReasoningPreprocessor(preprocess_config)\n",
        "\n",
        "# Create datasets\n",
        "esnli_train_dataset = ESNLIDataset(\n",
        "    esnli_train_small,\n",
        "    preprocessor,\n",
        "    cache_dir=\"../data/cache/esnli_train\",\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "esnli_val_dataset = ESNLIDataset(\n",
        "    esnli_val_small,\n",
        "    preprocessor,\n",
        "    cache_dir=\"../data/cache/esnli_val\",\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "alpaca_dataset = AlpacaDataset(\n",
        "    alpaca_small,\n",
        "    preprocessor,\n",
        "    cache_dir=\"../data/cache/alpaca\",\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ e-SNLI train dataset: {len(esnli_train_dataset)} samples\")\n",
        "print(f\"âœ“ e-SNLI val dataset: {len(esnli_val_dataset)} samples\")\n",
        "print(f\"âœ“ Alpaca dataset: {len(alpaca_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test dataset indexing\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TESTING DATASET INDEXING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get a sample from e-SNLI dataset\n",
        "sample_idx = 0\n",
        "esnli_sample = esnli_train_dataset[sample_idx]\n",
        "\n",
        "print(f\"\\nSample keys: {esnli_sample.keys()}\")\n",
        "print(f\"\\nShapes:\")\n",
        "for key, value in esnli_sample.items():\n",
        "    print(f\"  {key}: {value.shape}\")\n",
        "\n",
        "# Decode and display\n",
        "print(f\"\\n--- Decoded Sample ---\")\n",
        "decoded_input = preprocessor.decode_prediction(esnli_sample['input_ids'])\n",
        "print(f\"Input: {decoded_input}\")\n",
        "\n",
        "labels_for_decode = esnli_sample['labels'].clone()\n",
        "labels_for_decode[labels_for_decode == -100] = preprocessor.tokenizer.pad_token_id\n",
        "decoded_target = preprocessor.decode_prediction(labels_for_decode)\n",
        "print(f\"Target: {decoded_target}\")\n",
        "\n",
        "# Show raw sample\n",
        "raw = esnli_train_dataset.get_raw_sample(sample_idx)\n",
        "print(f\"\\n--- Raw Sample ---\")\n",
        "print(f\"Premise: {raw['premise']}\")\n",
        "print(f\"Hypothesis: {raw['hypothesis']}\")\n",
        "print(f\"Label: {raw['label']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Multi-Task Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create multi-task dataset with different sampling strategies\n",
        "print(\"=\" * 70)\n",
        "print(\"MULTI-TASK DATASET TESTING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "strategies = ['balanced', 'proportional', 'esnli_only', 'alpaca_only']\n",
        "\n",
        "for strategy in strategies:\n",
        "    print(f\"\\n--- Strategy: {strategy} ---\")\n",
        "    \n",
        "    multitask_dataset = MultiTaskDataset(\n",
        "        esnli_dataset=esnli_train_dataset,\n",
        "        alpaca_dataset=alpaca_dataset,\n",
        "        sampling_strategy=strategy\n",
        "    )\n",
        "    \n",
        "    # Sample 20 times and count sources\n",
        "    sample_counts = {'esnli': 0, 'alpaca': 0}\n",
        "    \n",
        "    for i in range(20):\n",
        "        sample = multitask_dataset[i]\n",
        "        decoded = preprocessor.decode_prediction(sample['input_ids'])\n",
        "        \n",
        "        # Detect source by prompt format\n",
        "        if 'nli premise:' in decoded:\n",
        "            sample_counts['esnli'] += 1\n",
        "        else:\n",
        "            sample_counts['alpaca'] += 1\n",
        "    \n",
        "    print(f\"  e-SNLI samples: {sample_counts['esnli']}/20 ({sample_counts['esnli']/20*100:.0f}%)\")\n",
        "    print(f\"  Alpaca samples: {sample_counts['alpaca']}/20 ({sample_counts['alpaca']/20*100:.0f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sampling distribution\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAMPLING DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test balanced strategy over 1000 samples\n",
        "multitask_balanced = MultiTaskDataset(\n",
        "    esnli_dataset=esnli_train_dataset,\n",
        "    alpaca_dataset=alpaca_dataset,\n",
        "    sampling_strategy='balanced'\n",
        ")\n",
        "\n",
        "n_samples = 500\n",
        "sample_sources = []\n",
        "\n",
        "for i in tqdm(range(n_samples), desc=\"Sampling\"):\n",
        "    sample = multitask_balanced[i]\n",
        "    decoded = preprocessor.decode_prediction(sample['input_ids'])\n",
        "    \n",
        "    if 'nli premise:' in decoded:\n",
        "        sample_sources.append('e-SNLI')\n",
        "    else:\n",
        "        sample_sources.append('Alpaca')\n",
        "\n",
        "# Plot distribution\n",
        "from collections import Counter\n",
        "counts = Counter(sample_sources)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(list(counts.keys()), list(counts.values()), color=['#3498db', '#e67e22'])\n",
        "plt.xlabel('Dataset Source')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title(f'Multi-Task Sampling Distribution (n={n_samples}, strategy=balanced)')\n",
        "for i, (key, value) in enumerate(counts.items()):\n",
        "    plt.text(i, value + 5, f'{value}\\n({value/n_samples*100:.1f}%)', \n",
        "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test DataLoader Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "print(\"=\" * 70)\n",
        "print(\"CREATING DATALOADERS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_loader, val_loader = create_dataloaders(\n",
        "    train_dataset=esnli_train_dataset,\n",
        "    val_dataset=esnli_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=0,  # Use 0 for notebook compatibility\n",
        "    pad_token_id=preprocessor.tokenizer.pad_token_id,\n",
        "    shuffle_train=True\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Train DataLoader: {len(train_loader)} batches\")\n",
        "print(f\"âœ“ Val DataLoader: {len(val_loader)} batches\")\n",
        "print(f\"\\nBatch size: {batch_size}\")\n",
        "print(f\"Total train samples: {len(train_loader) * batch_size}\")\n",
        "print(f\"Total val samples: {len(val_loader) * batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test batch iteration\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TESTING BATCH ITERATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get first batch\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "print(f\"\\nBatch keys: {batch.keys()}\")\n",
        "print(f\"\\nBatch shapes:\")\n",
        "for key, value in batch.items():\n",
        "    print(f\"  {key}: {value.shape}\")\n",
        "\n",
        "# Verify batch dimensions\n",
        "assert batch['input_ids'].shape[0] == batch_size, \"Batch size mismatch!\"\n",
        "assert batch['input_ids'].shape[1] == config.max_source_length, \"Sequence length mismatch!\"\n",
        "print(f\"\\nâœ“ Batch dimensions correct!\")\n",
        "\n",
        "# Check device and dtype\n",
        "print(f\"\\nTensor device: {batch['input_ids'].device}\")\n",
        "print(f\"Tensor dtype: {batch['input_ids'].dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display samples from batch\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BATCH SAMPLES PREVIEW\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "n_display = 3\n",
        "\n",
        "for i in range(min(n_display, batch_size)):\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    \n",
        "    # Decode input\n",
        "    input_text = preprocessor.decode_prediction(batch['input_ids'][i])\n",
        "    print(f\"Input: {input_text}\")\n",
        "    \n",
        "    # Decode target\n",
        "    labels = batch['labels'][i].clone()\n",
        "    labels[labels == -100] = preprocessor.tokenizer.pad_token_id\n",
        "    target_text = preprocessor.decode_prediction(labels)\n",
        "    print(f\"Target: {target_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure preprocessing speed\n",
        "import time\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"PREPROCESSING PERFORMANCE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test with caching\n",
        "n_iterations = 3\n",
        "\n",
        "print(\"\\n--- With Caching ---\")\n",
        "times_cached = []\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    start = time.time()\n",
        "    \n",
        "    for i in range(50):\n",
        "        _ = esnli_train_dataset[i]\n",
        "    \n",
        "    elapsed = time.time() - start\n",
        "    times_cached.append(elapsed)\n",
        "    print(f\"Iteration {iteration+1}: {elapsed:.3f}s ({50/elapsed:.1f} samples/sec)\")\n",
        "\n",
        "print(f\"\\nAverage: {np.mean(times_cached):.3f}s\")\n",
        "print(f\"Speedup (iter 2 vs iter 1): {times_cached[0]/times_cached[1]:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze token length distribution\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TOKEN LENGTH DISTRIBUTION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "input_lengths = []\n",
        "label_lengths = []\n",
        "\n",
        "for i in tqdm(range(len(esnli_train_dataset)), desc=\"Analyzing lengths\"):\n",
        "    sample = esnli_train_dataset[i]\n",
        "    \n",
        "    # Count non-padding tokens\n",
        "    input_len = sample['attention_mask'].sum().item()\n",
        "    label_len = (sample['labels'] != -100).sum().item()\n",
        "    \n",
        "    input_lengths.append(input_len)\n",
        "    label_lengths.append(label_len)\n",
        "\n",
        "# Plot distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(input_lengths, bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "axes[0].axvline(np.mean(input_lengths), color='red', linestyle='--', \n",
        "                label=f'Mean: {np.mean(input_lengths):.1f}')\n",
        "axes[0].set_xlabel('Input Length (tokens)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Input Token Length Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].hist(label_lengths, bins=30, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
        "axes[1].axvline(np.mean(label_lengths), color='red', linestyle='--',\n",
        "                label=f'Mean: {np.mean(label_lengths):.1f}')\n",
        "axes[1].set_xlabel('Target Length (tokens)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Target Token Length Distribution')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nInput lengths - Mean: {np.mean(input_lengths):.1f}, Std: {np.std(input_lengths):.1f}\")\n",
        "print(f\"Target lengths - Mean: {np.mean(label_lengths):.1f}, Std: {np.std(label_lengths):.1f}\")\n",
        "print(f\"\\nMax configured lengths: Input={config.max_source_length}, Target={config.max_target_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Extraction Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test label and explanation extraction\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING PREDICTION PARSING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test predictions\n",
        "test_predictions = [\n",
        "    \"entailment explanation: The person is definitely on a horse.\",\n",
        "    \"neutral explanation: We cannot determine if they are training.\",\n",
        "    \"contradiction\",\n",
        "    \"entailment This clearly follows from the premise.\"\n",
        "]\n",
        "\n",
        "for pred in test_predictions:\n",
        "    print(f\"\\nPrediction: {pred}\")\n",
        "    \n",
        "    label = preprocessor.extract_label_from_prediction(pred)\n",
        "    explanation = preprocessor.extract_explanation_from_prediction(pred)\n",
        "    \n",
        "    print(f\"  â†’ Label: {label}\")\n",
        "    print(f\"  â†’ Explanation: {explanation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PREPROCESSING PIPELINE SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nâœ… TaskFormatter: PASSED\")\n",
        "print(\"  â€¢ NLI tasks formatted correctly\")\n",
        "print(\"  â€¢ Instruction tasks formatted correctly\")\n",
        "\n",
        "print(\"\\nâœ… ReasoningPreprocessor: PASSED\")\n",
        "print(f\"  â€¢ Tokenizer loaded: {preprocessor.config.model_name}\")\n",
        "print(f\"  â€¢ Max source length: {preprocessor.config.max_source_length}\")\n",
        "print(f\"  â€¢ Max target length: {preprocessor.config.max_target_length}\")\n",
        "print(\"  â€¢ Tokenization working correctly\")\n",
        "print(\"  â€¢ Padding/truncation working\")\n",
        "\n",
        "print(\"\\nâœ… PyTorch Datasets: PASSED\")\n",
        "print(f\"  â€¢ e-SNLI train: {len(esnli_train_dataset)} samples\")\n",
        "print(f\"  â€¢ e-SNLI val: {len(esnli_val_dataset)} samples\")\n",
        "print(f\"  â€¢ Alpaca: {len(alpaca_dataset)} samples\")\n",
        "print(\"  â€¢ Caching working correctly\")\n",
        "print(\"  â€¢ Multi-task dataset working\")\n",
        "\n",
        "print(\"\\nâœ… DataLoaders: PASSED\")\n",
        "print(f\"  â€¢ Train batches: {len(train_loader)}\")\n",
        "print(f\"  â€¢ Val batches: {len(val_loader)}\")\n",
        "print(f\"  â€¢ Batch size: {batch_size}\")\n",
        "print(\"  â€¢ Collation working correctly\")\n",
        "\n",
        "print(\"\\nâœ… Performance:\")\n",
        "print(f\"  â€¢ Average preprocessing time: {np.mean(times_cached[1:]):.3f}s per 50 samples\")\n",
        "print(f\"  â€¢ Caching speedup: ~{times_cached[0]/np.mean(times_cached[1:]):.1f}x\")\n",
        "print(f\"  â€¢ Average input tokens: {np.mean(input_lengths):.1f}\")\n",
        "print(f\"  â€¢ Average target tokens: {np.mean(label_lengths):.1f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸŽ‰ ALL TESTS PASSED - READY FOR MODEL TRAINING!\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
