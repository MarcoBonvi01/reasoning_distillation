{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing and Dataset Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "# Configure the Python path to import project modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import required libraries for data processing and visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import custom modules for data loading, preprocessing, and dataset creation\n",
        "from src.data.data_loader import TeacherDataLoader, DatasetConfig\n",
        "from src.data.preprocessor import (\n",
        "    ReasoningPreprocessor,\n",
        "    PreprocessConfig,\n",
        "    TaskFormatter\n",
        ")\n",
        "from src.data.dataset import (\n",
        "    ESNLIDataset,\n",
        "    create_dataloaders\n",
        ")\n",
        "\n",
        "# Configure visualization styling\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Enable autoreload for automatic module reloading during development\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Test TaskFormatter\n",
        "Verify that prompts are correctly formatted for both NLI and instruction tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the task formatter\n",
        "formatter = TaskFormatter()\n",
        "\n",
        "# Test NLI (Natural Language Inference) formatting\n",
        "# This tests how the formatter creates prompts for NLI tasks\n",
        "print(\"=\" * 70)\n",
        "print(\"NLI TASK FORMATTING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Define a sample NLI example with premise, hypothesis, label, and explanation\n",
        "premise = \"A person on a horse jumps over a broken down airplane.\"\n",
        "hypothesis = \"A person is training his horse for a competition.\"\n",
        "label = 1  # neutral (0=entailment, 1=neutral, 2=contradiction)\n",
        "explanation = \"The person is not necessarily training his horse.\"\n",
        "\n",
        "# Format the NLI sample into source (input) and target (output) texts\n",
        "source, target = formatter.format_nli(premise, hypothesis, label, explanation)\n",
        "\n",
        "# Display the formatted prompts and their statistics\n",
        "print(f\"\\nSource (Input):\\n{source}\")\n",
        "print(f\"\\nTarget (Output):\\n{target}\")\n",
        "print(f\"\\nSource length: {len(source.split())} words\")\n",
        "if target:\n",
        "    print(f\"Target length: {len(target.split())} words\")\n",
        "else:\n",
        "    print(\"Target length: 0 words (None)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test ReasoningPreprocessor\n",
        "Test tokenization and encoding for FLAN-T5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the reasoning preprocessor with configuration\n",
        "# This handles tokenization and encoding for FLAN-T5 model\n",
        "config = PreprocessConfig(\n",
        "    model_name=\"google/flan-t5-base\",  # Use FLAN-T5 base model tokenizer\n",
        "    max_source_length=256,              # Maximum input sequence length\n",
        "    max_target_length=128,              # Maximum output sequence length\n",
        "    padding=\"max_length\"                # Pad sequences to max length\n",
        ")\n",
        "\n",
        "preprocessor = ReasoningPreprocessor(config)\n",
        "\n",
        "# Display information about the loaded tokenizer\n",
        "print(\"=\" * 70)\n",
        "print(\"TOKENIZER INFORMATION\")\n",
        "print(\"=\" * 70)\n",
        "print(preprocessor.get_tokenizer_info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test tokenization on an e-SNLI sample\n",
        "# This verifies that the preprocessor correctly tokenizes NLI examples\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TOKENIZING e-SNLI SAMPLE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "esnli_sample = {\n",
        "    'premise': \"A person on a horse jumps over a broken down airplane.\",\n",
        "    'hypothesis': \"A person is training his horse for a competition.\",\n",
        "    'label': 1,  # neutral\n",
        "    'explanation_1': \"The person is not necessarily training his horse.\"\n",
        "}\n",
        "\n",
        "print(\"\\nüìù Original Sample:\")\n",
        "print(f\"Premise: {esnli_sample['premise']}\")\n",
        "print(f\"Hypothesis: {esnli_sample['hypothesis']}\")\n",
        "print(f\"Label: {esnli_sample['label']} (neutral)\")\n",
        "print(f\"Explanation: {esnli_sample['explanation_1']}\")\n",
        "\n",
        "# ============================================================================\n",
        "# TOKENIZATION PROCESS\n",
        "# ============================================================================\n",
        "# The preprocessor will:\n",
        "# 1. Format the sample into a prompt (using TaskFormatter)\n",
        "# 2. Tokenize both input (source) and output (target) using T5 tokenizer\n",
        "# 3. Convert text into numerical IDs that the model can process\n",
        "# 4. Create attention masks (which tokens to pay attention to)\n",
        "# 5. Create labels for training (what the model should predict)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 1: TOKENIZE THE SAMPLE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "tokenized = preprocessor.preprocess_esnli_sample(esnli_sample)\n",
        "\n",
        "# ============================================================================\n",
        "# UNDERSTANDING THE OUTPUT TENSORS\n",
        "# ============================================================================\n",
        "# input_ids: The numerical representation of the input text\n",
        "#            Each number corresponds to a token in the vocabulary\n",
        "# attention_mask: Binary mask (1=real token, 0=padding)\n",
        "#                 Tells the model which tokens to attend to\n",
        "# labels: The numerical representation of the target/output text\n",
        "#         Uses -100 for padding\n",
        "\n",
        "print(\"\\nüìä Tokenized Tensor Shapes:\")\n",
        "print(f\"Input IDs:       {tokenized['input_ids'].shape}       ‚Üí {tokenized['input_ids'].shape[0]} tokens for input\")\n",
        "print(f\"Attention Mask:  {tokenized['attention_mask'].shape}  ‚Üí {tokenized['attention_mask'].shape[0]} mask values\")\n",
        "print(f\"Labels:          {tokenized['labels'].shape}          ‚Üí {tokenized['labels'].shape[0]} tokens for output\")\n",
        "\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 2: EXAMINE INPUT TOKENS (MODEL INPUT)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show the first 20 token IDs\n",
        "# These are numerical representations of the input prompt\n",
        "input_token_ids = tokenized['input_ids'][:20].tolist()\n",
        "print(f\"\\nFirst 20 Input Token IDs:\")\n",
        "print(input_token_ids)\n",
        "\n",
        "# Decode these IDs back to see what they represent\n",
        "print(f\"\\nWhat these IDs mean (decoded one by one):\")\n",
        "for i, token_id in enumerate(input_token_ids[:10]):  # Show first 10 for clarity\n",
        "    token_text = preprocessor.tokenizer.decode([token_id])\n",
        "    print(f\"  Token {i}: ID={token_id:5d} ‚Üí '{token_text}'\")\n",
        "\n",
        "# Decode the full input to see the complete formatted prompt\n",
        "print(f\"\\nFull Decoded Input:\")\n",
        "decoded_input = preprocessor.decode_prediction(tokenized['input_ids'])\n",
        "print(decoded_input)\n",
        "print(f\"\\nThis is what the model sees as input!\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXAMINING ATTENTION MASK\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 3: EXAMINE ATTENTION MASK\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "attention_values = tokenized['attention_mask'][:30].tolist()\n",
        "print(f\"\\nFirst 30 Attention Mask Values:\")\n",
        "print(attention_values)\n",
        "\n",
        "# Count real tokens vs padding\n",
        "num_real_tokens = tokenized['attention_mask'].sum().item()\n",
        "num_padding = len(tokenized['attention_mask']) - num_real_tokens\n",
        "print(f\"\\nüìà Attention Statistics:\")\n",
        "print(f\"  Real tokens (1):  {num_real_tokens}\")\n",
        "print(f\"  Padding (0):      {num_padding}\")\n",
        "print(f\"  Total:            {len(tokenized['attention_mask'])}\")\n",
        "print(f\"\\nThe model only processes the first {num_real_tokens} tokens, ignoring the rest!\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXAMINING LABEL TOKENS (What the model should OUTPUT)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 4: EXAMINE LABEL TOKENS (MODEL OUTPUT/TARGET)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show the first 20 label token IDs\n",
        "# -100 is a special value\n",
        "label_token_ids = tokenized['labels'][:20].tolist()\n",
        "print(f\"\\nFirst 20 Label Token IDs:\")\n",
        "print(label_token_ids)\n",
        "\n",
        "# Count real labels vs padding (-100)\n",
        "num_real_labels = (tokenized['labels'] != -100).sum().item()\n",
        "num_padding_labels = (tokenized['labels'] == -100).sum().item()\n",
        "print(f\"\\nLabel Statistics:\")\n",
        "print(f\"  Real label tokens:  {num_real_labels}\")\n",
        "print(f\"  Padding (-100):     {num_padding_labels}\")\n",
        "print(f\"  Total:              {len(tokenized['labels'])}\")\n",
        "\n",
        "# Decode labels to see what the model should predict\n",
        "# We need to replace -100 with the actual pad token ID for decoding\n",
        "print(f\"\\nWhat these label IDs mean (first 10 non-padding):\")\n",
        "non_padding_labels = tokenized['labels'][tokenized['labels'] != -100][:10]\n",
        "for i, token_id in enumerate(non_padding_labels.tolist()):\n",
        "    token_text = preprocessor.tokenizer.decode([token_id])\n",
        "    print(f\"  Token {i}: ID={token_id:5d} ‚Üí '{token_text}'\")\n",
        "\n",
        "print(f\"\\nFull Decoded Labels (What model should predict):\")\n",
        "labels_for_decode = tokenized['labels'].clone()\n",
        "labels_for_decode[labels_for_decode == -100] = preprocessor.tokenizer.pad_token_id\n",
        "decoded_labels = preprocessor.decode_prediction(labels_for_decode)\n",
        "print(decoded_labels)\n",
        "print(f\"\\nThis is what we want the model to generate!\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TOKENIZATION SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nThe preprocessor has:\")\n",
        "print(f\"  1. Formatted the NLI task into a text prompt\")\n",
        "print(f\"  2. Converted input text ‚Üí {num_real_tokens} numerical tokens\")\n",
        "print(f\"  3. Converted output text ‚Üí {num_real_labels} numerical tokens\")\n",
        "print(f\"  4. Added padding to reach max lengths ({config.max_source_length}/{config.max_target_length})\")\n",
        "print(f\"  5. Created attention masks to ignore padding\")\n",
        "print(f\"\\nTraining objective:\")\n",
        "print(f\"  Given INPUT:  {decoded_input[:100]}...\")\n",
        "print(f\"  Predict:      {decoded_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the attention mask\n",
        "# This shows which tokens the model should attend to (1) and which to ignore (0)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ATTENTION MASK VISUALIZATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Convert tensors to numpy for visualization\n",
        "attention_mask = tokenized['attention_mask'].numpy()\n",
        "input_ids = tokenized['input_ids'].numpy()\n",
        "\n",
        "# Calculate how many tokens are actual content vs padding\n",
        "content_length = attention_mask.sum()\n",
        "print(f\"\\nActual content tokens: {content_length} / {len(attention_mask)}\")\n",
        "print(f\"Padding tokens: {len(attention_mask) - content_length}\")\n",
        "\n",
        "# Create a heatmap visualization of the attention mask\n",
        "plt.figure(figsize=(14, 3))\n",
        "plt.imshow(attention_mask.reshape(1, -1), cmap='RdYlGn', aspect='auto')\n",
        "plt.colorbar(label='Attention (1=attend, 0=ignore)')\n",
        "plt.xlabel('Token Position')\n",
        "plt.yticks([])\n",
        "plt.title('Attention Mask Pattern')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Real Data and Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load small subsets of e-SNLI data for testing\n",
        "# Using small subsets makes testing faster during development\n",
        "print(\"Loading data...\")\n",
        "loader = TeacherDataLoader()\n",
        "\n",
        "# Load the full e-SNLI dataset from HuggingFace\n",
        "esnli_full = loader.load_esnli()\n",
        "\n",
        "# Select small subsets for quick testing\n",
        "esnli_train_small = esnli_full['train'].select(range(100))  # First 100 training samples\n",
        "esnli_val_small = esnli_full['validation'].select(range(50))  # First 50 validation samples\n",
        "\n",
        "print(f\"Loaded {len(esnli_train_small)} e-SNLI train samples\")\n",
        "print(f\"Loaded {len(esnli_val_small)} e-SNLI val samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create PyTorch datasets from the loaded data\n",
        "# These datasets handle tokenization and caching for efficient data loading\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING PYTORCH DATASETS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Initialize preprocessor with configuration\n",
        "preprocess_config = PreprocessConfig(\n",
        "    model_name=\"google/flan-t5-base\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=128\n",
        ")\n",
        "preprocessor = ReasoningPreprocessor(preprocess_config)\n",
        "\n",
        "# Create training dataset with caching enabled\n",
        "esnli_train_dataset = ESNLIDataset(\n",
        "    esnli_train_small,\n",
        "    preprocessor,\n",
        "    cache_dir=\"../data/cache/esnli_train\",  # Cache directory for faster reloading\n",
        "    use_cache=True                           # Enable caching\n",
        ")\n",
        "\n",
        "# Create validation dataset with caching enabled\n",
        "esnli_val_dataset = ESNLIDataset(\n",
        "    esnli_val_small,\n",
        "    preprocessor,\n",
        "    cache_dir=\"../data/cache/esnli_val\",\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "print(f\"\\ne-SNLI train dataset: {len(esnli_train_dataset)} samples\")\n",
        "print(f\"e-SNLI val dataset: {len(esnli_val_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test dataset indexing\n",
        "# Verify that we can access individual samples from the dataset\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TESTING DATASET INDEXING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get the first sample from the training dataset\n",
        "sample_idx = 0\n",
        "esnli_sample = esnli_train_dataset[sample_idx]\n",
        "\n",
        "# Display the structure of a dataset sample\n",
        "print(f\"\\nSample keys: {esnli_sample.keys()}\")\n",
        "print(f\"\\nShapes:\")\n",
        "for key, value in esnli_sample.items():\n",
        "    print(f\"  {key}: {value.shape}\")\n",
        "\n",
        "# Decode the tokenized sample to verify it's correct\n",
        "print(f\"\\n--- Decoded Sample ---\")\n",
        "decoded_input = preprocessor.decode_prediction(esnli_sample['input_ids'])\n",
        "print(f\"Input: {decoded_input}\")\n",
        "\n",
        "# Decode labels (replace -100 padding with pad token)\n",
        "labels_for_decode = esnli_sample['labels'].clone()\n",
        "labels_for_decode[labels_for_decode == -100] = preprocessor.tokenizer.pad_token_id\n",
        "decoded_target = preprocessor.decode_prediction(labels_for_decode)\n",
        "print(f\"Target: {decoded_target}\")\n",
        "\n",
        "# Show the original raw data before preprocessing\n",
        "raw = esnli_train_dataset.get_raw_sample(sample_idx)\n",
        "print(f\"\\n--- Raw Sample ---\")\n",
        "print(f\"Premise: {raw['premise']}\")\n",
        "print(f\"Hypothesis: {raw['hypothesis']}\")\n",
        "print(f\"Label: {raw['label']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test DataLoader Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoaders for batch training\n",
        "# DataLoaders handle batching, shuffling, and parallel data loading\n",
        "print(\"=\" * 70)\n",
        "print(\"CREATING DATALOADERS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "batch_size = 8  # Number of samples per batch\n",
        "\n",
        "# Create train and validation dataloaders\n",
        "train_loader, val_loader = create_dataloaders(\n",
        "    train_dataset=esnli_train_dataset,\n",
        "    val_dataset=esnli_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=0,  # Use 0 for notebook compatibility (no multiprocessing)\n",
        "    pad_token_id=preprocessor.tokenizer.pad_token_id,\n",
        "    shuffle_train=True  # Shuffle training data for better learning\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain DataLoader: {len(train_loader)} batches\")\n",
        "print(f\"Val DataLoader: {len(val_loader)} batches\")\n",
        "print(f\"\\nBatch size: {batch_size}\")\n",
        "print(f\"Total train samples: {len(train_loader) * batch_size}\")\n",
        "print(f\"Total val samples: {len(val_loader) * batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test batch iteration\n",
        "# Verify that batches are correctly formed and have the expected structure\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TESTING BATCH ITERATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get the first batch from the training dataloader\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# Display batch structure\n",
        "print(f\"\\nBatch keys: {batch.keys()}\")\n",
        "print(f\"\\nBatch shapes:\")\n",
        "for key, value in batch.items():\n",
        "    print(f\"  {key}: {value.shape}\")\n",
        "\n",
        "# Verify batch dimensions are correct\n",
        "assert batch['input_ids'].shape[0] == batch_size, \"Batch size mismatch!\"\n",
        "assert batch['input_ids'].shape[1] == config.max_source_length, \"Sequence length mismatch!\"\n",
        "print(f\"\\nBatch dimensions correct!\")\n",
        "\n",
        "# Check tensor properties\n",
        "print(f\"\\nTensor device: {batch['input_ids'].device}\")\n",
        "print(f\"Tensor dtype: {batch['input_ids'].dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display decoded samples from the batch\n",
        "# This helps verify that batching hasn't corrupted the data\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BATCH SAMPLES PREVIEW\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "n_display = 3  # Number of samples to display\n",
        "\n",
        "# Loop through first few samples in the batch\n",
        "for i in range(min(n_display, batch_size)):\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    \n",
        "    # Decode input sequence\n",
        "    input_text = preprocessor.decode_prediction(batch['input_ids'][i])\n",
        "    print(f\"Input: {input_text}\")\n",
        "    \n",
        "    # Decode target sequence (replace -100 padding)\n",
        "    labels = batch['labels'][i].clone()\n",
        "    labels[labels == -100] = preprocessor.tokenizer.pad_token_id\n",
        "    target_text = preprocessor.decode_prediction(labels)\n",
        "    print(f\"Target: {target_text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure preprocessing speed and caching effectiveness\n",
        "# This helps evaluate the performance benefits of caching\n",
        "import time\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"PREPROCESSING PERFORMANCE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test with caching enabled\n",
        "n_iterations = 3  # Run multiple iterations to see cache speedup\n",
        "\n",
        "print(\"\\n--- With Caching ---\")\n",
        "times_cached = []\n",
        "\n",
        "# Run multiple iterations to measure cache performance\n",
        "for iteration in range(n_iterations):\n",
        "    start = time.time()\n",
        "    \n",
        "    # Access 50 samples from the dataset\n",
        "    for i in range(50):\n",
        "        _ = esnli_train_dataset[i]\n",
        "    \n",
        "    elapsed = time.time() - start\n",
        "    times_cached.append(elapsed)\n",
        "    print(f\"Iteration {iteration+1}: {elapsed:.3f}s ({50/elapsed:.1f} samples/sec)\")\n",
        "\n",
        "# Calculate average performance and speedup from caching\n",
        "print(f\"\\nAverage: {np.mean(times_cached):.3f}s\")\n",
        "print(f\"Speedup (iter 2 vs iter 1): {times_cached[0]/times_cached[1]:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze token length distribution\n",
        "# This helps understand if our max_length settings are appropriate\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TOKEN LENGTH DISTRIBUTION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "input_lengths = []\n",
        "label_lengths = []\n",
        "\n",
        "# Iterate through dataset and collect token lengths\n",
        "for i in tqdm(range(len(esnli_train_dataset)), desc=\"Analyzing lengths\"):\n",
        "    sample = esnli_train_dataset[i]\n",
        "    \n",
        "    # Count non-padding tokens for input and labels\n",
        "    input_len = sample['attention_mask'].sum().item()\n",
        "    label_len = (sample['labels'] != -100).sum().item()\n",
        "    \n",
        "    input_lengths.append(input_len)\n",
        "    label_lengths.append(label_len)\n",
        "\n",
        "# Create histograms to visualize token length distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot input length distribution\n",
        "axes[0].hist(input_lengths, bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "axes[0].axvline(np.mean(input_lengths), color='red', linestyle='--', \n",
        "                label=f'Mean: {np.mean(input_lengths):.1f}')\n",
        "axes[0].set_xlabel('Input Length (tokens)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Input Token Length Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot target length distribution\n",
        "axes[1].hist(label_lengths, bins=30, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
        "axes[1].axvline(np.mean(label_lengths), color='red', linestyle='--',\n",
        "                label=f'Mean: {np.mean(label_lengths):.1f}')\n",
        "axes[1].set_xlabel('Target Length (tokens)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Target Token Length Distribution')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print statistics about token lengths\n",
        "print(f\"\\nInput lengths - Mean: {np.mean(input_lengths):.1f}, Std: {np.std(input_lengths):.1f}\")\n",
        "print(f\"Target lengths - Mean: {np.mean(label_lengths):.1f}, Std: {np.std(label_lengths):.1f}\")\n",
        "print(f\"\\nMax configured lengths: Input={config.max_source_length}, Target={config.max_target_length}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Extraction Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test label and explanation extraction from predictions\n",
        "# These functions parse model outputs to extract labels and explanations\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING PREDICTION PARSING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create test predictions in different formats\n",
        "test_predictions = [\n",
        "    \"entailment explanation: The person is definitely on a horse.\",\n",
        "    \"neutral explanation: We cannot determine if they are training.\",\n",
        "    \"contradiction\",\n",
        "    \"entailment This clearly follows from the premise.\"\n",
        "]\n",
        "\n",
        "# Test extraction functions on each prediction\n",
        "for pred in test_predictions:\n",
        "    print(f\"\\nPrediction: {pred}\")\n",
        "    \n",
        "    # Extract the predicted label (entailment/neutral/contradiction)\n",
        "    label = preprocessor.extract_label_from_prediction(pred)\n",
        "    # Extract the explanation text\n",
        "    explanation = preprocessor.extract_explanation_from_prediction(pred)\n",
        "    \n",
        "    print(f\"  ‚Üí Label: {label}\")\n",
        "    print(f\"  ‚Üí Explanation: {explanation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display comprehensive summary of all tests performed\n",
        "# This provides a final validation that all components are working correctly\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PREPROCESSING PIPELINE SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nTaskFormatter\")\n",
        "print(\"  ‚Ä¢ NLI tasks formatted correctly\")\n",
        "print(\"  ‚Ä¢ Instruction tasks formatted correctly\")\n",
        "\n",
        "print(\"\\nReasoningPreprocessor\")\n",
        "print(f\"  ‚Ä¢ Tokenizer loaded: {preprocessor.config.model_name}\")\n",
        "print(f\"  ‚Ä¢ Max source length: {preprocessor.config.max_source_length}\")\n",
        "print(f\"  ‚Ä¢ Max target length: {preprocessor.config.max_target_length}\")\n",
        "print(\"  ‚Ä¢ Tokenization working correctly\")\n",
        "print(\"  ‚Ä¢ Padding/truncation working\")\n",
        "\n",
        "print(\"\\nPyTorch Datasets\")\n",
        "print(f\"  ‚Ä¢ e-SNLI train: {len(esnli_train_dataset)} samples\")\n",
        "print(f\"  ‚Ä¢ e-SNLI val: {len(esnli_val_dataset)} samples\")\n",
        "print(\"  ‚Ä¢ Caching working correctly\")\n",
        "\n",
        "print(\"\\nDataLoaders\")\n",
        "print(f\"  ‚Ä¢ Train batches: {len(train_loader)}\")\n",
        "print(f\"  ‚Ä¢ Val batches: {len(val_loader)}\")\n",
        "print(f\"  ‚Ä¢ Batch size: {batch_size}\")\n",
        "print(\"  ‚Ä¢ Collation working correctly\")\n",
        "\n",
        "print(\"\\nPerformance\")\n",
        "print(f\"  ‚Ä¢ Average preprocessing time: {np.mean(times_cached[1:]):.3f}s per 50 samples\")\n",
        "print(f\"  ‚Ä¢ Caching speedup: ~{times_cached[0]/np.mean(times_cached[1:]):.1f}x\")\n",
        "print(f\"  ‚Ä¢ Average input tokens: {np.mean(input_lengths):.1f}\")\n",
        "print(f\"  ‚Ä¢ Average target tokens: {np.mean(label_lengths):.1f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
