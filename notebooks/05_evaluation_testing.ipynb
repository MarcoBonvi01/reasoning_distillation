{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Pipeline Testing Notebook\n",
        "## Reasoning Distillation Project\n",
        "\n",
        "This notebook tests:\n",
        "1. Individual metrics (Accuracy, ROUGE, BERTScore, Faithfulness)\n",
        "2. Full evaluation pipeline\n",
        "3. Error analysis and confusion matrix\n",
        "4. Per-label evaluation\n",
        "5. Model comparison\n",
        "6. Batch evaluation\n",
        "7. Results visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "\n",
        "from src.data.data_loader import TeacherDataLoader\n",
        "from src.data.preprocessor import ReasoningPreprocessor, PreprocessConfig\n",
        "from src.data.dataset import ESNLIDataset, create_dataloaders\n",
        "\n",
        "from src.models.student import StudentModel, StudentConfig\n",
        "\n",
        "from src.evaluation.metrics import (\n",
        "    LabelAccuracyMetric,\n",
        "    ROUGEMetric,\n",
        "    BERTScoreMetric,\n",
        "    ExplanationFaithfulnessMetric,\n",
        "    StudentTeacherAgreementMetric,\n",
        "    compute_all_metrics,\n",
        "    MetricsConfig,\n",
        "    format_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Test Individual Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample predictions and references\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING INDIVIDUAL METRICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "sample_predictions = [\n",
        "    \"entailment explanation: The person is definitely on a horse.\",\n",
        "    \"neutral explanation: We cannot determine if they are training.\",\n",
        "    \"contradiction explanation: The person cannot be both inside and outside.\",\n",
        "    \"entailment explanation: This clearly follows from the premise.\",\n",
        "    \"neutral explanation: It's unclear what the intention is.\"\n",
        "]\n",
        "\n",
        "sample_references = [\n",
        "    \"entailment explanation: The premise states a person is on a horse.\",\n",
        "    \"neutral explanation: Training is not mentioned in the premise.\",\n",
        "    \"contradiction explanation: These are mutually exclusive locations.\",\n",
        "    \"entailment explanation: The hypothesis is supported by the premise.\",\n",
        "    \"contradiction explanation: The actions contradict each other.\"  # Mismatch for testing\n",
        "]\n",
        "\n",
        "print(f\"\\nCreated {len(sample_predictions)} test samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Test Label Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test label accuracy metric\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LABEL ACCURACY METRIC\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "label_metric = LabelAccuracyMetric()\n",
        "\n",
        "# Test label extraction\n",
        "print(\"\\nLabel extraction test:\")\n",
        "for pred in sample_predictions[:3]:\n",
        "    label = label_metric.extract_label(pred)\n",
        "    print(f\"  '{pred[:40]}...' â†’ {label}\")\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy_results = label_metric.compute(sample_predictions, sample_references)\n",
        "\n",
        "print(\"\\nAccuracy results:\")\n",
        "print(format_metrics(accuracy_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Test ROUGE Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ROUGE metric\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ROUGE METRIC\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "rouge_metric = ROUGEMetric()\n",
        "\n",
        "if rouge_metric.available:\n",
        "    # Test explanation extraction\n",
        "    print(\"\\nExplanation extraction test:\")\n",
        "    for pred in sample_predictions[:2]:\n",
        "        explanation = rouge_metric.extract_explanation(pred)\n",
        "        print(f\"  Input: {pred}\")\n",
        "        print(f\"  Extracted: {explanation}\\n\")\n",
        "    \n",
        "    # Compute ROUGE\n",
        "    rouge_results = rouge_metric.compute(sample_predictions, sample_references)\n",
        "    \n",
        "    print(\"ROUGE results:\")\n",
        "    print(format_metrics(rouge_results))\n",
        "else:\n",
        "    print(\"\\nâš ï¸  ROUGE not available. Install with: pip install rouge-score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Test BERTScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test BERTScore metric\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BERTSCORE METRIC\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "bertscore_metric = BERTScoreMetric()\n",
        "\n",
        "if bertscore_metric.available:\n",
        "    print(\"\\nComputing BERTScore (this may take a moment)...\")\n",
        "    bertscore_results = bertscore_metric.compute(\n",
        "        sample_predictions, \n",
        "        sample_references,\n",
        "        batch_size=8\n",
        "    )\n",
        "    \n",
        "    print(\"\\nBERTScore results:\")\n",
        "    print(format_metrics(bertscore_results))\n",
        "else:\n",
        "    print(\"\\nâš ï¸  BERTScore not available. Install with: pip install bert-score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Test Explanation Faithfulness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test faithfulness metric\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPLANATION FAITHFULNESS METRIC\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "faithfulness_metric = ExplanationFaithfulnessMetric()\n",
        "\n",
        "# Test individual faithfulness checks\n",
        "print(\"\\nFaithfulness checks:\")\n",
        "for pred in sample_predictions:\n",
        "    label, explanation = faithfulness_metric.extract_label_and_explanation(pred)\n",
        "    if label is not None:\n",
        "        is_faithful = faithfulness_metric.check_faithfulness(label, explanation)\n",
        "        status = \"âœ“\" if is_faithful else \"âœ—\"\n",
        "        print(f\"  {status} {label}: {explanation[:50]}...\")\n",
        "    else:\n",
        "        print(f\"  âœ— No label found: {explanation[:50]}...\")\n",
        "\n",
        "# Compute overall faithfulness\n",
        "faithfulness_results = faithfulness_metric.compute(sample_predictions)\n",
        "\n",
        "print(\"\\nFaithfulness results:\")\n",
        "print(format_metrics(faithfulness_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Test All Metrics Combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute all metrics at once\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ALL METRICS COMBINED\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "metrics_config = MetricsConfig(\n",
        "    compute_rouge=True,\n",
        "    compute_bertscore=True,\n",
        "    compute_faithfulness=True\n",
        ")\n",
        "\n",
        "all_metrics = compute_all_metrics(\n",
        "    sample_predictions,\n",
        "    sample_references,\n",
        "    metrics_config\n",
        ")\n",
        "\n",
        "print(\"\\nAll metrics:\")\n",
        "print(format_metrics(all_metrics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Test Data and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "print(\"=\" * 70)\n",
        "print(\"LOADING TEST DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "loader = TeacherDataLoader()\n",
        "esnli_data = loader.load_esnli()\n",
        "\n",
        "# Use validation set for testing\n",
        "test_subset = esnli_data['validation'].select(range(50))  # 50 samples for quick test\n",
        "\n",
        "print(f\"\\nâœ“ Test samples: {len(test_subset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset and dataloader\n",
        "preprocess_config = PreprocessConfig(\n",
        "    model_name=\"google/flan-t5-small\",\n",
        "    max_source_length=128,\n",
        "    max_target_length=64\n",
        ")\n",
        "\n",
        "preprocessor = ReasoningPreprocessor(preprocess_config)\n",
        "test_dataset = ESNLIDataset(test_subset, preprocessor, use_cache=True)\n",
        "\n",
        "test_loader = create_dataloaders(\n",
        "    test_dataset,\n",
        "    batch_size=8,\n",
        "    num_workers=0,\n",
        "    pad_token_id=preprocessor.tokenizer.pad_token_id,\n",
        "    shuffle_train=False\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model (untrained for testing)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "student_config = StudentConfig(\n",
        "    model_name=\"google/flan-t5-small\",\n",
        "    max_source_length=128,\n",
        "    max_target_length=64,\n",
        "    device=device,\n",
        "    num_beams=4\n",
        ")\n",
        "\n",
        "student = StudentModel(student_config)\n",
        "\n",
        "print(f\"\\nâœ“ Model: {student.count_parameters():,} parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Full Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluator\n",
        "print(\"=\" * 70)\n",
        "print(\"INITIALIZING EVALUATOR\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "eval_config = EvaluationConfig(\n",
        "    metrics_config=MetricsConfig(\n",
        "        compute_rouge=True,\n",
        "        compute_bertscore=False,  # Skip BERTScore for speed\n",
        "        compute_faithfulness=True\n",
        "    ),\n",
        "    num_beams=4,\n",
        "    max_length=64,\n",
        "    save_predictions=True,\n",
        "    save_detailed_results=True,\n",
        "    analyze_errors=True,\n",
        "    num_error_examples=5,\n",
        "    output_dir=\"../experiments/evaluation_test\"\n",
        ")\n",
        "\n",
        "evaluator = Evaluator(student, eval_config)\n",
        "\n",
        "print(\"\\nâœ“ Evaluator initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"RUNNING FULL EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "results = evaluator.evaluate(test_loader, split_name=\"test\")\n",
        "\n",
        "print(\"\\nâœ“ Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "metrics = results['metrics']\n",
        "print(\"\\nMetrics:\")\n",
        "print(format_metrics(metrics))\n",
        "\n",
        "print(f\"\\nGenerated {len(results['predictions'])} predictions\")\n",
        "print(f\"Evaluation time: {metrics['eval_time_seconds']:.2f}s\")\n",
        "print(f\"Throughput: {metrics['samples_per_second']:.2f} samples/sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Analyze Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample predictions\n",
        "print(\"=\" * 70)\n",
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "predictions = results['predictions']\n",
        "references = results['references']\n",
        "inputs = results['inputs']\n",
        "\n",
        "for i in range(min(3, len(predictions))):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"SAMPLE {i+1}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nInput:\\n{inputs[i]}\")\n",
        "    print(f\"\\nReference:\\n{references[i]}\")\n",
        "    print(f\"\\nPrediction:\\n{predictions[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display error analysis\n",
        "print(\"=\" * 70)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "error_analysis = results['error_analysis']\n",
        "\n",
        "if error_analysis:\n",
        "    print(f\"\\nTotal errors: {error_analysis['total_errors']}\")\n",
        "    print(f\"Error rate: {error_analysis['error_rate']:.2%}\")\n",
        "    \n",
        "    # Display confusion matrix\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"CONFUSION MATRIX\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    confusion = error_analysis['confusion_matrix']\n",
        "    \n",
        "    # Create DataFrame for better display\n",
        "    labels = ['entailment', 'neutral', 'contradiction']\n",
        "    confusion_df = pd.DataFrame(\n",
        "        [[confusion[true_label][pred_label] for pred_label in labels]\n",
        "         for true_label in labels],\n",
        "        index=labels,\n",
        "        columns=labels\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\", confusion_df)\n",
        "    \n",
        "    # Sample errors\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"SAMPLE ERRORS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for i, error in enumerate(error_analysis['sample_errors'][:3], 1):\n",
        "        print(f\"\\n--- Error {i} ---\")\n",
        "        print(f\"Predicted: {error['predicted_label']}\")\n",
        "        print(f\"True: {error['true_label']}\")\n",
        "        print(f\"Input: {error['input'][:100]}...\")\n",
        "        print(f\"Prediction: {error['prediction'][:100]}...\")\n",
        "else:\n",
        "    print(\"\\nNo error analysis available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize confusion matrix\n",
        "if error_analysis:\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    # Create heatmap\n",
        "    sns.heatmap(\n",
        "        confusion_df,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        square=True,\n",
        "        cbar_kws={'label': 'Count'},\n",
        "        ax=ax\n",
        "    )\n",
        "    \n",
        "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
        "    ax.set_ylabel('True Label', fontsize=12)\n",
        "    ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Per-Label Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate performance by label\n",
        "print(\"=\" * 70)\n",
        "print(\"PER-LABEL EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "per_label_metrics = evaluator.evaluate_by_label(test_loader)\n",
        "\n",
        "print(\"\\nâœ“ Per-label evaluation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize per-label accuracy\n",
        "if per_label_metrics:\n",
        "    labels = list(per_label_metrics.keys())\n",
        "    accuracies = [per_label_metrics[label].get('label_accuracy', 0) \n",
        "                  for label in labels]\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
        "    bars = ax.bar(labels, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
        "    \n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.set_title('Label Accuracy by Category')\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.axhline(y=float(np.mean(accuracies)), color='red', linestyle='--', \n",
        "               label=f'Mean: {np.mean(accuracies):.3f}')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{acc:.3f}',\n",
        "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "    \n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare Metrics Across Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "if per_label_metrics:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"METRICS COMPARISON ACROSS LABELS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Extract common metrics\n",
        "    metric_names = ['label_accuracy', 'rouge1', 'rouge2', 'rougeL', 'faithfulness']\n",
        "    \n",
        "    comparison_data = []\n",
        "    for label in labels:\n",
        "        row = {'label': label}\n",
        "        for metric in metric_names:\n",
        "            row[metric] = float(per_label_metrics[label].get(metric, 0))\n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(\"\\n\", comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    metrics_to_plot = ['label_accuracy', 'rouge1', 'rougeL', 'faithfulness']\n",
        "    \n",
        "    for idx, metric in enumerate(metrics_to_plot):\n",
        "        values = comparison_df[metric].values\n",
        "        axes[idx].bar(labels, values, color=colors, alpha=0.7, edgecolor='black')\n",
        "        axes[idx].set_title(metric.upper().replace('_', ' '))\n",
        "        axes[idx].set_ylim(0, 1.0)\n",
        "        \n",
        "        # Add values\n",
        "        for i, v in enumerate(values):\n",
        "            axes[idx].text(i, v + 0.02, f'{v:.3f}', \n",
        "                          ha='center', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    plt.suptitle('Metrics Comparison Across Labels', \n",
        "                 fontsize=14, fontweight='bold', y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Quick Evaluate Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test quick evaluate helper\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING QUICK EVALUATE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "quick_metrics = quick_evaluate(\n",
        "    student,\n",
        "    test_loader,\n",
        "    output_dir=\"../experiments/quick_eval_test\"\n",
        ")\n",
        "\n",
        "print(\"\\nQuick evaluation metrics:\")\n",
        "print(format_metrics(quick_metrics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Batch Evaluator (Multiple Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create batch evaluator\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING BATCH EVALUATOR\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "batch_evaluator = BatchEvaluator(\n",
        "    output_dir=\"../experiments/batch_evaluation_test\"\n",
        ")\n",
        "\n",
        "# Add current model evaluation (flan-t5-small)\n",
        "print(\"\\nEvaluating Model 1 (FLAN-T5-small)...\")\n",
        "batch_evaluator.add_evaluation(\n",
        "    \"flan-t5-small\",\n",
        "    evaluator,\n",
        "    test_loader\n",
        ")\n",
        "\n",
        "# Create and evaluate flan-t5-base model\n",
        "print(\"\\nEvaluating Model 2 (FLAN-T5-base)...\")\n",
        "\n",
        "# Initialize base model\n",
        "student_config_base = StudentConfig(\n",
        "    model_name=\"google/flan-t5-base\",\n",
        "    max_source_length=128,\n",
        "    max_target_length=64,\n",
        "    device=device,\n",
        "    num_beams=4\n",
        ")\n",
        "\n",
        "student_base = StudentModel(student_config_base)\n",
        "\n",
        "# Create evaluator for base model\n",
        "eval_config_base = EvaluationConfig(\n",
        "    metrics_config=MetricsConfig(\n",
        "        compute_rouge=True,\n",
        "        compute_bertscore=False,  # Skip BERTScore for speed\n",
        "        compute_faithfulness=True\n",
        "    ),\n",
        "    num_beams=4,\n",
        "    max_length=64,\n",
        "    save_predictions=True,\n",
        "    save_detailed_results=True,\n",
        "    analyze_errors=True,\n",
        "    num_error_examples=5,\n",
        "    output_dir=\"../experiments/evaluation_base_test\"\n",
        ")\n",
        "\n",
        "evaluator_base = Evaluator(student_base, eval_config_base)\n",
        "\n",
        "# Add base model evaluation\n",
        "batch_evaluator.add_evaluation(\n",
        "    \"flan-t5-base\",\n",
        "    evaluator_base,\n",
        "    test_loader\n",
        ")\n",
        "\n",
        "# Display batch results\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BATCH EVALUATION RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in batch_evaluator.results:\n",
        "    print(f\"\\nModel: {result['model_name']}\")\n",
        "    print(format_metrics(result['metrics']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f12b255",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize batch results comparison\n",
        "if len(batch_evaluator.results) > 0:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BATCH RESULTS VISUALIZATION\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Extract metrics for comparison\n",
        "    models = [r['model_name'] for r in batch_evaluator.results]\n",
        "    accuracy_scores = [r['metrics'].get('label_accuracy', 0) for r in batch_evaluator.results]\n",
        "    rouge1_scores = [r['metrics'].get('rouge1', 0) for r in batch_evaluator.results]\n",
        "    faithfulness_scores = [r['metrics'].get('faithfulness', 0) for r in batch_evaluator.results]\n",
        "    \n",
        "    # Create comparison visualizations\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "    \n",
        "    # Plot 1: Accuracy\n",
        "    axes[0].bar(models, accuracy_scores, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_title('Label Accuracy Comparison')\n",
        "    axes[0].set_ylim(0, 1.0)\n",
        "    for i, v in enumerate(accuracy_scores):\n",
        "        axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    # Plot 2: ROUGE1\n",
        "    axes[1].bar(models, rouge1_scores, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "    axes[1].set_ylabel('ROUGE1 Score')\n",
        "    axes[1].set_title('ROUGE1 Comparison')\n",
        "    axes[1].set_ylim(0, 1.0)\n",
        "    for i, v in enumerate(rouge1_scores):\n",
        "        axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    # Plot 3: Faithfulness\n",
        "    axes[2].bar(models, faithfulness_scores, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
        "    axes[2].set_ylabel('Faithfulness Score')\n",
        "    axes[2].set_title('Explanation Faithfulness Comparison')\n",
        "    axes[2].set_ylim(0, 1.0)\n",
        "    for i, v in enumerate(faithfulness_scores):\n",
        "        axes[2].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    plt.suptitle('Multi-Model Performance Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9035df48",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save batch evaluation report\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAVING BATCH EVALUATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create summary DataFrame\n",
        "report_data = []\n",
        "for result in batch_evaluator.results:\n",
        "    metrics = result['metrics']\n",
        "    row = {\n",
        "        'Model': result['model_name'],\n",
        "        'Accuracy': metrics.get('label_accuracy', 0),\n",
        "        'ROUGE1': metrics.get('rouge1', 0),\n",
        "        'ROUGE2': metrics.get('rouge2', 0),\n",
        "        'ROUGEL': metrics.get('rougeL', 0),\n",
        "        'Faithfulness': metrics.get('faithfulness', 0),\n",
        "        'Eval Time (s)': metrics.get('eval_time_seconds', 0),\n",
        "        'Throughput (samples/s)': metrics.get('samples_per_second', 0)\n",
        "    }\n",
        "    report_data.append(row)\n",
        "\n",
        "report_df = pd.DataFrame(report_data)\n",
        "\n",
        "# Save as CSV\n",
        "output_path = Path(\"../experiments/batch_evaluation_test\")\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "csv_path = output_path / \"batch_comparison_report.csv\"\n",
        "report_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"\\nâœ“ Report saved to: {csv_path}\")\n",
        "print(\"\\n\" + report_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44629ffc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze distillation impact (if baseline and distilled models available)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DISTILLATION IMPACT ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create a dict for easier lookup\n",
        "batch_results_dict = {r['model_name']: r['metrics'] for r in batch_evaluator.results}\n",
        "\n",
        "# Check if we have both baseline and distilled models\n",
        "distilled_models = [m for m in models if 'distilled' in m.lower()]\n",
        "baseline_models = [m for m in models if 'baseline' in m.lower() or 'untrained' in m.lower()]\n",
        "\n",
        "if distilled_models and baseline_models:\n",
        "    print(f\"\\nFound {len(baseline_models)} baseline(s) and {len(distilled_models)} distilled model(s)\")\n",
        "    \n",
        "    # Compare each pair\n",
        "    improvements = []\n",
        "    for baseline in baseline_models:\n",
        "        for distilled in distilled_models:\n",
        "            # Check if they're the same model size\n",
        "            if any(x in baseline for x in ['small', 'base', 'large']):\n",
        "                model_size = next(x for x in ['small', 'base', 'large'] if x in baseline)\n",
        "                if model_size in distilled:\n",
        "                    baseline_acc = batch_results_dict[baseline].get('label_accuracy', 0)\n",
        "                    distilled_acc = batch_results_dict[distilled].get('label_accuracy', 0)\n",
        "                    improvement = (distilled_acc - baseline_acc) / max(baseline_acc, 0.001) * 100\n",
        "                    \n",
        "                    improvements.append({\n",
        "                        'model_size': model_size,\n",
        "                        'baseline': baseline,\n",
        "                        'distilled': distilled,\n",
        "                        'baseline_acc': baseline_acc,\n",
        "                        'distilled_acc': distilled_acc,\n",
        "                        'improvement_pct': improvement\n",
        "                    })\n",
        "    \n",
        "    # Display improvements\n",
        "    if improvements:\n",
        "        print(\"\\nDistillation Improvements:\")\n",
        "        for imp in improvements:\n",
        "            print(f\"\\n{imp['model_size'].upper()}:\")\n",
        "            print(f\"  Baseline: {imp['baseline_acc']:.4f}\")\n",
        "            print(f\"  Distilled: {imp['distilled_acc']:.4f}\")\n",
        "            print(f\"  Improvement: {imp['improvement_pct']:+.2f}%\")\n",
        "else:\n",
        "    print(\"\\nðŸ’¡ To analyze distillation impact, train models with both baseline and distilled variants\")\n",
        "    print(\"   and add them to the batch evaluator with 'baseline' and 'distilled' in their names.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13f1221e",
      "metadata": {},
      "source": [
        "## 10. Model Size Degradation Analysis\n",
        "\n",
        "Systematic analysis of performance degradation across FLAN-T5 model sizes (small, base, large)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "412a891d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive model size comparison\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL SIZE DEGRADATION ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model_sizes = [\"small\", \"base\", \"large\"]\n",
        "degradation_results = []\n",
        "\n",
        "for size in model_sizes:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Evaluating FLAN-T5-{size.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Create model\n",
        "    model_name = f\"google/flan-t5-{size}\"\n",
        "    student_config_size = StudentConfig(\n",
        "        model_name=model_name,\n",
        "        max_source_length=128,\n",
        "        max_target_length=64,\n",
        "        device=device,\n",
        "        num_beams=4\n",
        "    )\n",
        "    \n",
        "    student_size = StudentModel(student_config_size)\n",
        "    \n",
        "    # Get model info\n",
        "    model_info = student_size.get_model_info()\n",
        "    memory_info = student_size.get_memory_footprint()\n",
        "    \n",
        "    print(f\"\\nModel Information:\")\n",
        "    print(f\"  Parameters: {model_info['parameters']:,}\")\n",
        "    print(f\"  Memory: {memory_info['total_mb']:.2f} MB\")\n",
        "    print(f\"  Layers: {model_info['encoder_layers']}\")\n",
        "    \n",
        "    # Evaluate\n",
        "    eval_config_size = EvaluationConfig(\n",
        "        metrics_config=MetricsConfig(\n",
        "            compute_rouge=True,\n",
        "            compute_bertscore=False,\n",
        "            compute_faithfulness=True\n",
        "        ),\n",
        "        num_beams=4,\n",
        "        max_length=64,\n",
        "        save_predictions=False,\n",
        "        output_dir=f\"../experiments/degradation_{size}_test\"\n",
        "    )\n",
        "    \n",
        "    evaluator_size = Evaluator(student_size, eval_config_size)\n",
        "    \n",
        "    # Time the evaluation\n",
        "    start_time = time.time()\n",
        "    results_size = evaluator_size.evaluate(test_loader, split_name=f\"t5-{size}\")\n",
        "    eval_time = time.time() - start_time\n",
        "    \n",
        "    # Store comprehensive results\n",
        "    degradation_results.append({\n",
        "        'model_size': size,\n",
        "        'model_name': model_name,\n",
        "        'parameters': model_info['parameters'],\n",
        "        'memory_mb': memory_info['total_mb'],\n",
        "        'layers': model_info['encoder_layers'],\n",
        "        'hidden_size': model_info['hidden_size'],\n",
        "        'accuracy': results_size['metrics']['label_accuracy'],\n",
        "        'rouge1': results_size['metrics']['rouge1'],\n",
        "        'rouge2': results_size['metrics']['rouge2'],\n",
        "        'rougeL': results_size['metrics']['rougeL'],\n",
        "        'faithfulness': results_size['metrics']['faithfulness'],\n",
        "        'eval_time': eval_time,\n",
        "        'throughput': results_size['metrics']['samples_per_second']\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nâœ“ {size.upper()} evaluation complete\")\n",
        "    print(f\"  Accuracy: {results_size['metrics']['label_accuracy']:.4f}\")\n",
        "    print(f\"  ROUGE-L: {results_size['metrics']['rougeL']:.4f}\")\n",
        "    print(f\"  Eval time: {eval_time:.2f}s\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEGRADATION ANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1bfa84",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive degradation analysis DataFrame\n",
        "degradation_df = pd.DataFrame(degradation_results)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DEGRADATION ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n\" + degradation_df.to_string(index=False))\n",
        "\n",
        "# Calculate degradation percentages\n",
        "base_idx = degradation_df[degradation_df['model_size'] == 'base'].index[0]\n",
        "base_accuracy = degradation_df.loc[base_idx, 'accuracy']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DEGRADATION vs BASE MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for _, row in degradation_df.iterrows():\n",
        "    if row['model_size'] != 'base':\n",
        "        acc_degradation = ((row['accuracy'] - base_accuracy) / base_accuracy) * 100\n",
        "        rouge_degradation = ((row['rougeL'] - degradation_df.loc[base_idx, 'rougeL']) / degradation_df.loc[base_idx, 'rougeL']) * 100\n",
        "        param_ratio = (row['parameters'] / degradation_df.loc[base_idx, 'parameters']) * 100\n",
        "        \n",
        "        print(f\"\\n{row['model_size'].upper()} vs BASE:\")\n",
        "        print(f\"  Parameters: {param_ratio:.1f}% of base ({row['parameters']:,})\")\n",
        "        print(f\"  Accuracy degradation: {acc_degradation:+.2f}%\")\n",
        "        print(f\"  ROUGE-L degradation: {rouge_degradation:+.2f}%\")\n",
        "        print(f\"  Speed ratio: {degradation_df.loc[base_idx, 'throughput'] / row['throughput']:.2f}x\")\n",
        "        print(f\"  Memory ratio: {row['memory_mb'] / degradation_df.loc[base_idx, 'memory_mb']:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab5055d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize degradation curves\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Plot 1: Accuracy vs Parameters\n",
        "axes[0, 0].plot(degradation_df['parameters']/1e6, degradation_df['accuracy'], \n",
        "                marker='o', linewidth=3, markersize=12, color='#e74c3c')\n",
        "axes[0, 0].set_xlabel('Parameters (Millions)')\n",
        "axes[0, 0].set_ylabel('Label Accuracy')\n",
        "axes[0, 0].set_title('Accuracy Degradation vs Model Size')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "for x, y, label in zip(degradation_df['parameters']/1e6, degradation_df['accuracy'], degradation_df['model_size']):\n",
        "    axes[0, 0].text(x, y + 0.01, label, ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 2: ROUGE-L vs Parameters\n",
        "axes[0, 1].plot(degradation_df['parameters']/1e6, degradation_df['rougeL'], \n",
        "                marker='s', linewidth=3, markersize=12, color='#3498db')\n",
        "axes[0, 1].set_xlabel('Parameters (Millions)')\n",
        "axes[0, 1].set_ylabel('ROUGE-L')\n",
        "axes[0, 1].set_title('ROUGE-L Degradation vs Model Size')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "for x, y, label in zip(degradation_df['parameters']/1e6, degradation_df['rougeL'], degradation_df['model_size']):\n",
        "    axes[0, 1].text(x, y + 0.01, label, ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 3: Faithfulness vs Parameters\n",
        "axes[0, 2].plot(degradation_df['parameters']/1e6, degradation_df['faithfulness'], \n",
        "                marker='^', linewidth=3, markersize=12, color='#2ecc71')\n",
        "axes[0, 2].set_xlabel('Parameters (Millions)')\n",
        "axes[0, 2].set_ylabel('Faithfulness')\n",
        "axes[0, 2].set_title('Faithfulness vs Model Size')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "for x, y, label in zip(degradation_df['parameters']/1e6, degradation_df['faithfulness'], degradation_df['model_size']):\n",
        "    axes[0, 2].text(x, y + 0.01, label, ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 4: Throughput vs Parameters\n",
        "axes[1, 0].plot(degradation_df['parameters']/1e6, degradation_df['throughput'], \n",
        "                marker='d', linewidth=3, markersize=12, color='#9b59b6')\n",
        "axes[1, 0].set_xlabel('Parameters (Millions)')\n",
        "axes[1, 0].set_ylabel('Throughput (samples/sec)')\n",
        "axes[1, 0].set_title('Inference Speed vs Model Size')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "for x, y, label in zip(degradation_df['parameters']/1e6, degradation_df['throughput'], degradation_df['model_size']):\n",
        "    axes[1, 0].text(x, y + 0.5, label, ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 5: Memory vs Parameters\n",
        "axes[1, 1].plot(degradation_df['parameters']/1e6, degradation_df['memory_mb'], \n",
        "                marker='p', linewidth=3, markersize=12, color='#f39c12')\n",
        "axes[1, 1].set_xlabel('Parameters (Millions)')\n",
        "axes[1, 1].set_ylabel('Memory (MB)')\n",
        "axes[1, 1].set_title('Memory Footprint vs Model Size')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "for x, y, label in zip(degradation_df['parameters']/1e6, degradation_df['memory_mb'], degradation_df['model_size']):\n",
        "    axes[1, 1].text(x, y + 20, label, ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 6: Efficiency (Accuracy per Parameter)\n",
        "efficiency = degradation_df['accuracy'] / (degradation_df['parameters'] / 1e6)\n",
        "axes[1, 2].bar(degradation_df['model_size'], efficiency, \n",
        "               color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.7, edgecolor='black')\n",
        "axes[1, 2].set_xlabel('Model Size')\n",
        "axes[1, 2].set_ylabel('Accuracy per Million Parameters')\n",
        "axes[1, 2].set_title('Parameter Efficiency')\n",
        "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(efficiency):\n",
        "    axes[1, 2].text(i, v + 0.0001, f'{v:.5f}', ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Model Size Degradation Analysis: FLAN-T5 Family', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca2f2d9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance-Efficiency Trade-off Analysis\n",
        "print(\"=\" * 70)\n",
        "print(\"PERFORMANCE-EFFICIENCY TRADE-OFF\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate compression ratio vs performance retention\n",
        "base_params = degradation_df.loc[degradation_df['model_size'] == 'base', 'parameters'].values[0]\n",
        "base_acc = degradation_df.loc[degradation_df['model_size'] == 'base', 'accuracy'].values[0]\n",
        "\n",
        "tradeoff_data = []\n",
        "for _, row in degradation_df.iterrows():\n",
        "    compression_ratio = base_params / row['parameters']\n",
        "    accuracy_retention = (row['accuracy'] / base_acc) * 100\n",
        "    rouge_retention = (row['rougeL'] / degradation_df.loc[degradation_df['model_size'] == 'base', 'rougeL'].values[0]) * 100\n",
        "    speedup = row['throughput'] / degradation_df.loc[degradation_df['model_size'] == 'base', 'throughput'].values[0]\n",
        "    \n",
        "    tradeoff_data.append({\n",
        "        'model': row['model_size'],\n",
        "        'compression_ratio': compression_ratio,\n",
        "        'accuracy_retention_%': accuracy_retention,\n",
        "        'rouge_retention_%': rouge_retention,\n",
        "        'speedup': speedup\n",
        "    })\n",
        "\n",
        "tradeoff_df = pd.DataFrame(tradeoff_data)\n",
        "\n",
        "print(\"\\n\" + tradeoff_df.to_string(index=False))\n",
        "\n",
        "# Visualize trade-off\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Accuracy Retention vs Compression\n",
        "colors_tradeoff = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "for idx, row in tradeoff_df.iterrows():\n",
        "    axes[0].scatter(row['compression_ratio'], row['accuracy_retention_%'], \n",
        "                    s=300, alpha=0.7, color=colors_tradeoff[idx], edgecolor='black', linewidth=2)\n",
        "    axes[0].text(row['compression_ratio'], row['accuracy_retention_%'] + 1, \n",
        "                 row['model'], ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "axes[0].set_xlabel('Compression Ratio (vs Base)', fontsize=12)\n",
        "axes[0].set_ylabel('Accuracy Retention (%)', fontsize=12)\n",
        "axes[0].set_title('Compression vs Performance Trade-off', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axhline(y=100, color='red', linestyle='--', alpha=0.5, label='Base Performance')\n",
        "axes[0].axhline(y=90, color='orange', linestyle='--', alpha=0.5, label='90% Retention')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot 2: Speedup vs Accuracy Retention\n",
        "for idx, row in tradeoff_df.iterrows():\n",
        "    axes[1].scatter(row['speedup'], row['accuracy_retention_%'], \n",
        "                    s=300, alpha=0.7, color=colors_tradeoff[idx], edgecolor='black', linewidth=2)\n",
        "    axes[1].text(row['speedup'], row['accuracy_retention_%'] + 1, \n",
        "                 row['model'], ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "axes[1].set_xlabel('Speedup (vs Base)', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy Retention (%)', fontsize=12)\n",
        "axes[1].set_title('Speed vs Performance Trade-off', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axhline(y=100, color='red', linestyle='--', alpha=0.5, label='Base Performance')\n",
        "axes[1].axhline(y=90, color='orange', linestyle='--', alpha=0.5, label='90% Retention')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY FINDINGS:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Find best compression with minimal degradation\n",
        "best_small = tradeoff_df[tradeoff_df['model'] == 'small'].iloc[0]\n",
        "print(f\"\\nSMALL Model Trade-off:\")\n",
        "print(f\"  â€¢ {best_small['compression_ratio']:.1f}x compression\")\n",
        "print(f\"  â€¢ {best_small['accuracy_retention_%']:.1f}% accuracy retention\")\n",
        "print(f\"  â€¢ {best_small['speedup']:.2f}x faster inference\")\n",
        "print(f\"  â€¢ Recommendation: {'EXCELLENT' if best_small['accuracy_retention_%'] >= 95 else 'GOOD' if best_small['accuracy_retention_%'] >= 90 else 'ACCEPTABLE'} for production\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d5b988",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save degradation analysis results\n",
        "degradation_output_dir = Path(\"../experiments/degradation_analysis\")\n",
        "degradation_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "degradation_df.to_csv(degradation_output_dir / \"model_size_comparison.csv\", index=False)\n",
        "tradeoff_df.to_csv(degradation_output_dir / \"tradeoff_analysis.csv\", index=False)\n",
        "\n",
        "print(f\"\\nâœ“ Degradation analysis results saved to {degradation_output_dir}\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸŽ¯ DEGRADATION ANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nðŸ“Š Summary:\")\n",
        "print(f\"  â€¢ Evaluated {len(model_sizes)} model sizes\")\n",
        "print(f\"  â€¢ Parameter range: {degradation_df['parameters'].min()/1e6:.1f}M - {degradation_df['parameters'].max()/1e6:.1f}M\")\n",
        "print(f\"  â€¢ Accuracy range: {degradation_df['accuracy'].min():.3f} - {degradation_df['accuracy'].max():.3f}\")\n",
        "print(f\"  â€¢ Best efficiency: {tradeoff_df.loc[tradeoff_df['accuracy_retention_%'].idxmax(), 'model']}\")\n",
        "print(\"\\nðŸ’¡ Use these insights to select optimal model size for your use case!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
