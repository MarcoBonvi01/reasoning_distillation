{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a432a4",
   "metadata": {},
   "source": [
    "# Compression Analysis Notebook\n",
    "## Reasoning Distillation Project\n",
    "\n",
    "This notebook performs comprehensive **compression analysis** comparing teacher and student models:\n",
    "\n",
    "### Analysis Components:\n",
    "1. **Model Size Comparison**: Parameter counts, memory footprint\n",
    "2. **Compression Ratio**: How much smaller is the student vs teacher\n",
    "3. **Inference Speed**: Latency and throughput comparison\n",
    "4. **Performance vs Efficiency Trade-offs**: Quality degradation analysis\n",
    "5. **Multi-Model Comparison**: FLAN-T5 small, base, large vs XL teacher\n",
    "6. **Visualization**: Publication-ready figures for the paper\n",
    "\n",
    "### Key Metrics:\n",
    "- **Compression Ratio**: $\\frac{\\text{Teacher Parameters}}{\\text{Student Parameters}}$\n",
    "- **Speedup**: $\\frac{\\text{Teacher Latency}}{\\text{Student Latency}}$\n",
    "- **Memory Reduction**: $\\frac{\\text{Teacher Memory}}{\\text{Student Memory}}$\n",
    "- **Performance Retention**: $\\frac{\\text{Student Accuracy}}{\\text{Teacher Accuracy}} \\times 100\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2090df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "from src.data.data_loader import TeacherDataLoader\n",
    "from src.data.preprocessor import ReasoningPreprocessor, PreprocessConfig\n",
    "from src.data.dataset import ESNLIDataset, create_dataloaders\n",
    "\n",
    "from src.models.student import StudentModel, StudentConfig\n",
    "from src.models.teacher import FlanT5Teacher, TeacherConfig\n",
    "\n",
    "from src.evaluation.evaluator import Evaluator, EvaluationConfig\n",
    "from src.evaluation.metrics import MetricsConfig, format_metrics\n",
    "\n",
    "# Styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Create output directory for results\n",
    "OUTPUT_DIR = Path(\"../experiments/compression_analysis\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a0d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2731d5",
   "metadata": {},
   "source": [
    "## 1. Model Size Comparison\n",
    "\n",
    "Compare parameter counts and memory footprints across different FLAN-T5 model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL SIZE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Model specifications (from documentation + actual loading)\n",
    "MODEL_SPECS = {\n",
    "    'flan-t5-small': {\n",
    "        'name': 'google/flan-t5-small',\n",
    "        'approx_params': 77_000_000,\n",
    "        'layers': 6,\n",
    "        'hidden_size': 512,\n",
    "        'role': 'student'\n",
    "    },\n",
    "    'flan-t5-base': {\n",
    "        'name': 'google/flan-t5-base',\n",
    "        'approx_params': 248_000_000,\n",
    "        'layers': 12,\n",
    "        'hidden_size': 768,\n",
    "        'role': 'student'\n",
    "    },\n",
    "    'flan-t5-large': {\n",
    "        'name': 'google/flan-t5-large',\n",
    "        'approx_params': 780_000_000,\n",
    "        'layers': 24,\n",
    "        'hidden_size': 1024,\n",
    "        'role': 'student/teacher'\n",
    "    },\n",
    "    'flan-t5-xl': {\n",
    "        'name': 'google/flan-t5-xl',\n",
    "        'approx_params': 3_000_000_000,\n",
    "        'layers': 24,\n",
    "        'hidden_size': 2048,\n",
    "        'role': 'teacher'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display model specs table\n",
    "specs_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': key,\n",
    "        'Parameters': f\"{specs['approx_params']/1e6:.0f}M\",\n",
    "        'Layers': specs['layers'],\n",
    "        'Hidden Size': specs['hidden_size'],\n",
    "        'Role': specs['role']\n",
    "    }\n",
    "    for key, specs in MODEL_SPECS.items()\n",
    "])\n",
    "\n",
    "print(\"\\nModel Specifications:\")\n",
    "print(specs_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student models for actual measurement\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOADING STUDENT MODELS FOR MEASUREMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "student_models = {}\n",
    "model_stats = []\n",
    "\n",
    "# Load small and base models (these fit in memory easily)\n",
    "for model_key in ['flan-t5-small', 'flan-t5-base']:\n",
    "    print(f\"\\nLoading {model_key}...\")\n",
    "    \n",
    "    config = StudentConfig(\n",
    "        model_name=MODEL_SPECS[model_key]['name'],\n",
    "        max_source_length=128,\n",
    "        max_target_length=64,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    model = StudentModel(config)\n",
    "    student_models[model_key] = model\n",
    "    \n",
    "    # Get actual stats\n",
    "    params = model.count_parameters()\n",
    "    memory = model.get_memory_footprint()\n",
    "    \n",
    "    model_stats.append({\n",
    "        'model': model_key,\n",
    "        'parameters': params,\n",
    "        'memory_mb': memory['total_mb'],\n",
    "        'trainable_params': sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "    })\n",
    "    \n",
    "    print(f\"  âœ“ Parameters: {params:,}\")\n",
    "    print(f\"  âœ“ Memory: {memory['total_mb']:.2f} MB\")\n",
    "\n",
    "# Store stats DataFrame\n",
    "student_stats_df = pd.DataFrame(model_stats)\n",
    "print(\"\\n\" + student_stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9408e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOADING TEACHER MODEL (FLAN-T5-XL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "teacher_config = TeacherConfig(\n",
    "    model_name=\"google/flan-t5-xl\",\n",
    "    device=device,\n",
    "    use_fp16=True if device == \"cuda\" else False,\n",
    "    max_source_length=128,\n",
    "    max_target_length=64\n",
    ")\n",
    "\n",
    "print(f\"Loading {teacher_config.model_name}...\")\n",
    "print(f\"Device: {device}, FP16: {teacher_config.use_fp16}\")\n",
    "\n",
    "teacher = FlanT5Teacher(teacher_config)\n",
    "\n",
    "teacher_params = teacher.count_parameters()\n",
    "# Estimate memory (FP16 = 2 bytes per param, FP32 = 4 bytes)\n",
    "bytes_per_param = 2 if teacher_config.use_fp16 else 4\n",
    "teacher_memory_mb = (teacher_params * bytes_per_param) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nâœ“ Teacher loaded!\")\n",
    "print(f\"  Parameters: {teacher_params:,}\")\n",
    "print(f\"  Estimated Memory: {teacher_memory_mb:.2f} MB ({bytes_per_param} bytes/param)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ef4a3",
   "metadata": {},
   "source": [
    "## 2. Compression Ratio Analysis\n",
    "\n",
    "Calculate compression ratios between teacher and student models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate compression ratios\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPRESSION RATIO ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "compression_results = []\n",
    "\n",
    "for stats in model_stats:\n",
    "    model_name = stats['model']\n",
    "    student_params = stats['parameters']\n",
    "    student_memory = stats['memory_mb']\n",
    "    \n",
    "    # Compression ratios\n",
    "    param_compression = teacher_params / student_params\n",
    "    memory_compression = teacher_memory_mb / student_memory\n",
    "    \n",
    "    compression_results.append({\n",
    "        'Student Model': model_name,\n",
    "        'Student Params': f\"{student_params/1e6:.1f}M\",\n",
    "        'Teacher Params': f\"{teacher_params/1e6:.0f}M\",\n",
    "        'Compression Ratio': f\"{param_compression:.1f}x\",\n",
    "        'Student Memory (MB)': f\"{student_memory:.1f}\",\n",
    "        'Teacher Memory (MB)': f\"{teacher_memory_mb:.1f}\",\n",
    "        'Memory Reduction': f\"{memory_compression:.1f}x\",\n",
    "        'param_compression_numeric': param_compression,\n",
    "        'memory_compression_numeric': memory_compression\n",
    "    })\n",
    "\n",
    "compression_df = pd.DataFrame(compression_results)\n",
    "\n",
    "print(\"\\nCompression Analysis:\")\n",
    "display_cols = ['Student Model', 'Student Params', 'Teacher Params', \n",
    "                'Compression Ratio', 'Student Memory (MB)', 'Memory Reduction']\n",
    "print(compression_df[display_cols].to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "compression_df.to_csv(OUTPUT_DIR / \"compression_ratios.csv\", index=False)\n",
    "print(f\"\\nâœ“ Saved to {OUTPUT_DIR / 'compression_ratios.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede24073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize compression ratios\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = [r['Student Model'].replace('flan-t5-', '') for r in compression_results]\n",
    "param_compression = [r['param_compression_numeric'] for r in compression_results]\n",
    "memory_compression = [r['memory_compression_numeric'] for r in compression_results]\n",
    "\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "# Plot 1: Parameter Compression\n",
    "bars1 = axes[0].bar(models, param_compression, color=colors[0], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Compression Ratio (x)')\n",
    "axes[0].set_xlabel('Student Model')\n",
    "axes[0].set_title('Parameter Compression Ratio\\n(Teacher / Student)')\n",
    "axes[0].axhline(y=10, color='gray', linestyle='--', alpha=0.5, label='10x compression')\n",
    "\n",
    "for bar, val in zip(bars1, param_compression):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                 f'{val:.1f}x', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Plot 2: Memory Reduction\n",
    "bars2 = axes[1].bar(models, memory_compression, color=colors[1], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Memory Reduction (x)')\n",
    "axes[1].set_xlabel('Student Model')\n",
    "axes[1].set_title('Memory Reduction Ratio\\n(Teacher / Student)')\n",
    "\n",
    "for bar, val in zip(bars2, memory_compression):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                 f'{val:.1f}x', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.suptitle('Model Compression Analysis: FLAN-T5-XL Teacher vs Students', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f71eb",
   "metadata": {},
   "source": [
    "## 3. Inference Speed Comparison\n",
    "\n",
    "Benchmark inference latency and throughput for teacher vs student models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74962cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data for benchmarking\n",
    "print(\"=\" * 70)\n",
    "print(\"PREPARING BENCHMARK DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "loader = TeacherDataLoader()\n",
    "esnli_data = loader.load_esnli()\n",
    "\n",
    "# Use validation subset for benchmarking\n",
    "benchmark_data = esnli_data['validation'].select(range(100))\n",
    "\n",
    "preprocess_config = PreprocessConfig(\n",
    "    model_name=\"google/flan-t5-small\",\n",
    "    max_source_length=128,\n",
    "    max_target_length=64\n",
    ")\n",
    "\n",
    "preprocessor = ReasoningPreprocessor(preprocess_config)\n",
    "benchmark_dataset = ESNLIDataset(benchmark_data, preprocessor, use_cache=True)\n",
    "\n",
    "benchmark_loader = create_dataloaders(\n",
    "    benchmark_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    pad_token_id=preprocessor.tokenizer.pad_token_id,\n",
    "    shuffle_train=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Benchmark samples: {len(benchmark_data)}\")\n",
    "print(f\"âœ“ Benchmark batches: {len(benchmark_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b552bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark function\n",
    "def benchmark_model(model, dataloader, model_name, num_warmup=2, num_runs=5, is_teacher=False):\n",
    "    \"\"\"\n",
    "    Benchmark model inference speed.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Benchmark results with latency and throughput metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nBenchmarking {model_name}...\")\n",
    "    \n",
    "    if is_teacher:\n",
    "        model.model.eval()\n",
    "    else:\n",
    "        model.model.eval()\n",
    "    \n",
    "    # Get a single batch for consistent testing\n",
    "    batch = next(iter(dataloader))\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    batch_size = batch['input_ids'].shape[0]\n",
    "    \n",
    "    # Warmup runs\n",
    "    print(f\"  Warmup ({num_warmup} runs)...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmup):\n",
    "            if is_teacher:\n",
    "                _ = model.generate(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    max_length=64\n",
    "                )\n",
    "            else:\n",
    "                _ = model.generate(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    max_length=64,\n",
    "                    num_beams=4\n",
    "                )\n",
    "    \n",
    "    # Synchronize GPU if available\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark runs\n",
    "    print(f\"  Benchmark ({num_runs} runs)...\")\n",
    "    latencies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            if is_teacher:\n",
    "                _ = model.generate(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    max_length=64\n",
    "                )\n",
    "            else:\n",
    "                _ = model.generate(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    max_length=64,\n",
    "                    num_beams=4\n",
    "                )\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            latencies.append(end_time - start_time)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "    throughput = batch_size / avg_latency\n",
    "    latency_per_sample = avg_latency / batch_size * 1000  # ms\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'batch_size': batch_size,\n",
    "        'avg_latency_s': avg_latency,\n",
    "        'std_latency_s': std_latency,\n",
    "        'latency_per_sample_ms': latency_per_sample,\n",
    "        'throughput_samples_per_sec': throughput,\n",
    "        'num_runs': num_runs\n",
    "    }\n",
    "    \n",
    "    print(f\"  âœ“ Avg Latency: {avg_latency:.3f}s Â± {std_latency:.3f}s\")\n",
    "    print(f\"  âœ“ Per-sample: {latency_per_sample:.1f}ms\")\n",
    "    print(f\"  âœ“ Throughput: {throughput:.2f} samples/sec\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ae05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark all models\n",
    "print(\"=\" * 70)\n",
    "print(\"INFERENCE SPEED BENCHMARK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "benchmark_results = []\n",
    "\n",
    "# Benchmark teacher\n",
    "teacher_bench = benchmark_model(\n",
    "    teacher, benchmark_loader, \n",
    "    \"flan-t5-xl (Teacher)\", \n",
    "    is_teacher=True\n",
    ")\n",
    "teacher_bench['role'] = 'teacher'\n",
    "benchmark_results.append(teacher_bench)\n",
    "\n",
    "# Benchmark students\n",
    "for model_key, model in student_models.items():\n",
    "    student_bench = benchmark_model(\n",
    "        model, benchmark_loader,\n",
    "        model_key,\n",
    "        is_teacher=False\n",
    "    )\n",
    "    student_bench['role'] = 'student'\n",
    "    benchmark_results.append(student_bench)\n",
    "\n",
    "# Create results DataFrame\n",
    "benchmark_df = pd.DataFrame(benchmark_results)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(benchmark_df[['model', 'latency_per_sample_ms', 'throughput_samples_per_sec']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speedup ratios\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SPEEDUP ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "teacher_latency = benchmark_df[benchmark_df['role'] == 'teacher']['latency_per_sample_ms'].values[0]\n",
    "\n",
    "speedup_results = []\n",
    "for _, row in benchmark_df.iterrows():\n",
    "    speedup = teacher_latency / row['latency_per_sample_ms']\n",
    "    speedup_results.append({\n",
    "        'Model': row['model'],\n",
    "        'Latency (ms/sample)': f\"{row['latency_per_sample_ms']:.1f}\",\n",
    "        'Throughput (samples/s)': f\"{row['throughput_samples_per_sec']:.2f}\",\n",
    "        'Speedup vs Teacher': f\"{speedup:.2f}x\",\n",
    "        'speedup_numeric': speedup\n",
    "    })\n",
    "\n",
    "speedup_df = pd.DataFrame(speedup_results)\n",
    "print(\"\\n\" + speedup_df[['Model', 'Latency (ms/sample)', 'Throughput (samples/s)', 'Speedup vs Teacher']].to_string(index=False))\n",
    "\n",
    "# Save benchmark results\n",
    "benchmark_df.to_csv(OUTPUT_DIR / \"inference_benchmark.csv\", index=False)\n",
    "speedup_df.to_csv(OUTPUT_DIR / \"speedup_analysis.csv\", index=False)\n",
    "print(f\"\\nâœ“ Results saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inference speed comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "models = [r['Model'].replace('flan-t5-', '').replace(' (Teacher)', '\\n(Teacher)') for r in speedup_results]\n",
    "latencies = [float(r['Latency (ms/sample)']) for r in speedup_results]\n",
    "throughputs = [float(r['Throughput (samples/s)']) for r in speedup_results]\n",
    "speedups = [r['speedup_numeric'] for r in speedup_results]\n",
    "\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']  # Teacher=red, students=blue/green\n",
    "\n",
    "# Plot 1: Latency\n",
    "bars1 = axes[0].bar(models, latencies, color=colors[:len(models)], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Latency (ms/sample)')\n",
    "axes[0].set_title('Inference Latency\\n(lower is better)')\n",
    "for bar, val in zip(bars1, latencies):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                 f'{val:.1f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Throughput\n",
    "bars2 = axes[1].bar(models, throughputs, color=colors[:len(models)], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Throughput (samples/sec)')\n",
    "axes[1].set_title('Inference Throughput\\n(higher is better)')\n",
    "for bar, val in zip(bars2, throughputs):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                 f'{val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Speedup\n",
    "bars3 = axes[2].bar(models, speedups, color=colors[:len(models)], alpha=0.7, edgecolor='black')\n",
    "axes[2].set_ylabel('Speedup (x)')\n",
    "axes[2].set_title('Speedup vs Teacher\\n(higher is better)')\n",
    "axes[2].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "for bar, val in zip(bars3, speedups):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                 f'{val:.2f}x', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Inference Speed Comparison: Teacher vs Student Models', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c3124",
   "metadata": {},
   "source": [
    "## 4. Performance vs Efficiency Trade-off Analysis\n",
    "\n",
    "Evaluate how much performance is retained as model size decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained student model (if available)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING TRAINED STUDENT MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "checkpoint_dir = Path(\"../experiments/distillation\")\n",
    "model_zip = checkpoint_dir / \"best_model.zip\"\n",
    "model_dir = checkpoint_dir / \"best_model\"\n",
    "\n",
    "trained_student = None\n",
    "\n",
    "# Extract if needed\n",
    "if model_zip.exists() and not model_dir.exists():\n",
    "    print(f\"Extracting model from {model_zip}...\")\n",
    "    with zipfile.ZipFile(model_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(checkpoint_dir)\n",
    "    print(\"âœ“ Extracted successfully\")\n",
    "\n",
    "# Load trained model\n",
    "if model_dir.exists():\n",
    "    trained_config = StudentConfig(\n",
    "        model_name=\"google/flan-t5-small\",\n",
    "        max_source_length=128,\n",
    "        max_target_length=64,\n",
    "        device=device\n",
    "    )\n",
    "    trained_student = StudentModel.load_model(str(model_dir), trained_config)\n",
    "    print(f\"\\nâœ“ Trained student loaded from: {model_dir}\")\n",
    "    print(f\"  Parameters: {trained_student.count_parameters():,}\")\n",
    "else:\n",
    "    print(f\"\\nâš  Trained model not found at {model_dir}\")\n",
    "    print(\"  Using untrained student for demonstration\")\n",
    "    trained_student = student_models['flan-t5-small']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on test data\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATING MODELS FOR PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use validation data for evaluation\n",
    "eval_data = esnli_data['validation'].select(range(200))  # 200 samples for quick eval\n",
    "eval_dataset = ESNLIDataset(eval_data, preprocessor, use_cache=True)\n",
    "eval_loader = create_dataloaders(\n",
    "    eval_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    pad_token_id=preprocessor.tokenizer.pad_token_id,\n",
    "    shuffle_train=False\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Evaluation samples: {len(eval_data)}\")\n",
    "print(f\"âœ“ Evaluation batches: {len(eval_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85141a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, model_name):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    eval_config = EvaluationConfig(\n",
    "        metrics_config=MetricsConfig(\n",
    "            compute_rouge=True,\n",
    "            compute_bertscore=False,  # Skip for speed\n",
    "            compute_faithfulness=True\n",
    "        ),\n",
    "        num_beams=4,\n",
    "        max_length=64,\n",
    "        save_predictions=False,\n",
    "        output_dir=str(OUTPUT_DIR / f\"eval_{model_name.replace(' ', '_')}\")\n",
    "    )\n",
    "    \n",
    "    evaluator = Evaluator(model, eval_config)\n",
    "    results = evaluator.evaluate(dataloader, split_name=\"val\")\n",
    "    \n",
    "    metrics = results['metrics']\n",
    "    print(f\"  âœ“ Accuracy: {metrics['label_accuracy']:.4f}\")\n",
    "    print(f\"  âœ“ ROUGE-L: {metrics['rougeL']:.4f}\")\n",
    "    print(f\"  âœ“ Faithfulness: {metrics['faithfulness']:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "print(\"=\" * 70)\n",
    "print(\"PERFORMANCE EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "# Evaluate trained student (distilled)\n",
    "if trained_student is not None:\n",
    "    trained_metrics = evaluate_model(trained_student, eval_loader, \"flan-t5-small (Distilled)\")\n",
    "    performance_results.append({\n",
    "        'model': 'flan-t5-small (Distilled)',\n",
    "        'parameters': trained_student.count_parameters(),\n",
    "        'type': 'distilled',\n",
    "        **trained_metrics\n",
    "    })\n",
    "\n",
    "# Evaluate untrained students (baseline)\n",
    "for model_key, model in student_models.items():\n",
    "    if model_key == 'flan-t5-small' and trained_student is not None:\n",
    "        continue  # Already evaluated distilled version\n",
    "    \n",
    "    metrics = evaluate_model(model, eval_loader, f\"{model_key} (Baseline)\")\n",
    "    performance_results.append({\n",
    "        'model': f\"{model_key} (Baseline)\",\n",
    "        'parameters': model.count_parameters(),\n",
    "        'type': 'baseline',\n",
    "        **metrics\n",
    "    })\n",
    "\n",
    "# Create performance DataFrame\n",
    "performance_df = pd.DataFrame(performance_results)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(performance_df[['model', 'label_accuracy', 'rougeL', 'faithfulness']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance retention\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE RETENTION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use baseline flan-t5-base as reference (larger untrained model)\n",
    "reference_model = 'flan-t5-base (Baseline)'\n",
    "reference_row = performance_df[performance_df['model'] == reference_model]\n",
    "\n",
    "if len(reference_row) > 0:\n",
    "    ref_accuracy = reference_row['label_accuracy'].values[0]\n",
    "    ref_rouge = reference_row['rougeL'].values[0]\n",
    "    ref_params = reference_row['parameters'].values[0]\n",
    "    \n",
    "    retention_results = []\n",
    "    for _, row in performance_df.iterrows():\n",
    "        acc_retention = (row['label_accuracy'] / ref_accuracy) * 100 if ref_accuracy > 0 else 0\n",
    "        rouge_retention = (row['rougeL'] / ref_rouge) * 100 if ref_rouge > 0 else 0\n",
    "        compression = ref_params / row['parameters']\n",
    "        \n",
    "        retention_results.append({\n",
    "            'Model': row['model'],\n",
    "            'Parameters': f\"{row['parameters']/1e6:.1f}M\",\n",
    "            'Compression': f\"{compression:.2f}x\",\n",
    "            'Accuracy': f\"{row['label_accuracy']:.4f}\",\n",
    "            'Acc. Retention': f\"{acc_retention:.1f}%\",\n",
    "            'ROUGE-L': f\"{row['rougeL']:.4f}\",\n",
    "            'ROUGE Retention': f\"{rouge_retention:.1f}%\"\n",
    "        })\n",
    "    \n",
    "    retention_df = pd.DataFrame(retention_results)\n",
    "    print(f\"\\nReference model: {reference_model}\")\n",
    "    print(retention_df.to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    retention_df.to_csv(OUTPUT_DIR / \"performance_retention.csv\", index=False)\n",
    "    print(f\"\\nâœ“ Saved to {OUTPUT_DIR / 'performance_retention.csv'}\")\n",
    "else:\n",
    "    print(\"Reference model not found in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733753a1",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Trade-off Visualization\n",
    "\n",
    "Create publication-ready figures showing compression vs performance trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ad8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive trade-off visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Prepare data\n",
    "models_plot = performance_df['model'].values\n",
    "params_plot = performance_df['parameters'].values / 1e6  # In millions\n",
    "accuracy_plot = performance_df['label_accuracy'].values\n",
    "rouge_plot = performance_df['rougeL'].values\n",
    "faithfulness_plot = performance_df['faithfulness'].values\n",
    "\n",
    "# Colors based on model type\n",
    "colors_plot = ['#2ecc71' if 'Distilled' in m else '#3498db' for m in models_plot]\n",
    "\n",
    "# Plot 1: Parameters vs Accuracy\n",
    "ax1 = axes[0, 0]\n",
    "scatter1 = ax1.scatter(params_plot, accuracy_plot, c=colors_plot, s=200, alpha=0.7, edgecolors='black')\n",
    "for i, model in enumerate(models_plot):\n",
    "    ax1.annotate(model.replace('flan-t5-', '').replace(' (Baseline)', '\\n(Base)').replace(' (Distilled)', '\\n(Dist)'),\n",
    "                 (params_plot[i], accuracy_plot[i]),\n",
    "                 textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=9)\n",
    "ax1.set_xlabel('Parameters (Millions)')\n",
    "ax1.set_ylabel('Label Accuracy')\n",
    "ax1.set_title('Model Size vs Label Accuracy')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameters vs ROUGE-L\n",
    "ax2 = axes[0, 1]\n",
    "scatter2 = ax2.scatter(params_plot, rouge_plot, c=colors_plot, s=200, alpha=0.7, edgecolors='black')\n",
    "for i, model in enumerate(models_plot):\n",
    "    ax2.annotate(model.replace('flan-t5-', '').replace(' (Baseline)', '\\n(Base)').replace(' (Distilled)', '\\n(Dist)'),\n",
    "                 (params_plot[i], rouge_plot[i]),\n",
    "                 textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=9)\n",
    "ax2.set_xlabel('Parameters (Millions)')\n",
    "ax2.set_ylabel('ROUGE-L Score')\n",
    "ax2.set_title('Model Size vs Explanation Quality (ROUGE-L)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Compression Ratio vs Performance (bar chart)\n",
    "ax3 = axes[1, 0]\n",
    "model_labels = [m.replace('flan-t5-', '').replace(' (Baseline)', '\\n(Base)').replace(' (Distilled)', '\\n(Dist)') \n",
    "                for m in models_plot]\n",
    "x = np.arange(len(model_labels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, accuracy_plot, width, label='Accuracy', color='#3498db', alpha=0.7)\n",
    "bars2 = ax3.bar(x + width/2, rouge_plot, width, label='ROUGE-L', color='#e74c3c', alpha=0.7)\n",
    "\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title('Model Performance Comparison')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(model_labels)\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1.0)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Efficiency Summary\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Combine compression and speedup data\n",
    "efficiency_data = []\n",
    "for i, model in enumerate(models_plot):\n",
    "    model_key = model.replace(' (Baseline)', '').replace(' (Distilled)', '')\n",
    "    \n",
    "    # Find matching speedup data\n",
    "    speedup_match = [r for r in speedup_results if model_key in r['Model']]\n",
    "    if speedup_match:\n",
    "        speedup = speedup_match[0]['speedup_numeric']\n",
    "    else:\n",
    "        speedup = 1.0\n",
    "    \n",
    "    # Find matching compression data\n",
    "    compression_match = [r for r in compression_results if model_key in r['Student Model']]\n",
    "    if compression_match:\n",
    "        compression = compression_match[0]['param_compression_numeric']\n",
    "    else:\n",
    "        compression = 1.0\n",
    "    \n",
    "    efficiency_data.append({\n",
    "        'model': model_labels[i],\n",
    "        'speedup': speedup,\n",
    "        'compression': compression,\n",
    "        'accuracy': accuracy_plot[i]\n",
    "    })\n",
    "\n",
    "# Scatter: Speedup vs Accuracy (bubble size = compression)\n",
    "speedups_eff = [d['speedup'] for d in efficiency_data]\n",
    "accuracies_eff = [d['accuracy'] for d in efficiency_data]\n",
    "compressions_eff = [d['compression'] * 20 for d in efficiency_data]  # Scale for visibility\n",
    "\n",
    "scatter4 = ax4.scatter(speedups_eff, accuracies_eff, s=compressions_eff, \n",
    "                       c=colors_plot, alpha=0.7, edgecolors='black')\n",
    "for i, d in enumerate(efficiency_data):\n",
    "    ax4.annotate(d['model'], (d['speedup'], d['accuracy']),\n",
    "                 textcoords=\"offset points\", xytext=(0,15), ha='center', fontsize=9)\n",
    "\n",
    "ax4.set_xlabel('Speedup vs Teacher (x)')\n",
    "ax4.set_ylabel('Label Accuracy')\n",
    "ax4.set_title('Efficiency vs Performance\\n(bubble size = compression ratio)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.axhline(y=np.mean(accuracies_eff), color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', edgecolor='black', alpha=0.7, label='Distilled Student'),\n",
    "    Patch(facecolor='#3498db', edgecolor='black', alpha=0.7, label='Baseline Model')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "plt.suptitle('Compression Analysis: Model Size vs Performance Trade-offs', \n",
    "             fontsize=16, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19ed50",
   "metadata": {},
   "source": [
    "## 6. Summary Report\n",
    "\n",
    "Generate a comprehensive summary of all compression analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19fbbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPRESSION ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'device': device,\n",
    "    'teacher_model': {\n",
    "        'name': 'google/flan-t5-xl',\n",
    "        'parameters': teacher_params,\n",
    "        'memory_mb': teacher_memory_mb\n",
    "    },\n",
    "    'student_models': [],\n",
    "    'key_findings': []\n",
    "}\n",
    "\n",
    "# Add student model details\n",
    "for stats in model_stats:\n",
    "    model_name = stats['model']\n",
    "    \n",
    "    # Find compression ratio\n",
    "    comp_match = [r for r in compression_results if model_name in r['Student Model']]\n",
    "    comp_ratio = comp_match[0]['param_compression_numeric'] if comp_match else 1.0\n",
    "    \n",
    "    # Find speedup\n",
    "    speed_match = [r for r in speedup_results if model_name in r['Model']]\n",
    "    speedup = speed_match[0]['speedup_numeric'] if speed_match else 1.0\n",
    "    \n",
    "    summary['student_models'].append({\n",
    "        'name': model_name,\n",
    "        'parameters': stats['parameters'],\n",
    "        'memory_mb': stats['memory_mb'],\n",
    "        'compression_ratio': comp_ratio,\n",
    "        'speedup': speedup\n",
    "    })\n",
    "\n",
    "# Key findings\n",
    "best_compression = max(compression_results, key=lambda x: x['param_compression_numeric'])\n",
    "best_speedup = max(speedup_results, key=lambda x: x['speedup_numeric'])\n",
    "\n",
    "summary['key_findings'] = [\n",
    "    f\"Best compression: {best_compression['Student Model']} achieves {best_compression['Compression Ratio']} parameter reduction\",\n",
    "    f\"Best speedup: {best_speedup['Model']} achieves {best_speedup['Speedup vs Teacher']} faster inference\",\n",
    "    f\"Teacher model: {teacher_params/1e9:.2f}B parameters, {teacher_memory_mb/1024:.2f}GB memory\",\n",
    "]\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nðŸ“Š COMPRESSION RATIOS:\")\n",
    "for student in summary['student_models']:\n",
    "    print(f\"   â€¢ {student['name']}: {student['compression_ratio']:.1f}x smaller than teacher\")\n",
    "\n",
    "print(f\"\\nâš¡ SPEEDUP:\")\n",
    "for student in summary['student_models']:\n",
    "    print(f\"   â€¢ {student['name']}: {student['speedup']:.2f}x faster than teacher\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ MEMORY SAVINGS:\")\n",
    "for student in summary['student_models']:\n",
    "    memory_ratio = teacher_memory_mb / student['memory_mb']\n",
    "    print(f\"   â€¢ {student['name']}: {memory_ratio:.1f}x less memory ({student['memory_mb']:.0f}MB vs {teacher_memory_mb:.0f}MB)\")\n",
    "\n",
    "print(f\"\\nðŸ”‘ KEY FINDINGS:\")\n",
    "for finding in summary['key_findings']:\n",
    "    print(f\"   â€¢ {finding}\")\n",
    "\n",
    "# Save summary as JSON\n",
    "with open(OUTPUT_DIR / \"compression_summary.json\", 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nâœ“ Summary saved to {OUTPUT_DIR / 'compression_summary.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e76a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-ready table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PUBLICATION-READY TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pub_table = []\n",
    "\n",
    "# Add teacher row\n",
    "pub_table.append({\n",
    "    'Model': 'FLAN-T5-XL (Teacher)',\n",
    "    'Params (M)': f\"{teacher_params/1e6:.0f}\",\n",
    "    'Memory (MB)': f\"{teacher_memory_mb:.0f}\",\n",
    "    'Compression': '1.0x (reference)',\n",
    "    'Speedup': '1.0x (reference)',\n",
    "    'Role': 'Teacher'\n",
    "})\n",
    "\n",
    "# Add student rows\n",
    "for student in summary['student_models']:\n",
    "    pub_table.append({\n",
    "        'Model': student['name'].replace('flan-t5-', 'FLAN-T5-').title(),\n",
    "        'Params (M)': f\"{student['parameters']/1e6:.0f}\",\n",
    "        'Memory (MB)': f\"{student['memory_mb']:.0f}\",\n",
    "        'Compression': f\"{student['compression_ratio']:.1f}x\",\n",
    "        'Speedup': f\"{student['speedup']:.2f}x\",\n",
    "        'Role': 'Student'\n",
    "    })\n",
    "\n",
    "pub_df = pd.DataFrame(pub_table)\n",
    "print(\"\\n\" + pub_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPRESSION ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY TAKEAWAYS FOR PAPER:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "1. FLAN-T5-small achieves ~{summary['student_models'][0]['compression_ratio']:.0f}x compression\n",
    "   with {summary['student_models'][0]['speedup']:.1f}x speedup vs FLAN-T5-XL teacher.\n",
    "\n",
    "2. Memory reduction: {teacher_memory_mb/summary['student_models'][0]['memory_mb']:.0f}x smaller footprint\n",
    "   ({summary['student_models'][0]['memory_mb']:.0f}MB vs {teacher_memory_mb:.0f}MB).\n",
    "\n",
    "3. Knowledge distillation enables efficient deployment while\n",
    "   retaining reasoning capabilities from the larger teacher.\n",
    "\n",
    "4. Trade-off: Smaller models sacrifice some accuracy for significant\n",
    "   efficiency gains, making them suitable for resource-constrained deployment.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
