{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21104051",
   "metadata": {},
   "source": [
    "# Baseline Training (No Distillation)\n",
    "## Reasoning Distillation Project\n",
    "\n",
    "This notebook trains the **same student model WITHOUT knowledge distillation** to serve as a baseline for comparison.\n",
    "\n",
    "### Training Setup:\n",
    "```\n",
    "Loss = α·CE(student, labels) + β·KL(student||teacher)\n",
    "     = 1.0·CE + 0.0·KL\n",
    "     = CE (standard supervised learning)\n",
    "```\n",
    "\n",
    "### Purpose:\n",
    "- Train student model with **β = 0.0** (no teacher guidance)\n",
    "- Use **identical hyperparameters** as notebook 06 (distillation training)\n",
    "- Provide a **fair baseline** to measure the value of knowledge distillation\n",
    "\n",
    "### Key Differences from Notebook 06:\n",
    "| Parameter | Notebook 06 (Distillation) | This Notebook (Baseline) |\n",
    "|-----------|---------------------------|-------------------------|\n",
    "| β (distill_weight) | 0.5 | **0.0** |\n",
    "| Teacher guidance | Yes | **No** |\n",
    "| Loss function | CE + KL | **CE only** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "from src.data.data_loader import TeacherDataLoader\n",
    "from src.data.preprocessor import ReasoningPreprocessor, PreprocessConfig\n",
    "from src.data.dataset import ESNLIDataset, create_dataloaders\n",
    "\n",
    "from src.models.student import StudentModel, StudentConfig\n",
    "from src.models.teacher import FlanT5Teacher, TeacherConfig\n",
    "\n",
    "from src.training.distillation import (\n",
    "    DistillationConfig,\n",
    "    TokenLevelDistillation,\n",
    ")\n",
    "\n",
    "from src.training.trainer import Trainer, TrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f81d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6aa73",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "Use the **same dataset configuration** as notebook 06 for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8621bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DATASET FOR BASELINE TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "loader = TeacherDataLoader()\n",
    "esnli_data = loader.load_esnli()\n",
    "\n",
    "train_subset = esnli_data['train'].select(range(50000))\n",
    "val_subset = esnli_data['validation'].select(range(5000))\n",
    "\n",
    "print(f\"\\n✓ Train samples: {len(train_subset)}\")\n",
    "print(f\"✓ Val samples: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "preprocess_config = PreprocessConfig(\n",
    "    model_name=\"google/flan-t5-small\",\n",
    "    max_source_length=128,\n",
    "    max_target_length=64\n",
    ")\n",
    "\n",
    "preprocessor = ReasoningPreprocessor(preprocess_config)\n",
    "\n",
    "train_dataset = ESNLIDataset(train_subset, preprocessor, use_cache=True)\n",
    "val_dataset = ESNLIDataset(val_subset, preprocessor, use_cache=True)\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    pad_token_id=preprocessor.tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3da8d3",
   "metadata": {},
   "source": [
    "## 2. Load Teacher Model\n",
    "\n",
    "We still need to load the teacher model for the distillation interface, but with **β = 0.0**, the KL loss will be multiplied by zero, effectively disabling teacher guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Teacher Model\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING TEACHER MODEL (for interface only, β=0.0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "teacher_config = TeacherConfig(\n",
    "    model_name=\"google/flan-t5-xl\",\n",
    "    device=device,\n",
    "    use_fp16=True if device == \"cuda\" else False,\n",
    "    max_source_length=128,\n",
    "    max_target_length=64\n",
    ")\n",
    "\n",
    "print(f\"Loading {teacher_config.model_name}...\")\n",
    "teacher = FlanT5Teacher(teacher_config)\n",
    "\n",
    "print(f\"\\n✓ Teacher loaded (will NOT be used for training, β=0.0)\")\n",
    "print(f\"  Parameters: {teacher.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acfb50",
   "metadata": {},
   "source": [
    "## 3. Initialize Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda10689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Student Model - SAME config as notebook 06\n",
    "print(\"=\" * 70)\n",
    "print(\"INITIALIZING STUDENT MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "student_config = StudentConfig(\n",
    "    model_name=\"google/flan-t5-small\",\n",
    "    max_source_length=128,\n",
    "    max_target_length=64,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "student = StudentModel(student_config)\n",
    "\n",
    "print(f\"\\n✓ Student loaded!\")\n",
    "print(f\"  Parameters: {student.count_parameters():,}\")\n",
    "print(f\"  Memory: {student.get_memory_footprint()['total_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02763f92",
   "metadata": {},
   "source": [
    "## 4. Configure Baseline Training (β = 0.0)\n",
    "\n",
    "**KEY DIFFERENCE**: `distill_weight = 0.0` means NO knowledge distillation.\n",
    "\n",
    "```\n",
    "Loss = 1.0·CE + 0.0·KL = CE (standard supervised learning)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd921cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Baseline Training (NO DISTILLATION)\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURING BASELINE TRAINING (β = 0.0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# BASELINE configuration: β = 0.0 (NO teacher guidance)\n",
    "baseline_distill_config = DistillationConfig(\n",
    "    ce_weight=1.0,        # α - Cross-entropy weight\n",
    "    distill_weight=0.0,   # β = 0.0 → NO KL divergence loss!\n",
    "    temperature=2.0,      # Not used when β=0.0\n",
    "    label_smoothing=0.0,\n",
    "    distillation_type=\"token_level\"\n",
    ")\n",
    "\n",
    "# Create distillation strategy (KL will be multiplied by 0)\n",
    "baseline_strategy = TokenLevelDistillation(\n",
    "    teacher_model=teacher,\n",
    "    config=baseline_distill_config\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Baseline Training configured!\")\n",
    "print(f\"  Loss = {baseline_distill_config.ce_weight}·CE + {baseline_distill_config.distill_weight}·KL\")\n",
    "print(f\"  Loss = CE only (standard supervised learning)\")\n",
    "print(f\"  ⚠️  Teacher guidance: DISABLED (β=0.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb5aec",
   "metadata": {},
   "source": [
    "## 5. Initialize Trainer\n",
    "\n",
    "Use **IDENTICAL training hyperparameters** as notebook 06 for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training configuration - SAME as notebook 06\n",
    "print(\"=\" * 70)\n",
    "print(\"INITIALIZING TRAINER (SAME config as distillation training)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    num_epochs=7,                          # SAME\n",
    "    learning_rate=5e-5,                    # SAME\n",
    "    warmup_steps=1200,                     # SAME\n",
    "    eval_steps=1000,                       # SAME\n",
    "    save_steps=1000,                       # SAME\n",
    "    logging_steps=50,                      # SAME\n",
    "    output_dir=\"../experiments/baseline\",  # DIFFERENT: separate output dir\n",
    "    eval_strategy=\"steps\",                 # SAME\n",
    "    save_strategy=\"steps\",                 # SAME\n",
    "    save_total_limit=3,                    # SAME\n",
    "    early_stopping_patience=5,             # SAME\n",
    "    early_stopping_threshold=0.001,        # SAME\n",
    "    lr_scheduler_type=\"cosine\",            # SAME\n",
    "    gradient_accumulation_steps=2,         # SAME\n",
    "    max_grad_norm=1.0,                     # SAME\n",
    "    fp16=False,                            # SAME\n",
    "    seed=42                                # SAME seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create trainer with baseline strategy\n",
    "trainer = Trainer(\n",
    "    model=student,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=val_loader,\n",
    "    distillation_strategy=baseline_strategy,\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Trainer initialized!\")\n",
    "print(f\"  Total training steps: {len(train_loader) * training_config.num_epochs}\")\n",
    "print(f\"  Training type: BASELINE (no distillation)\")\n",
    "print(f\"  Loss: CE only (β=0.0)\")\n",
    "print(f\"  Output: {training_config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b85753b",
   "metadata": {},
   "source": [
    "## 6. Run Baseline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline Model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING BASELINE TRAINING (NO DISTILLATION)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nStudent: {student_config.model_name}\")\n",
    "print(f\"Loss: {baseline_distill_config.ce_weight}·CE + {baseline_distill_config.distill_weight}·KL\")\n",
    "print(f\"Loss: CE only (standard supervised learning)\")\n",
    "print(f\"Teacher guidance: DISABLED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = trainer.train()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Baseline training completed in {training_time/60:.1f} minutes\")\n",
    "print(f\"✓ Average time per epoch: {training_time / training_config.num_epochs:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccbec09",
   "metadata": {},
   "source": [
    "## 7. Analyze Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training history\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE TRAINING HISTORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_history = history['train_history']\n",
    "eval_history = history['eval_history']\n",
    "\n",
    "print(f\"\\nTrain history ({len(train_history)} epochs):\")\n",
    "for i, metrics in enumerate(train_history):\n",
    "    print(f\"  Epoch {i+1}: loss={metrics['loss']:.4f}\")\n",
    "\n",
    "print(f\"\\nEval history ({len(eval_history)} evaluations):\")\n",
    "for i, metrics in enumerate(eval_history[:5]):\n",
    "    print(f\"  Eval {i+1}: eval_loss={metrics['eval_loss']:.4f}\")\n",
    "if len(eval_history) > 5:\n",
    "    print(f\"  ... and {len(eval_history) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training loss per epoch\n",
    "epochs = range(1, len(train_history) + 1)\n",
    "train_losses = [m['loss'] for m in train_history]\n",
    "\n",
    "axes[0].plot(epochs, train_losses, marker='o', linewidth=2, markersize=8, \n",
    "             color='#e74c3c', label='Train Loss (Baseline)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Baseline Training Loss (β=0.0, No Distillation)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Evaluation loss over time\n",
    "if eval_history:\n",
    "    eval_steps = range(1, len(eval_history) + 1)\n",
    "    eval_losses = [m['eval_loss'] for m in eval_history]\n",
    "    \n",
    "    axes[1].plot(eval_steps, eval_losses, marker='s', linewidth=2, markersize=8,\n",
    "                 color='#3498db', label='Eval Loss (Baseline)')\n",
    "    axes[1].set_xlabel('Evaluation Step')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Baseline Evaluation Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Baseline Training (No Knowledge Distillation)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print loss reduction\n",
    "if len(train_losses) > 1:\n",
    "    initial_loss = train_losses[0]\n",
    "    final_loss = train_losses[-1]\n",
    "    reduction = (initial_loss - final_loss) / initial_loss * 100\n",
    "    print(f\"\\nLoss reduction: {reduction:.2f}%\")\n",
    "    print(f\"Initial loss: {initial_loss:.4f}\")\n",
    "    print(f\"Final loss: {final_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e4d14",
   "metadata": {},
   "source": [
    "## 8. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation after baseline training\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING GENERATION (BASELINE MODEL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "student.model.eval()\n",
    "\n",
    "# Get a batch from validation set\n",
    "val_batch = next(iter(val_loader))\n",
    "val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    generated_ids = student.generate(\n",
    "        input_ids=val_batch['input_ids'][:3],\n",
    "        attention_mask=val_batch['attention_mask'][:3],\n",
    "        max_length=64,\n",
    "        num_beams=4\n",
    "    )\n",
    "\n",
    "# Decode\n",
    "predictions = student.decode_batch(generated_ids)\n",
    "inputs = student.decode_batch(val_batch['input_ids'][:3])\n",
    "\n",
    "labels = val_batch['labels'][:3].clone()\n",
    "labels[labels == -100] = student.tokenizer.pad_token_id\n",
    "ground_truths = student.decode_batch(labels)\n",
    "\n",
    "# Display\n",
    "for i in range(3):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SAMPLE {i+1}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nInput:\\n{inputs[i]}\")\n",
    "    print(f\"\\nGround Truth:\\n{ground_truths[i]}\")\n",
    "    print(f\"\\nBaseline Prediction:\\n{predictions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373f987",
   "metadata": {},
   "source": [
    "## 9. Save Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check saved checkpoints\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVED BASELINE CHECKPOINTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "output_dir = Path(training_config.output_dir)\n",
    "\n",
    "if output_dir.exists():\n",
    "    checkpoints = list(output_dir.iterdir())\n",
    "    print(f\"\\nFound {len(checkpoints)} items in output directory:\")\n",
    "    for checkpoint in sorted(checkpoints):\n",
    "        if checkpoint.is_dir():\n",
    "            size = sum(f.stat().st_size for f in checkpoint.rglob('*') if f.is_file())\n",
    "            print(f\"- {checkpoint.name} ({size / 1e6:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"- {checkpoint.name}\")\n",
    "else:\n",
    "    print(\"\\nOutput directory not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8194a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training summary for comparison\n",
    "import json\n",
    "\n",
    "summary = {\n",
    "    'model_type': 'baseline',\n",
    "    'distill_weight': 0.0,\n",
    "    'ce_weight': 1.0,\n",
    "    'description': 'No knowledge distillation (standard supervised learning)',\n",
    "    'training_config': {\n",
    "        'num_epochs': training_config.num_epochs,\n",
    "        'learning_rate': training_config.learning_rate,\n",
    "        'batch_size': 32,\n",
    "        'gradient_accumulation_steps': training_config.gradient_accumulation_steps,\n",
    "        'seed': training_config.seed\n",
    "    },\n",
    "    'final_train_loss': train_history[-1]['loss'] if train_history else None,\n",
    "    'final_eval_loss': eval_history[-1]['eval_loss'] if eval_history else None,\n",
    "    'training_time_minutes': training_time / 60\n",
    "}\n",
    "\n",
    "summary_path = output_dir / 'training_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Training summary saved to: {summary_path}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2302439",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BASELINE TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n✓ Model saved to: {training_config.output_dir}/best_model\")\n",
    "print(f\"✓ Training type: Baseline (β=0.0, no distillation)\")\n",
    "print(f\"✓ Training time: {training_time/60:.1f} minutes\")\n",
    "print(f\"\\n→ Now run notebook 07 to compare with distilled model!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
