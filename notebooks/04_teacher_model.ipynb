{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3f1d42",
   "metadata": {},
   "source": [
    "# 04 - Teacher Model Testing\n",
    "\n",
    "This notebook tests the teacher model (FLAN-T5-XL) for knowledge distillation.\n",
    "\n",
    "## Objectives\n",
    "1. Load and test the teacher model `google/flan-t5-xl`\n",
    "2. Verify soft logits (probabilities) generation\n",
    "3. Test teacher inference on NLI examples\n",
    "4. Verify teacher-student integration for distillation\n",
    "\n",
    "## Distillation Flow\n",
    "```\n",
    "Dataset → Teacher Model → Soft Logits (probabilities)\n",
    "       ↘                ↗\n",
    "         Student Model\n",
    "         \n",
    "Loss = α·CE(student, labels) + β·KL(student||teacher)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03e788",
   "metadata": {},
   "source": [
    "## 1. Caricamento del Teacher Model\n",
    "\n",
    "Carichiamo FLAN-T5-XL come teacher. Il modello ha ~3B di parametri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cc645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.teacher import FlanT5Teacher, TeacherConfig\n",
    "\n",
    "# Configurazione del teacher\n",
    "teacher_config = TeacherConfig(\n",
    "    model_name=\"google/flan-t5-xl\",\n",
    "    device=device,\n",
    "    use_fp16=True if device == \"cuda\" else False\n",
    ")\n",
    "\n",
    "print(\"Caricamento del teacher model...\")\n",
    "print(f\"Model: {teacher_config.model_name}\")\n",
    "print(f\"Device: {teacher_config.device}\")\n",
    "print(f\"FP16: {teacher_config.use_fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il teacher model\n",
    "teacher = FlanT5Teacher(teacher_config)\n",
    "\n",
    "print(f\"\\nTeacher model caricato!\")\n",
    "print(f\"Parametri: {teacher.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236cc2a",
   "metadata": {},
   "source": [
    "## 2. Test Inferenza del Teacher\n",
    "\n",
    "Testiamo il teacher su alcuni esempi NLI per verificare che funzioni correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempi di test NLI\n",
    "test_examples = [\n",
    "    {\n",
    "        \"premise\": \"A man is playing guitar on stage.\",\n",
    "        \"hypothesis\": \"A person is performing music.\",\n",
    "        \"expected_label\": \"entailment\"\n",
    "    },\n",
    "    {\n",
    "        \"premise\": \"The cat is sleeping on the couch.\",\n",
    "        \"hypothesis\": \"The cat is running outside.\",\n",
    "        \"expected_label\": \"contradiction\"\n",
    "    },\n",
    "    {\n",
    "        \"premise\": \"Children are playing in the park.\",\n",
    "        \"hypothesis\": \"The children are wearing hats.\",\n",
    "        \"expected_label\": \"neutral\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Formatta come prompt NLI\n",
    "def format_nli_prompt(premise, hypothesis):\n",
    "    return f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nDoes the premise entail, contradict, or is neutral to the hypothesis? Explain your reasoning.\"\n",
    "\n",
    "for ex in test_examples:\n",
    "    prompt = format_nli_prompt(ex[\"premise\"], ex[\"hypothesis\"])\n",
    "    print(f\"Prompt: {prompt[:100]}...\")\n",
    "    print(f\"Expected: {ex['expected_label']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generazione del teacher\n",
    "tokenizer = teacher.tokenizer\n",
    "\n",
    "for i, ex in enumerate(test_examples):\n",
    "    prompt = format_nli_prompt(ex[\"premise\"], ex[\"hypothesis\"])\n",
    "    \n",
    "    # Tokenizza\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Genera risposta\n",
    "    outputs = teacher.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n=== Esempio {i+1} ===\")\n",
    "    print(f\"Premise: {ex['premise']}\")\n",
    "    print(f\"Hypothesis: {ex['hypothesis']}\")\n",
    "    print(f\"Expected: {ex['expected_label']}\")\n",
    "    print(f\"Teacher response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b98632",
   "metadata": {},
   "source": [
    "## 3. Soft Logits del Teacher\n",
    "\n",
    "Verifichiamo la generazione di soft logits (distribuzioni di probabilità) che lo student dovrà imitare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test soft logits\n",
    "ex = test_examples[0]\n",
    "prompt = format_nli_prompt(ex[\"premise\"], ex[\"hypothesis\"])\n",
    "target = \"entailment. The man playing guitar is performing music.\"\n",
    "\n",
    "# Tokenizza input e target\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "targets = tokenizer(\n",
    "    target,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "# Prepara decoder_input_ids (shift right)\n",
    "decoder_input_ids = targets[\"input_ids\"].clone()\n",
    "decoder_input_ids[:, 1:] = targets[\"input_ids\"][:, :-1].clone()\n",
    "decoder_input_ids[:, 0] = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Decoder input shape: {decoder_input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ca0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ottieni soft logits dal teacher\n",
    "with torch.no_grad():\n",
    "    teacher_outputs = teacher(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        decoder_input_ids=decoder_input_ids\n",
    "    )\n",
    "\n",
    "teacher_logits = teacher_outputs[\"logits\"]\n",
    "print(f\"Teacher logits shape: {teacher_logits.shape}\")\n",
    "print(f\"Vocabulary size: {teacher_logits.size(-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti logits in probabilità soft\n",
    "temperature = 2.0  # Temperature per soft labels\n",
    "soft_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "\n",
    "print(f\"Soft probabilities shape: {soft_probs.shape}\")\n",
    "print(f\"\\nPer il primo token:\")\n",
    "print(f\"  Max probability: {soft_probs[0, 0].max().item():.4f}\")\n",
    "print(f\"  Min probability: {soft_probs[0, 0].min().item():.6f}\")\n",
    "print(f\"  Top-5 token indices: {soft_probs[0, 0].topk(5).indices.tolist()}\")\n",
    "\n",
    "# Decodifica i top-5 tokens\n",
    "top5_tokens = soft_probs[0, 0].topk(5)\n",
    "print(f\"\\nTop-5 tokens con probabilità:\")\n",
    "for idx, prob in zip(top5_tokens.indices, top5_tokens.values):\n",
    "    token = tokenizer.decode([idx.item()])\n",
    "    print(f\"  '{token}': {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3bc47",
   "metadata": {},
   "source": [
    "## 4. Confronto Teacher vs Student\n",
    "\n",
    "Confrontiamo le distribuzioni di probabilità tra teacher e student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.student import StudentModel, StudentConfig\n",
    "\n",
    "# Carica student model (più piccolo)\n",
    "student_config = StudentConfig(\n",
    "    model_name=\"google/flan-t5-base\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "student = StudentModel(student_config)\n",
    "print(f\"\\nStudent model caricato!\")\n",
    "print(f\"Parametri student: {student.count_parameters():,}\")\n",
    "print(f\"Parametri teacher: {teacher.count_parameters():,}\")\n",
    "print(f\"Ratio: {teacher.count_parameters() / student.count_parameters():.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ebeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ottieni logits dallo student\n",
    "student.model.eval()\n",
    "with torch.no_grad():\n",
    "    student_outputs = student(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        labels=targets[\"input_ids\"]\n",
    "    )\n",
    "\n",
    "student_logits = student_outputs[\"logits\"]\n",
    "print(f\"Student logits shape: {student_logits.shape}\")\n",
    "print(f\"Teacher logits shape: {teacher_logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola KL divergence tra student e teacher\n",
    "T = 2.0  # Temperature\n",
    "\n",
    "# Soft probabilities\n",
    "student_soft = F.log_softmax(student_logits / T, dim=-1)\n",
    "teacher_soft = F.softmax(teacher_logits / T, dim=-1)\n",
    "\n",
    "# Allinea vocabulary size se necessario\n",
    "if student_soft.size(-1) != teacher_soft.size(-1):\n",
    "    min_vocab = min(student_soft.size(-1), teacher_soft.size(-1))\n",
    "    student_soft = student_soft[:, :, :min_vocab]\n",
    "    teacher_soft = teacher_soft[:, :, :min_vocab]\n",
    "\n",
    "# KL divergence\n",
    "kl_div = F.kl_div(student_soft, teacher_soft, reduction='batchmean')\n",
    "print(f\"KL Divergence (student || teacher): {kl_div.item():.4f}\")\n",
    "\n",
    "# Con temperature scaling\n",
    "kl_div_scaled = kl_div * (T ** 2)\n",
    "print(f\"KL Divergence scaled (T²): {kl_div_scaled.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d09ca",
   "metadata": {},
   "source": [
    "## 5. Test Knowledge Distillation\n",
    "\n",
    "Testiamo la classe `KnowledgeDistillation` che combina teacher e student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.distillation import (\n",
    "    KnowledgeDistillation,\n",
    "    DistillationConfig,\n",
    "    compare_distillation_strategies\n",
    ")\n",
    "\n",
    "# Mostra strategie disponibili\n",
    "compare_distillation_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0368acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura distillazione\n",
    "distill_config = DistillationConfig(\n",
    "    ce_weight=1.0,       # α - peso per CE loss\n",
    "    distill_weight=0.5,  # β - peso per KL divergence\n",
    "    temperature=2.0\n",
    ")\n",
    "\n",
    "# Crea knowledge distillation\n",
    "kd = KnowledgeDistillation(\n",
    "    teacher_model=teacher,\n",
    "    student_model=student,\n",
    "    config=distill_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training step\n",
    "batch = {\n",
    "    \"input_ids\": inputs[\"input_ids\"],\n",
    "    \"attention_mask\": inputs[\"attention_mask\"],\n",
    "    \"labels\": targets[\"input_ids\"]\n",
    "}\n",
    "\n",
    "# Attiva gradient per lo student\n",
    "student.model.train()\n",
    "losses = kd.train_step(batch)\n",
    "\n",
    "print(\"\\n=== Distillation Losses ===\")\n",
    "print(f\"Total Loss: {losses['total_loss'].item():.4f}\")\n",
    "print(f\"CE Loss: {losses['ce_loss'].item():.4f}\")\n",
    "if 'distill_loss' in losses:\n",
    "    print(f\"Distill Loss (KL): {losses['distill_loss'].item():.4f}\")\n",
    "print(f\"\\nFormula: Loss = {distill_config.ce_weight}·CE + {distill_config.distill_weight}·KL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b10a3",
   "metadata": {},
   "source": [
    "## 6. Visualizzazione Distribuzioni\n",
    "\n",
    "Visualizziamo le distribuzioni di probabilità di teacher e student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prendi le probabilità per il primo token\n",
    "token_idx = 0\n",
    "top_k = 20\n",
    "\n",
    "teacher_probs = F.softmax(teacher_logits[0, token_idx], dim=-1)\n",
    "student_probs = F.softmax(student_logits[0, token_idx], dim=-1)\n",
    "\n",
    "# Top-k tokens dal teacher\n",
    "top_teacher = teacher_probs.topk(top_k)\n",
    "top_indices = top_teacher.indices.cpu().numpy()\n",
    "\n",
    "# Prendi le stesse probabilità dallo student\n",
    "teacher_top_probs = top_teacher.values.cpu().numpy()\n",
    "student_top_probs = student_probs[top_indices].cpu().numpy()\n",
    "\n",
    "# Nomi dei token\n",
    "token_names = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(token_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, teacher_top_probs, width, label='Teacher (flan-t5-xl)', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, student_top_probs, width, label='Student (flan-t5-base)', color='coral')\n",
    "\n",
    "ax.set_xlabel('Token')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title(f'Distribuzione probabilità - Token position {token_idx}')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(token_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b822e7",
   "metadata": {},
   "source": [
    "## 7. Memory Usage\n",
    "\n",
    "Verifichiamo l'utilizzo di memoria per il training con distillazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf60d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    print(\"=== GPU Memory Usage ===\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(f\"Max Allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(f\"Running on {device} - GPU memory stats not available\")\n",
    "    \n",
    "# Stima memoria modelli\n",
    "teacher_mem_gb = teacher.count_parameters() * 2 / 1e9  # FP16\n",
    "student_mem_gb = student.count_parameters() * 4 / 1e9  # FP32\n",
    "\n",
    "print(f\"\\n=== Stima memoria modelli ===\")\n",
    "print(f\"Teacher ({teacher_config.model_name}): ~{teacher_mem_gb:.1f} GB (FP16)\")\n",
    "print(f\"Student ({student_config.model_name}): ~{student_mem_gb:.1f} GB (FP32)\")\n",
    "print(f\"Totale stimato: ~{teacher_mem_gb + student_mem_gb:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64765329",
   "metadata": {},
   "source": [
    "## 8. Riepilogo\n",
    "\n",
    "Il teacher model è pronto per la knowledge distillation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75612a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RIEPILOGO - TEACHER MODEL TESTING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n✓ Teacher Model: {teacher_config.model_name}\")\n",
    "print(f\"  - Parametri: {teacher.count_parameters():,}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(f\"  - FP16: {teacher_config.use_fp16}\")\n",
    "print(f\"\\n✓ Student Model: {student_config.model_name}\")\n",
    "print(f\"  - Parametri: {student.count_parameters():,}\")\n",
    "print(f\"  - Compression ratio: {teacher.count_parameters() / student.count_parameters():.1f}x\")\n",
    "print(f\"\\n✓ Distillation Config:\")\n",
    "print(f\"  - α (CE weight): {distill_config.ce_weight}\")\n",
    "print(f\"  - β (KL weight): {distill_config.distill_weight}\")\n",
    "print(f\"  - Temperature: {distill_config.temperature}\")\n",
    "print(f\"  - Loss formula: α·CE(student, labels) + β·KL(student||teacher)\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pronto per il training! Vai al notebook 05_training_loop.ipynb\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
