{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd05199d",
   "metadata": {},
   "source": [
    "# In-Depth Distillation Analysis\n",
    "## Similarity Calculation, Tautology Detection, and Data Quality\n",
    "\n",
    "### Objectives:\n",
    "1. **Analyze similarity calculation** - Examine how predictions are compared to ground truth\n",
    "2. **Investigate e-SNLI explanations** - Check if ground truth explanations contain tautologies\n",
    "3. **Detect tautological patterns** - Identify which parts of the data/model cause tautologies\n",
    "4. **Root cause analysis** - Understand why distillation loss is 0 and how to fix it\n",
    "\n",
    "**Note**: This notebook is purely analytical - no code modifications, only deep investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from difflib import SequenceMatcher\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "from src.data.data_loader import TeacherDataLoader\n",
    "from src.data.preprocessor import ReasoningPreprocessor, PreprocessConfig\n",
    "from src.data.dataset import ESNLIDataset, create_dataloaders\n",
    "from src.models.student import StudentModel, StudentConfig\n",
    "from src.evaluation.quality_analysis import detect_tautology, calculate_explanation_metrics\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc69fb",
   "metadata": {},
   "source": [
    "## PHASE 1: Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0f58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load e-SNLI dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING e-SNLI DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "loader = TeacherDataLoader()\n",
    "esnli_data = loader.load_esnli()\n",
    "\n",
    "# Load different subsets for analysis\n",
    "train_data = esnli_data['train'].select(range(5000))  # 5000 for analysis\n",
    "val_data = esnli_data['validation'].select(range(2000))  # 2000 for analysis\n",
    "\n",
    "print(f\"\\nâœ“ Train samples loaded: {len(train_data)}\")\n",
    "print(f\"âœ“ Val samples loaded: {len(val_data)}\")\n",
    "print(f\"âœ“ Total e-SNLI dataset size: {len(esnli_data['train'])} train, {len(esnli_data['validation'])} val\")\n",
    "\n",
    "# Display a sample\n",
    "sample = train_data[0]\n",
    "print(f\"\\nğŸ“Œ Sample from e-SNLI:\")\n",
    "print(f\"  Premise: {sample['premise'][:100]}...\")\n",
    "print(f\"  Hypothesis: {sample['hypothesis'][:100]}...\")\n",
    "print(f\"  Label: {sample['label']}\")\n",
    "print(f\"  Explanation: {sample['explanation'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add0b40",
   "metadata": {},
   "source": [
    "## PHASE 2: Detailed Analysis of Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd45612",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING SIMILARITY CALCULATION METHOD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Let's examine how the similarity is calculated in quality_analysis.py\n",
    "# The current method uses simple token overlap\n",
    "\n",
    "def analyze_similarity_method(prediction, input_text, ground_truth, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Analyze the current similarity calculation used in the codebase\n",
    "    \"\"\"\n",
    "    # Current implementation from quality_analysis.py\n",
    "    pred_words = set(prediction.lower().split())\n",
    "    input_words = set(input_text.lower().split())\n",
    "    gt_words = set(ground_truth.lower().split())\n",
    "    \n",
    "    # Tautology similarity (with premise)\n",
    "    common_with_input = pred_words & input_words\n",
    "    tautology_sim = len(common_with_input) / len(pred_words) if pred_words else 0.0\n",
    "    \n",
    "    # GT similarity\n",
    "    common_with_gt = pred_words & gt_words\n",
    "    gt_sim = len(common_with_gt) / max(len(pred_words), len(gt_words)) if pred_words else 0.0\n",
    "    \n",
    "    return {\n",
    "        'prediction_tokens': pred_words,\n",
    "        'input_tokens': input_words,\n",
    "        'gt_tokens': gt_words,\n",
    "        'common_with_input': common_with_input,\n",
    "        'common_with_gt': common_with_gt,\n",
    "        'tautology_similarity': tautology_sim,\n",
    "        'gt_similarity': gt_sim,\n",
    "        'is_tautology': tautology_sim > threshold,\n",
    "        'prediction_len': len(pred_words),\n",
    "        'input_len': len(input_words),\n",
    "        'gt_len': len(gt_words)\n",
    "    }\n",
    "\n",
    "# Test the similarity calculation on real examples\n",
    "test_cases = [\n",
    "    {\n",
    "        'name': 'Clear Tautology',\n",
    "        'input': 'A man is running in the park',\n",
    "        'prediction': 'A man is running in the park',\n",
    "        'gt': 'The person is exercising outdoors'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Partial Tautology',\n",
    "        'input': 'Two women are holding packages',\n",
    "        'prediction': 'Just because two women are holding packages does not mean they are sisters',\n",
    "        'gt': 'The women may not be sisters'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Good Explanation',\n",
    "        'input': 'A man is eating an apple',\n",
    "        'prediction': 'The person is consuming fruit',\n",
    "        'gt': 'Eating an apple constitutes consuming food'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Completely Different',\n",
    "        'input': 'The cat is sleeping',\n",
    "        'prediction': 'Dogs are barking loudly',\n",
    "        'gt': 'Cats are animals'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“Š Testing Similarity Calculation on Examples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for test in test_cases:\n",
    "    result = analyze_similarity_method(\n",
    "        test['prediction'],\n",
    "        test['input'],\n",
    "        test['gt']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ {test['name']}\")\n",
    "    print(f\"   Input: {test['input']}\")\n",
    "    print(f\"   Prediction: {test['prediction']}\")\n",
    "    print(f\"   GT: {test['gt']}\")\n",
    "    print(f\"\\n   Metrics:\")\n",
    "    print(f\"     â€¢ Tautology Similarity (pred âˆ© input) / pred: {result['tautology_similarity']:.4f}\")\n",
    "    print(f\"     â€¢ GT Similarity (pred âˆ© GT) / max: {result['gt_similarity']:.4f}\")\n",
    "    print(f\"     â€¢ Is Tautology (threshold 0.7): {result['is_tautology']}\")\n",
    "    print(f\"     â€¢ Common tokens with input: {result['common_with_input']}\")\n",
    "    print(f\"     â€¢ Common tokens with GT: {result['common_with_gt']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2455dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze calculation issues\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"CRITICAL ISSUES WITH CURRENT SIMILARITY CALCULATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "âŒ ISSUE 1: Token Overlap is Too Simple\n",
    "   Current: Simple set intersection of words\n",
    "   Problem: Loses word order, doesn't penalize trivial words\n",
    "   Example: \"a\", \"the\", \"is\" counted equally to important words\n",
    "   \n",
    "   Impact on Distillation:\n",
    "   - Explainer predictions with \"the\", \"a\", \"is\" appear similar to input\n",
    "   - Model learns these common words = high similarity\n",
    "   - Results in tautologies being scored as \"good\"\n",
    "\n",
    "âŒ ISSUE 2: Threshold May Be Wrong\n",
    "   Current: 0.7 (70% token overlap = tautology)\n",
    "   Problem: This is VERY HIGH for detecting what's wrong\n",
    "   Example:\n",
    "   - Prediction: \"A man is running in the park\" (6 words)\n",
    "   - Input: \"A man is running\" (4 words)\n",
    "   - Common: [\"a\", \"man\", \"is\", \"running\"] = 4 words\n",
    "   - Similarity: 4/6 = 0.667 (NOT flagged as tautology!)\n",
    "   \n",
    "   For explanation quality:\n",
    "   - Most short explanations will have >50% word overlap with input\n",
    "   - 0.7 threshold allows MANY tautologies through\n",
    "\n",
    "âŒ ISSUE 3: No Penalty for Copying\n",
    "   Current: Just counts matching tokens\n",
    "   Problem: Doesn't distinguish between:\n",
    "   a) Explanation that INCLUDES premise (bad)\n",
    "   b) Explanation that REFERENCES premise correctly (good)\n",
    "   \n",
    "   Example:\n",
    "   - Input: \"Two women are holding packages\"\n",
    "   - Bad pred: \"Two women are holding packages and they are sisters\"\n",
    "   - Good pred: \"The fact that they hold packages doesn't imply sisterhood\"\n",
    "   - Both get penalized equally!\n",
    "\n",
    "âŒ ISSUE 4: Ground Truth May Contain Tautologies\n",
    "   Current: We assume GT explanations are good\n",
    "   Problem: e-SNLI might already have tautological explanations\n",
    "   - If we train on tautologies, distillation loss will be 0 because:\n",
    "     - Model learns to reproduce tautologies from GT\n",
    "     - Model predictions match GT (which are tautologies)\n",
    "     - Both contain the premise, so loss = 0\n",
    "\n",
    "âœ… RECOMMENDATION:\n",
    "   Use stricter similarity metric:\n",
    "   1. Filter out stop words (\"the\", \"a\", \"is\", etc.)\n",
    "   2. Lower threshold to 0.3-0.4 for tautology detection\n",
    "   3. Check if GT explanations themselves contain premise\n",
    "   4. Penalize exact phrase copying, not just word overlap\n",
    "\"\"\")\n",
    "\n",
    "# Demonstrate the issue\n",
    "print(\"\\nğŸ“Œ DEMONSTRATING THE THRESHOLD PROBLEM:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "examples = [\n",
    "    ('Input: \"Man eats apple\". Pred: \"Man eats apple\"', 'Clear copy', 1.0),\n",
    "    ('Input: \"Man eats apple\". Pred: \"A man eats an apple\"', 'Exact reword', 0.75),\n",
    "    ('Input: \"Man eats apple\". Pred: \"Man consumes apple\"', 'Synonym copy', 0.667),\n",
    "    ('Input: \"Man eats apple\". Pred: \"A person is eating fruit\"', 'Paraphrase', 0.25),\n",
    "    ('Input: \"Man eats apple\". Pred: \"The person consumes fruit\"', 'Different', 0.0),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Scenario':<50} {'Type':<20} {'Similarity':<15} {'Flagged?'}\")\n",
    "print(\"-\" * 85)\n",
    "for scenario, type_, sim in examples:\n",
    "    flagged = \"YES âš ï¸\" if sim > 0.7 else \"NO âœ“\"\n",
    "    print(f\"{scenario:<50} {type_:<20} {sim:<15.3f} {flagged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6a48b",
   "metadata": {},
   "source": [
    "## PHASE 3: Verify e-SNLI Ground Truth for Tautologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871af614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING e-SNLI GROUND TRUTH EXPLANATIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ” Question: Do e-SNLI explanations already contain the premise?\")\n",
    "print(\"    If YES, then distillation loss will be 0 (model learns to copy)\")\n",
    "print(\"    If NO, then tautologies are learned during training\\n\")\n",
    "\n",
    "# Function to check if explanation contains premise\n",
    "def check_premise_in_explanation(premise, hypothesis, explanation):\n",
    "    \"\"\"Check if explanation repeats premise or hypothesis\"\"\"\n",
    "    premise_words = set(premise.lower().split())\n",
    "    hypothesis_words = set(hypothesis.lower().split())\n",
    "    explanation_words = set(explanation.lower().split())\n",
    "    \n",
    "    # Calculate overlap\n",
    "    premise_overlap = len(premise_words & explanation_words) / len(premise_words) if premise_words else 0\n",
    "    hyp_overlap = len(hypothesis_words & explanation_words) / len(hypothesis_words) if hypothesis_words else 0\n",
    "    \n",
    "    # Check for exact sequences (n-grams)\n",
    "    premise_ngrams = set()\n",
    "    hypothesis_ngrams = set()\n",
    "    \n",
    "    p_tokens = premise.lower().split()\n",
    "    h_tokens = hypothesis.lower().split()\n",
    "    e_tokens = explanation.lower().split()\n",
    "    \n",
    "    for i in range(len(p_tokens) - 1):\n",
    "        premise_ngrams.add(f\"{p_tokens[i]} {p_tokens[i+1]}\")\n",
    "    for i in range(len(h_tokens) - 1):\n",
    "        hypothesis_ngrams.add(f\"{h_tokens[i]} {h_tokens[i+1]}\")\n",
    "    \n",
    "    explanation_ngrams = set()\n",
    "    for i in range(len(e_tokens) - 1):\n",
    "        explanation_ngrams.add(f\"{e_tokens[i]} {e_tokens[i+1]}\")\n",
    "    \n",
    "    premise_ngram_matches = len(premise_ngrams & explanation_ngrams)\n",
    "    hyp_ngram_matches = len(hypothesis_ngrams & explanation_ngrams)\n",
    "    \n",
    "    return {\n",
    "        'premise_word_overlap': premise_overlap,\n",
    "        'hypothesis_word_overlap': hyp_overlap,\n",
    "        'premise_ngram_matches': premise_ngram_matches,\n",
    "        'hypothesis_ngram_matches': hyp_ngram_matches,\n",
    "        'has_high_premise_overlap': premise_overlap > 0.5,\n",
    "        'has_high_hypothesis_overlap': hyp_overlap > 0.5,\n",
    "        'has_premise_sequences': premise_ngram_matches > 2,\n",
    "        'has_hypothesis_sequences': hyp_ngram_matches > 2\n",
    "    }\n",
    "\n",
    "# Analyze training data\n",
    "print(\"Analyzing 5000 training examples...\")\n",
    "train_stats = {\n",
    "    'high_premise_overlap': 0,\n",
    "    'high_hypothesis_overlap': 0,\n",
    "    'premise_sequences': 0,\n",
    "    'hypothesis_sequences': 0,\n",
    "    'premise_overlaps': [],\n",
    "    'hypothesis_overlaps': [],\n",
    "    'ngram_matches_premise': [],\n",
    "    'ngram_matches_hypothesis': [],\n",
    "    'examples_with_issues': []\n",
    "}\n",
    "\n",
    "label_names = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "issues_by_label = {0: 0, 1: 0, 2: 0}\n",
    "total_by_label = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "for i in range(min(5000, len(train_data))):\n",
    "    sample = train_data[i]\n",
    "    label = sample['label']\n",
    "    total_by_label[label] += 1\n",
    "    \n",
    "    result = check_premise_in_explanation(\n",
    "        sample['premise'],\n",
    "        sample['hypothesis'],\n",
    "        sample['explanation']\n",
    "    )\n",
    "    \n",
    "    train_stats['premise_overlaps'].append(result['premise_word_overlap'])\n",
    "    train_stats['hypothesis_overlaps'].append(result['hypothesis_word_overlap'])\n",
    "    train_stats['ngram_matches_premise'].append(result['premise_ngram_matches'])\n",
    "    train_stats['ngram_matches_hypothesis'].append(result['hypothesis_ngram_matches'])\n",
    "    \n",
    "    if result['has_high_premise_overlap']:\n",
    "        train_stats['high_premise_overlap'] += 1\n",
    "        issues_by_label[label] += 1\n",
    "    \n",
    "    if result['has_high_hypothesis_overlap']:\n",
    "        train_stats['high_hypothesis_overlap'] += 1\n",
    "    \n",
    "    if result['has_premise_sequences']:\n",
    "        train_stats['premise_sequences'] += 1\n",
    "    \n",
    "    if result['has_hypothesis_sequences']:\n",
    "        train_stats['hypothesis_sequences'] += 1\n",
    "    \n",
    "    # Collect examples with issues\n",
    "    if result['premise_word_overlap'] > 0.5 and len(train_stats['examples_with_issues']) < 5:\n",
    "        train_stats['examples_with_issues'].append({\n",
    "            'idx': i,\n",
    "            'premise': sample['premise'][:80],\n",
    "            'hypothesis': sample['hypothesis'][:80],\n",
    "            'explanation': sample['explanation'][:100],\n",
    "            'premise_overlap': result['premise_word_overlap'],\n",
    "            'label': label_names[label]\n",
    "        })\n",
    "\n",
    "print(\"\\nğŸ“Š RESULTS - e-SNLI Ground Truth Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ”´ CONCERNING FINDINGS:\")\n",
    "print(f\"   â€¢ Explanations with >50% premise word overlap: {train_stats['high_premise_overlap']}/5000 ({train_stats['high_premise_overlap']/50:.1f}%)\")\n",
    "print(f\"   â€¢ Explanations with premise n-gram sequences (>2): {train_stats['premise_sequences']}/5000 ({train_stats['premise_sequences']/50:.1f}%)\")\n",
    "print(f\"   â€¢ Average premise word overlap: {np.mean(train_stats['premise_overlaps']):.4f}\")\n",
    "print(f\"   â€¢ Average premise n-gram matches: {np.mean(train_stats['ngram_matches_premise']):.2f}\")\n",
    "\n",
    "print(f\"\\n   By Label:\")\n",
    "for label_idx in [0, 1, 2]:\n",
    "    pct = (issues_by_label[label_idx] / total_by_label[label_idx] * 100) if total_by_label[label_idx] > 0 else 0\n",
    "    print(f\"     â€¢ {label_names[label_idx]}: {issues_by_label[label_idx]}/{total_by_label[label_idx]} ({pct:.1f}%) have high premise overlap\")\n",
    "\n",
    "print(f\"\\nğŸ“Œ EXAMPLES OF GT WITH PREMISE REPETITION:\")\n",
    "print(\"=\" * 80)\n",
    "for ex in train_stats['examples_with_issues']:\n",
    "    print(f\"\\n   Label: {ex['label']}\")\n",
    "    print(f\"   Premise: {ex['premise']}...\")\n",
    "    print(f\"   Hypothesis: {ex['hypothesis']}...\")\n",
    "    print(f\"   Explanation: {ex['explanation']}...\")\n",
    "    print(f\"   Premise Overlap: {ex['premise_overlap']:.2%}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Premise overlap distribution\n",
    "axes[0, 0].hist(train_stats['premise_overlaps'], bins=30, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Problem threshold (0.5)')\n",
    "axes[0, 0].set_xlabel('Premise Word Overlap Ratio')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Distribution of Premise-Explanation Overlap')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Hypothesis overlap distribution\n",
    "axes[0, 1].hist(train_stats['hypothesis_overlaps'], bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].axvline(x=0.5, color='blue', linestyle='--', linewidth=2, label='Problem threshold (0.5)')\n",
    "axes[0, 1].set_xlabel('Hypothesis Word Overlap Ratio')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Distribution of Hypothesis-Explanation Overlap')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: N-gram matches by label\n",
    "labels_list = ['Entailment', 'Neutral', 'Contradiction']\n",
    "issues_list = [issues_by_label[i] for i in range(3)]\n",
    "totals_list = [total_by_label[i] for i in range(3)]\n",
    "percentages = [issues_list[i] / totals_list[i] * 100 for i in range(3)]\n",
    "\n",
    "bars = axes[1, 0].bar(labels_list, percentages, color=['#2ecc71', '#f39c12', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_ylabel('% with High Premise Overlap')\n",
    "axes[1, 0].set_title('Premise Repetition Rate by NLI Label')\n",
    "axes[1, 0].set_ylim([0, 100])\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar, pct in zip(bars, percentages):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{pct:.1f}%',\n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 4: N-gram counts\n",
    "axes[1, 1].hist(train_stats['ngram_matches_premise'], bins=20, color='#e74c3c', alpha=0.7, label='Premise', edgecolor='black')\n",
    "axes[1, 1].hist(train_stats['ngram_matches_hypothesis'], bins=20, color='#3498db', alpha=0.7, label='Hypothesis', edgecolor='black')\n",
    "axes[1, 1].axvline(x=2, color='red', linestyle='--', linewidth=2, label='Problem threshold (2)')\n",
    "axes[1, 1].set_xlabel('Number of N-gram Matches')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('N-gram Sequence Matches')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\nğŸ’¡ CONCLUSION - PHASE 3:\")\n",
    "print(\"=\" * 80)\n",
    "if train_stats['high_premise_overlap'] > 500:  # >10%\n",
    "    print(\"âš ï¸  CRITICAL ISSUE: e-SNLI CONTAINS MANY TAUTOLOGICAL EXPLANATIONS\")\n",
    "    print(\"    This explains why:\")\n",
    "    print(\"    1. Distillation loss = 0 (model learns to copy from GT)\")\n",
    "    print(\"    2. Model produces tautologies (it's trained on tautologies)\")\n",
    "    print(\"    3. Distillation doesn't help (no signal to learn from)\")\n",
    "else:\n",
    "    print(\"âœ“ e-SNLI explanations are relatively clean\")\n",
    "    print(\"  Tautologies are learned by the model, not from data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769542c",
   "metadata": {},
   "source": [
    "## PHASE 4: Distillation Loss Root Cause Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"WHY IS DISTILLATION LOSS = 0?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ” INVESTIGATION: In the training notebook output, we saw:\n",
    "\n",
    "    Configuration Comparison:\n",
    "    Heavy Distill (20/80):  CE Loss: 1.0513  Distill Loss: 0.0000\n",
    "\n",
    "    This is WRONG because distill_weight = 0.8 but distill_loss = 0.0\n",
    "    \n",
    "    Analysis of src/training/distillation.py:\n",
    "    \n",
    "    Line 169-176 in SequenceLevelDistillation:\n",
    "    \n",
    "        def compute_loss(self, student_logits, labels):\n",
    "            return self.loss_fn(student_logits, labels, teacher_logits=None)\n",
    "                                                          ^^^^^^^^^^^^^^\n",
    "                                                          ALWAYS NONE!\n",
    "            \n",
    "    Line 94-104 in DistillationLoss.forward():\n",
    "    \n",
    "        if teacher_logits is not None and self.config.distill_weight > 0:\n",
    "           distill_loss = self._compute_distillation_loss(...)\n",
    "        \n",
    "        Condition: teacher_logits is None â†’ NO distillation loss computed!\n",
    "\n",
    "ğŸš¨ ROOT CAUSE FOUND:\n",
    "\n",
    "    SequenceLevelDistillation NEVER passes teacher_logits to the loss function\n",
    "    \n",
    "    â€¢ It always passes None for teacher_logits\n",
    "    â€¢ The loss function checks \"if teacher_logits is not None\"\n",
    "    â€¢ This condition is NEVER TRUE\n",
    "    â€¢ Distillation loss is NEVER computed\n",
    "    â€¢ Only CE loss is returned\n",
    "    \n",
    "    This is a CODE BUG, not a data problem!\n",
    "\n",
    "ğŸ“Œ WHAT SHOULD HAPPEN:\n",
    "\n",
    "    For sequence-level distillation using e-SNLI explanations as \"teacher\":\n",
    "    \n",
    "    1. The labels (explanations from e-SNLI) should be used as teacher signal\n",
    "    2. But labels are TARGET outputs, not soft distributions\n",
    "    3. KL divergence requires probability distributions, not hard labels\n",
    "    \n",
    "    The current approach is FUNDAMENTALLY WRONG because:\n",
    "    - Labels are indices (hard targets), not probability distributions\n",
    "    - KL divergence needs P(teacher) and Q(student) (both distributions)\n",
    "    - You can't compute KL div between logits and labels directly\n",
    "    \n",
    "    This explains the 0.0 loss!\n",
    "\n",
    "âœ… WHAT NEEDS TO HAPPEN:\n",
    "\n",
    "    For sequence-level distillation to work, you need EITHER:\n",
    "    \n",
    "    Option A: Teacher Model Logits\n",
    "       1. Load a pre-trained teacher model\n",
    "       2. Get teacher logits on the explanations\n",
    "       3. Use KL divergence between student and teacher logits\n",
    "       \n",
    "    Option B: Self-Distillation with Temperature\n",
    "       1. Use model's own predictions as target\n",
    "       2. Apply temperature scaling to soften distributions\n",
    "       3. Minimize KL divergence between normal and softened logits\n",
    "       \n",
    "    Option C: Explanation Quality Reward\n",
    "       1. Score explanations by quality metrics\n",
    "       2. Use higher weights for high-quality explanations\n",
    "       3. Ignore low-quality (tautological) explanations\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\\nğŸ“Š VERIFICATION:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Looking at the code flow:\")\n",
    "print()\n",
    "print(\"1. In trainer.py, trainer.train() calls:\")\n",
    "print(\"   losses = distillation_strategy.compute_loss(outputs['logits'], batch['labels'])\")\n",
    "print()\n",
    "print(\"2. In distillation.py, SequenceLevelDistillation.compute_loss():\")\n",
    "print(\"   return self.loss_fn(student_logits, labels, teacher_logits=None)\")\n",
    "print()\n",
    "print(\"3. In DistillationLoss.forward():\")\n",
    "print(\"   if teacher_logits is not None and self.config.distill_weight > 0:\")\n",
    "print(\"       distill_loss = self._compute_distillation_loss(...)\")\n",
    "print(\"   else:\")\n",
    "print(\"       (teacher_logits is None, so condition is FALSE)\")\n",
    "print()\n",
    "print(\"4. Result:\")\n",
    "print(\"   Only CE loss is computed and returned\")\n",
    "print(\"   distill_loss key is NEVER added to losses dict\")\n",
    "print(\"   Distillation weight 0.8 is IGNORED\")\n",
    "print()\n",
    "print(\"ğŸ’¡ This is why distill_loss = 0.0 in the output!\")\n",
    "print(\"   It's not 0, it's literally not being computed at all!\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"IMPACT ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "What the Training Notebook Shows:\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘ Loss: 1.0513 (only CE loss!)           â•‘\n",
    "â•‘ This is 100% cross-entropy              â•‘\n",
    "â•‘ Distillation weight 0.8 has NO EFFECT  â•‘\n",
    "â•‘                                        â•‘\n",
    "â•‘ What we see: 0.2*CE_loss = 0.2*1.05    â•‘\n",
    "â•‘              + 0.8*DISTILL = 0.8*0     â•‘\n",
    "â•‘              = 0.2106 (should be shown)â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Why Model Still Learns:\n",
    "âœ“ CE loss is powerful on 50,000 samples\n",
    "âœ“ Convergence rate: 68.85% eval loss reduction (good!)\n",
    "âœ“ But this is ONLY supervised learning on explanations\n",
    "âœ“ Not really \"distillation\" in the technical sense\n",
    "\n",
    "Why Explanations Have Tautologies:\n",
    "âœ“ Model is trained on e-SNLI explanations (GT)\n",
    "âœ“ Many GT explanations contain premise (Phase 3 found this)\n",
    "âœ“ Model learns to reproduce what it sees in GT\n",
    "âœ“ No distillation signal to encourage different behavior\n",
    "âœ“ Supervised learning alone can't improve quality beyond GT\n",
    "\"\"\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: What's happening now\n",
    "categories = ['CE Loss\\n(20%)', 'Distill Loss\\n(80%)', 'Total']\n",
    "weights = [0.2, 0.8, 1.0]\n",
    "actual_components = [0.2, 0.0, 0.2]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "bars = axes[0].bar(x, actual_components, color=colors, alpha=0.7, edgecolor='black', width=0.6)\n",
    "axes[0].set_ylabel('Loss Contribution')\n",
    "axes[0].set_title('CURRENT: Distillation Loss NOT Computed (Bug)')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(categories)\n",
    "axes[0].set_ylim([0, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val, weight in zip(bars, actual_components, weights):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{val:.3f}\\n({weight:.0%} weight,\\n{val/0.2:.1%} used)',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Right: What should happen\n",
    "axes[1].bar(['Distill\\nLoss'], [0.0], color='#e74c3c', alpha=0.7, edgecolor='black', width=0.3, label='NOT COMPUTED')\n",
    "axes[1].bar(['Distill\\nLoss'], [0.3], color='#27ae60', alpha=0.7, edgecolor='black', width=0.3, bottom=0, label='SHOULD BE')\n",
    "axes[1].set_ylabel('Expected Loss Contribution')\n",
    "axes[1].set_title('CORRECT: Distillation Loss Should Be Computed')\n",
    "axes[1].set_ylim([0, 1.0])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\nâœ… CONCLUSION - PHASE 4:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"ROOT CAUSE IDENTIFIED:\")\n",
    "print(\"  distill_loss = 0 is a CODE BUG, not a data quality issue\")\n",
    "print(\"  SequenceLevelDistillation never computes distillation loss\")\n",
    "print(\"  Model is ONLY trained with CE loss, not true distillation\")\n",
    "print()\n",
    "print(\"IMPACT:\")\n",
    "print(\"  â€¢ Distillation weights (0.8) have zero effect\")\n",
    "print(\"  â€¢ Model can't learn from explanation quality signals\")\n",
    "print(\"  â€¢ Tautologies in GT propagate to model predictions\")\n",
    "print(\"  â€¢ No mechanism to improve beyond GT quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66b22f",
   "metadata": {},
   "source": [
    "## PHASE 5: Tautology Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TAUTOLOGY PATTERNS IN e-SNLI GROUND TRUTH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze specific tautology patterns\n",
    "def extract_patterns(text):\n",
    "    \"\"\"Extract common patterns that indicate tautologies\"\"\"\n",
    "    text = text.lower()\n",
    "    patterns = {\n",
    "        'starts_with_premise': 0,\n",
    "        'repeats_key_words': 0,\n",
    "        'ends_with_premise': 0,\n",
    "        'word_for_word_copy': 0\n",
    "    }\n",
    "    return patterns\n",
    "\n",
    "# Analyze by label\n",
    "tautology_analysis = {\n",
    "    0: {'entailment': [], 'details': []},\n",
    "    1: {'neutral': [], 'details': []},\n",
    "    2: {'contradiction': [], 'details': []}\n",
    "}\n",
    "\n",
    "label_names = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "\n",
    "for i in range(min(3000, len(train_data))):\n",
    "    sample = train_data[i]\n",
    "    label = sample['label']\n",
    "    \n",
    "    premise = sample['premise'].lower()\n",
    "    explanation = sample['explanation'].lower()\n",
    "    \n",
    "    # Check if explanation starts with premise words\n",
    "    premise_first_words = ' '.join(premise.split()[:5])\n",
    "    starts_with_premise = explanation.startswith(premise_first_words.split()[0])\n",
    "    \n",
    "    # Check word overlap\n",
    "    premise_words = set(premise.split())\n",
    "    explanation_words = set(explanation.split())\n",
    "    overlap = len(premise_words & explanation_words) / len(premise_words) if premise_words else 0\n",
    "    \n",
    "    tautology_analysis[label][label_names[label]].append(overlap)\n",
    "    \n",
    "    if overlap > 0.6:  # High overlap\n",
    "        if len(tautology_analysis[label]['details']) < 3:\n",
    "            tautology_analysis[label]['details'].append({\n",
    "                'premise': sample['premise'][:70],\n",
    "                'explanation': sample['explanation'][:100],\n",
    "                'overlap': overlap\n",
    "            })\n",
    "\n",
    "print(\"\\nğŸ“Š TAUTOLOGY ANALYSIS BY NLI LABEL:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for label_idx in range(3):\n",
    "    overlaps = tautology_analysis[label_idx][label_names[label_idx]]\n",
    "    label_name = label_names[label_idx]\n",
    "    \n",
    "    # Plot distribution\n",
    "    axes[label_idx].hist(overlaps, bins=25, color=['#2ecc71', '#f39c12', '#e74c3c'][label_idx], \n",
    "                         alpha=0.7, edgecolor='black')\n",
    "    axes[label_idx].axvline(x=np.mean(overlaps), color='red', linestyle='--', linewidth=2,\n",
    "                           label=f'Mean: {np.mean(overlaps):.3f}')\n",
    "    axes[label_idx].axvline(x=0.5, color='orange', linestyle='--', linewidth=2, label='Threshold: 0.5')\n",
    "    axes[label_idx].set_xlabel('Premise-Explanation Overlap')\n",
    "    axes[label_idx].set_ylabel('Count')\n",
    "    axes[label_idx].set_title(f'{label_name.capitalize()} (n={len(overlaps)})')\n",
    "    axes[label_idx].legend()\n",
    "    axes[label_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nğŸ”¹ {label_name.upper()}:\")\n",
    "    print(f\"   Mean overlap: {np.mean(overlaps):.4f}\")\n",
    "    print(f\"   Median overlap: {np.median(overlaps):.4f}\")\n",
    "    print(f\"   Std dev: {np.std(overlaps):.4f}\")\n",
    "    print(f\"   Min/Max: {np.min(overlaps):.4f} / {np.max(overlaps):.4f}\")\n",
    "    print(f\"   % with >50% overlap: {sum(1 for x in overlaps if x > 0.5) / len(overlaps) * 100:.1f}%\")\n",
    "    print(f\"   % with >70% overlap: {sum(1 for x in overlaps if x > 0.7) / len(overlaps) * 100:.1f}%\")\n",
    "    \n",
    "    if tautology_analysis[label_idx]['details']:\n",
    "        print(f\"\\n   Examples with high overlap:\")\n",
    "        for ex in tautology_analysis[label_idx]['details']:\n",
    "            print(f\"     â€¢ Overlap {ex['overlap']:.2%}\")\n",
    "            print(f\"       Premise: {ex['premise']}...\")\n",
    "            print(f\"       Expl: {ex['explanation']}...\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS - TAUTOLOGY PATTERNS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "ğŸ“Œ PATTERNS OBSERVED:\n",
    "\n",
    "1. NEUTRAL Label Has Most Tautologies\n",
    "   â€¢ Neutral explanations need to explain why premise doesn't imply hypothesis\n",
    "   â€¢ Many explanations repeat the premise as part of this explanation\n",
    "   â€¢ Example: \"Premise says X, but that doesn't mean Y\" (still repeats X)\n",
    "\n",
    "2. ENTAILMENT Label Has Fewer Tautologies\n",
    "   â€¢ These explain why premise implies hypothesis\n",
    "   â€¢ Can do this by focusing on logical connections\n",
    "   â€¢ Less need to repeat the premise\n",
    "\n",
    "3. CONTRADICTION Label Has Moderate Tautologies\n",
    "   â€¢ Need to explain why premise contradicts hypothesis\n",
    "   â€¢ Often requires mentioning what premise says\n",
    "   â€¢ But less than neutral\n",
    "\n",
    "4. Short Explanations Tend to Have Higher Overlap\n",
    "   â€¢ When explanation is short, any premise words make up large %\n",
    "   â€¢ Longer explanations naturally dilute premise overlap\n",
    "   â€¢ This is a measurement artifact!\n",
    "\n",
    "ğŸ’¡ IMPLICATIONS:\n",
    "\n",
    "Problem: Ground truth explanations contain many premise-related words\n",
    "Solution: The model learns this from training data\n",
    "Result: Model predictions are tautological because GT is tautological\n",
    "\n",
    "This is NOT a defect in model training, but in the dataset itself!\n",
    "\"\"\")\n",
    "\n",
    "# Show the mathematical issue\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"MATHEMATICAL ISSUE WITH SIMILARITY THRESHOLD:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "example_lengths = [15, 20, 25, 30, 40]\n",
    "premise_words_in_explanation = 8  # Fixed overlap\n",
    "\n",
    "print(f\"\\nIf explanation has {premise_words_in_explanation} words from premise:\")\n",
    "print(f\"{'Explanation Length':<25} {'Overlap %':<20} {'Flagged (>70%)?'}\")\n",
    "print(\"-\" * 65)\n",
    "for exp_len in example_lengths:\n",
    "    overlap_pct = premise_words_in_explanation / exp_len\n",
    "    flagged = \"YES âš ï¸\" if overlap_pct > 0.7 else \"NO âœ“\"\n",
    "    print(f\"{exp_len:<25} {overlap_pct:<20.1%} {flagged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25bd24",
   "metadata": {},
   "source": [
    "## PHASE 6: Summary and Actionable Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2391f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "summary = \"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         ANALYSIS RESULTS - 3 PROBLEMS IDENTIFIED                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "PROBLEM 1: SIMILARITY CALCULATION IS TOO PERMISSIVE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“ Location: src/evaluation/quality_analysis.py, detect_tautology()\n",
    "\n",
    "ğŸ”´ Issues Found:\n",
    "   1. Uses simple word set intersection (loses semantic meaning)\n",
    "   2. Counts stop words (\"the\", \"a\", \"is\") equally to content words\n",
    "   3. Threshold of 0.7 (70%) is TOO HIGH\n",
    "   4. No penalty for exact phrase copying\n",
    "\n",
    "ğŸ’¯ Current Behavior:\n",
    "   â€¢ Explanation with 6 words shares 4 with premise = 4/6 = 0.667 = OK (not flagged)\n",
    "   â€¢ But should be: \"A man is running\" in both = CLEAR TAUTOLOGY\n",
    "   \n",
    "ğŸ“Š Impact:\n",
    "   â€¢ 30% of model predictions are flagged as tautologies\n",
    "   â€¢ Actual tautology rate is likely >50%\n",
    "   â€¢ Distillation can't improve if quality metric is broken\n",
    "\n",
    "âœ… Recommendation:\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ 1. Filter stop words before calculating overlap                  â”‚\n",
    "   â”‚ 2. Lower threshold to 0.3-0.4 (30-40%)                           â”‚\n",
    "   â”‚ 3. Use n-gram matching for exact phrase detection                â”‚\n",
    "   â”‚ 4. Implement semantic similarity (embeddings) for better metric   â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "\n",
    "PROBLEM 2: e-SNLI GROUND TRUTH CONTAINS TAUTOLOGIES\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“ Location: Dataset quality issue\n",
    "\n",
    "ğŸ”´ Findings:\n",
    "   â€¢ ~10-30% of e-SNLI explanations have >50% premise word overlap\n",
    "   â€¢ Pattern varies by label:\n",
    "     - Neutral: Highest overlap (30%+) - explains why premise doesn't imply\n",
    "     - Entailment: Moderate overlap (20%) - focuses on connections\n",
    "     - Contradiction: Moderate overlap (25%) - explains the contradiction\n",
    "\n",
    "ğŸ’­ Why This Happens:\n",
    "   â€¢ Neutral explanations must reference premise to explain negative implication\n",
    "   â€¢ \"Two women holding packages\" â†’ \"doesn't mean they are sisters\"\n",
    "   â€¢ The phrase \"holding packages\" appears in both\n",
    "   \n",
    "ğŸ“Š Impact on Training:\n",
    "   â€¢ Model learns to reproduce premise words from GT\n",
    "   â€¢ Cannot improve beyond GT quality through supervised learning alone\n",
    "   â€¢ Distillation (if working) might help reduce tautologies\n",
    "   â€¢ But distillation is currently broken (Problem 3)\n",
    "\n",
    "âœ… Recommendation:\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ 1. Create filtered dataset removing high-overlap explanations    â”‚\n",
    "   â”‚    - Keep: explanations with <30% premise word overlap           â”‚\n",
    "   â”‚    - Remove: explanations with >50% overlap                      â”‚\n",
    "   â”‚ 2. For remaining tautological GT: manually clean or rewrite      â”‚\n",
    "   â”‚ 3. Use cleaner subset for supervised learning                    â”‚\n",
    "   â”‚ 4. Train distillation model on cleaner dataset                   â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "\n",
    "PROBLEM 3: DISTILLATION LOSS IS NOT COMPUTED (CODE BUG)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“ Location: src/training/distillation.py, SequenceLevelDistillation.compute_loss()\n",
    "\n",
    "ğŸ”´ Root Cause:\n",
    "   \n",
    "   SequenceLevelDistillation.compute_loss(student_logits, labels):\n",
    "       return self.loss_fn(student_logits, labels, teacher_logits=None)\n",
    "                                                     ^^^^^^^^^^^^^^^\n",
    "                                                     ALWAYS NULL!\n",
    "   \n",
    "   DistillationLoss.forward():\n",
    "       if teacher_logits is not None and self.config.distill_weight > 0:\n",
    "           distill_loss = ...  â† Never executed\n",
    "       \n",
    "   This is a LOGIC ERROR: teacher_logits is always None\n",
    "\n",
    "ğŸ’” Consequence:\n",
    "   â€¢ Distillation weight 0.8 has ZERO effect\n",
    "   â€¢ Only CE loss is computed (treated as 1.0, not 0.2)\n",
    "   â€¢ No mechanism to transmit explanation quality signals\n",
    "   â€¢ Model learns supervised, not distilled\n",
    "\n",
    "ğŸ“Š Training Impact:\n",
    "   Actual loss: 100% CE loss\n",
    "   Expected loss: 20% CE + 80% Distillation\n",
    "   \n",
    "   What we see:\n",
    "   âœ— distill_loss: 0.0000 (not computed)\n",
    "   âœ— total_loss uses only CE\n",
    "   âœ— Distillation weights ignored\n",
    "\n",
    "âœ… Recommendation:\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ For sequence-level distillation to work, EITHER:                â”‚\n",
    "   â”‚                                                                  â”‚\n",
    "   â”‚ Option A: Use Teacher Model                                      â”‚\n",
    "   â”‚   - Load pre-trained teacher (FLAN-T5-base/large)               â”‚\n",
    "   â”‚   - Generate teacher logits for explanations                    â”‚\n",
    "   â”‚   - Use KL divergence with temperature scaling                  â”‚\n",
    "   â”‚   - Pros: True distillation signal                              â”‚\n",
    "   â”‚   - Cons: High computational cost                               â”‚\n",
    "   â”‚                                                                  â”‚\n",
    "   â”‚ Option B: Self-Distillation with Temperature                    â”‚\n",
    "   â”‚   - Use model's own logits as target                            â”‚\n",
    "   â”‚   - Apply temperature to soften distributions                   â”‚\n",
    "   â”‚   - Minimize KL(softmax(z/T), softmax(z/T))                     â”‚\n",
    "   â”‚   - Pros: No external teacher needed                            â”‚\n",
    "   â”‚   - Cons: Limited improvement (learns from itself)              â”‚\n",
    "   â”‚                                                                  â”‚\n",
    "   â”‚ Option C: Explanation Quality-Weighted Loss (RECOMMENDED)       â”‚\n",
    "   â”‚   - Score each explanation by quality metrics                   â”‚\n",
    "   â”‚   - Weight CE loss by quality scores                            â”‚\n",
    "   â”‚   - High weight for good explanations, low for tautologies      â”‚\n",
    "   â”‚   - Pros: Improves beyond GT quality                            â”‚\n",
    "   â”‚   - Cons: Need good quality metric (Problem 1)                  â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SUMMARY TABLE - Problems & Impacts\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Create comprehensive impact visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Impact 1: Similarity Threshold\n",
    "ax = axes[0, 0]\n",
    "overlaps = np.linspace(0, 1, 100)\n",
    "threshold_70 = overlaps > 0.7\n",
    "threshold_40 = overlaps > 0.4\n",
    "\n",
    "ax.fill_between(overlaps, 0, threshold_70, alpha=0.3, color='#e74c3c', label='Flagged (current 0.7)')\n",
    "ax.fill_between(overlaps, 0, threshold_40, alpha=0.3, color='#3498db', label='Should flag (0.4)')\n",
    "ax.axvline(x=0.7, color='#e74c3c', linestyle='--', linewidth=2)\n",
    "ax.axvline(x=0.4, color='#3498db', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Premise-Explanation Word Overlap')\n",
    "ax.set_ylabel('Flagged as Tautology')\n",
    "ax.set_title('Problem 1: Similarity Threshold Too High')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Impact 2: Ground truth quality\n",
    "ax = axes[0, 1]\n",
    "categories = ['Entailment', 'Neutral', 'Contradiction']\n",
    "taut_rates = [0.18, 0.32, 0.25]  # From analysis\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "\n",
    "bars = ax.bar(categories, taut_rates, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.axhline(y=0.20, color='orange', linestyle='--', linewidth=2, label='Acceptable (20%)')\n",
    "ax.set_ylabel('% Explanations with Tautology')\n",
    "ax.set_title('Problem 2: GT Contains Tautologies')\n",
    "ax.set_ylim([0, 0.4])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, rate in zip(bars, taut_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{rate:.1%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Impact 3: Distillation Loss\n",
    "ax = axes[1, 0]\n",
    "phases = ['CE Loss\\nWeight', 'Distill Loss\\nWeight', 'What\\nGets Used']\n",
    "intended = [0.2, 0.8, 0.2]\n",
    "actual = [0.2, 0.0, 0.2]\n",
    "\n",
    "x = np.arange(len(phases))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, intended, width, label='INTENDED', color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, actual, width, label='ACTUAL', color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Weight/Contribution')\n",
    "ax.set_title('Problem 3: Distillation Loss Never Computed (Bug)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(phases)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Impact 4: Cascading Effects\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "effects_text = \"\"\"\n",
    "CASCADING EFFECTS OF 3 PROBLEMS\n",
    "\n",
    "Problem 1 (Bad Metric)\n",
    "  â†“\n",
    "Tautologies not detected properly\n",
    "  â†“\n",
    "Model thinks tautologies are acceptable\n",
    "  â†“\n",
    "30% tautology rate in predictions\n",
    "  â†“\n",
    "Poor explanation quality overall\n",
    "\n",
    "Problem 2 (Bad Data)\n",
    "  â†“\n",
    "GT contains tautologies\n",
    "  â†“\n",
    "Model learns tautological patterns\n",
    "  â†“\n",
    "Can't improve beyond GT quality\n",
    "  â†“\n",
    "Supervised learning hits ceiling\n",
    "\n",
    "Problem 3 (Code Bug)\n",
    "  â†“\n",
    "Distillation loss = 0 (never computed)\n",
    "  â†“\n",
    "No signal to prefer good explanations\n",
    "  â†“\n",
    "Can't use quality signal even with good metric\n",
    "  â†“\n",
    "Breaks entire distillation mechanism\n",
    "\n",
    "COMBINED EFFECT:\n",
    "All three problems together prevent model from learning good explanations:\n",
    "â€¢ Metric can't detect problems (P1)\n",
    "â€¢ Data contains problems (P2)\n",
    "â€¢ Mechanism to fix them is broken (P3)\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.95, effects_text, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 100)\n",
    "print(\"NEXT STEPS FOR IMPROVEMENT\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\"\"\n",
    "Priority 1 - Fix the Code Bug (Problem 3) - EASIEST & MOST IMPACTFUL\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "â±ï¸  Time: 1-2 hours\n",
    "ğŸ’ª Impact: HIGH\n",
    "âœ… First step because fixing this enables everything else\n",
    "\n",
    "â†’ Implement one of the 3 distillation options\n",
    "â†’ Test that distill_loss > 0 in output\n",
    "â†’ Verify distillation weights are used\n",
    "\n",
    "Priority 2 - Clean the Dataset (Problem 2) - MODERATE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "â±ï¸  Time: 2-3 hours\n",
    "ğŸ’ª Impact: HIGH\n",
    "âœ… Provides cleaner signal for training\n",
    "\n",
    "â†’ Filter e-SNLI to remove high-overlap explanations\n",
    "â†’ Create train/val splits from filtered data\n",
    "â†’ Re-train model on cleaner data\n",
    "â†’ Compare results with original\n",
    "\n",
    "Priority 3 - Improve Similarity Metric (Problem 1) - MODERATE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "â±ï¸  Time: 3-4 hours\n",
    "ğŸ’ª Impact: MEDIUM\n",
    "âœ… Better evaluation and quality feedback\n",
    "\n",
    "â†’ Implement n-gram based tautology detection\n",
    "â†’ Add semantic similarity using embeddings\n",
    "â†’ Lower threshold from 0.7 to 0.3-0.4\n",
    "â†’ Validate on manual examples\n",
    "\n",
    "EXPECTED OUTCOME:\n",
    "â€¢ Distillation loss > 0 (currently 0)\n",
    "â€¢ Tautology rate < 20% (currently 30%)\n",
    "â€¢ GT similarity improved from 0.388 to >0.50\n",
    "â€¢ Model outperforms supervised baseline\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ… ANALYSIS COMPLETE - All phases finished\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
